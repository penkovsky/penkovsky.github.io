<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="Breaking news! Artificial intelligence is taking over the world. Or it
is not? Here is what you need to know about a deeper concept
of meta-learning.

Meta-learning is learning about learning.
Learning how to learn belongs here too.">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/neural-networks/meta-learning/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/neural-networks/meta-learning/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/neural-networks/meta-learning/">
  <meta property="og:title" content="Meta-Learning | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="Breaking news! Artificial intelligence is taking over the world. Or it
is not? Here is what you need to know about a deeper concept
of meta-learning.

Meta-learning is learning about learning.
Learning how to learn belongs here too."><meta property="og:image" content="https://penkovsky.com/img/posts/neural-networks/meta-learning.jpg">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2023-12-16T14:20:00&#43;01:00">
  
  <meta property="article:modified_time" content="2023-12-16T14:20:00&#43;01:00">
  

  

  

  <title>Meta-Learning | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/posts/neural-networks/meta-learning.jpg" class="article-banner" itemprop="image">
  

  <span class="article-header-caption">Meta-learning (image by stability.ai)</span>
</div>



  <div class="article-container">

    <h1 itemprop="name">Meta-Learning</h2>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2023-12-16 14:20:00 &#43;0100 CET" itemprop="datePublished">
    <time datetime="2023-12-16 14:20:00 &#43;0100 CET" itemprop="dateModified">
      Dec 16, 2023
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    6 min read
  </span>
  

  
  

  
  
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>Breaking news! Artificial intelligence is taking over the world. Or it
is not? Here is what you need to know about a deeper concept
of meta-learning.</p>

<p><em>Meta-learning</em> is learning about learning.
Learning how to learn belongs here too.</p>

<hr />

<p><strong>Table of Contents</strong></p>

<ol>
<li><a href="#models-analyzing-other-models">Models Analyzing Other Models</a></li>
<li><a href="#updating-neural-network-during-run">Updating Neural Network During
Run</a></li>
<li><a href="#self-modifying-learning-algorithms">Self-Modifying Learning Algorithms</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<hr />

<p>The first thing that pops to mind is <a href="https://en.wikipedia.org/wiki/Automated_machine_learning">AutoML</a>,
a search for the best model architecture. It often includes
data preparation, feature engineering, and hyperparameter search.
Companies like Google adore this approach: First, AutoML requires
a lot of computational resources, and companies like Google have it.
Second, AutoML can make machine learning accessible to non-experts,
enabling companies like Google to sell their compute power to
more customers.</p>

<p>However, meta-learning is more than AutoML. It is a broad subject.
Here are the articles that have marked my thinking.</p>

<h2 id="models-analyzing-other-models">Models Analyzing Other Models</h2>

<p>In <a href="https://openreview.net/pdf?id=cmJiEqniEc">this work</a>, Langosco and colleagues
detect whether a <a href="/neural-networks/day1/">neural network</a> has a <em>backdoor</em>.
A backdoor is a way to modify the input to force a network
to produce an undesirable output. As an extreme example, imagine
a self-driving car that crashes into another car after seeing a malicious
drawing on the road. That drawing would be a neural network's input
that was misinterpreted to cause a bad decision, which itself led to a crash.
And a backdoor in this case could be a susceptibility of a neural
network to a particular form or texture.</p>

<p>Therefore, you see, it could be quite hard to manually design such an algorithm
that would detect if a neural network has a backdoor in it.
So it seems natural to use another neural network (which we call a
<em>meta-model</em>) to analyze the weights of the first one and to detect whether
there is a built-in backdoor or not.</p>

<p>I think the importance of this work is not only
technical, it also raises the awareness about the
challenges that we <em>will</em> face when relying on AI. In 2023 AI is
often defined as &quot;allowing computers to learn and solve problems
almost like a person&quot; (<a href="https://www.bbc.com/news/technology-65855333">BBC, 01/11/2023</a>). Which, to be honest, has
almost nothing to do with intelligence<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>. And even though
there is no imminent <a href="https://en.wikipedia.org/wiki/The_Terminator">Terminator</a> danger, we need
to beware of malicious human actors out there.</p>

<p>Here are a <a href="https://arxiv.org/abs/2301.12780">few</a> <a href="https://arxiv.org/abs/2002.05688">more</a> <a href="https://arxiv.org/abs/2110.15288">examples</a>
<a href="https://arxiv.org/abs/2002.11448">of</a> models analyzing other models.</p>

<h2 id="updating-neural-network-during-run">Updating Neural Network During Run</h2>

<p>From the perspective of <a href="/neural-networks/beyond/">reinforcement learning</a>,
meta-learning is about agents that learn from ongoing experience. For instance,
a robotic dog was trained to walk on the grass. Now, as it crosses a patch of
sandy terrain, can it adapt its walking gait?
<a href="https://arxiv.org/abs/2007.02686">Najarro and Risi</a> suggest a recipe of an agent that
modifies its behavior based on <em>local</em> learning. That is, based on the
<a href="/neural-networks/day1/">neural network</a> <a href="/neural-networks/day2/">layers'</a>
inputs <em>and</em> outputs. Using the Hebbian rule (&quot;what fires together wires
together&quot;), the network is updated on the fly. Evolutionary techniques
are employed to train such agents. In a remarkable demonstration, a robot
manages to walk despite one of its legs being damaged. This would be hardly
possible with neural networks having static weights (that is, &quot;normal&quot; neural
networks).</p>

<p>To capture invariant object representations, <a href="https://www.nature.com/articles/s41593-023-01460-y">Halvagal and Zenke</a>
propose to augment the Hebbian rule to include a predictive term. &quot;It cancels
when the neural activity does not change and, therefore, accurately predicts
future activity,&quot; the authors explain.
The idea resonates with HTM neurons from <a href="https://link.springer.com/content/pdf/10.1007/s42452-021-04715-0.pdf">thousand brains theory</a>,
originally conceptualized by <a href="https://www.youtube.com/watch?v=-EVqrDlAqYo&amp;t=2075s">Jeff Hawkins</a>. Each HTM neuron
predicts its activation. The hypothesis claims that these predictions
of expected inputs allow brains to filter out what has changed in the
environment and what is immediately important to notice. Like a branch cracking.
Suddenly, a wild animal detects a threat in the jungle. Such alertness is
essential for survival in a complex environment.</p>

<p>&quot;So how about <a href="https://arxiv.org/abs/1706.03762">transformers</a>?&quot; you might ask, &quot;They use attention.&quot;
Yes, transformers technically belong to the category of dynamic neural networks,
too. What happens with GPT models (<em>T</em> is for transformer of course) is that
when you provide a text prompt, it serves as a context. The <em>attention</em>
mechanism underpinning the transformer architecture is transforming the neural
network weights, as a function of that context. And that is what makes
transformers flexible.</p>

<p>By the way don't be misled thinking that the attention mechanism/dynamically
updated weights is the reason why transformer models are called <em>generative</em>!
Generative models already exist for decades. Other examples are generative
adversarial networks (GANs), auto-encoders,
<a href="/neural-networks/day9/">variational auto-encoders</a> (VAEs), Hidden Markov
models, etc. The actual reason why all those models are called generative is
learning a <a href="https://en.wikipedia.org/wiki/Generative_model">joint distribution</a>.
Combining a generative model with neural networks is now called
<a href="https://en.wikipedia.org/wiki/Generative_artificial_intelligence">Generative AI</a> (not so elegant, as you can see).
It just happens that the transformer architecture is popular,
and is the first thing associated with &quot;Generative AI&quot; these days.</p>

<p>Coming back to transformers, <a href="https://arxiv.org/abs/2109.02869">Tang and Ha</a> demonstrate what they call
a <em>sensory neuron</em>. Based on the attention mechanism, their agents
trained by reinforcement learning are able to quickly re-adapt to the
change of order of sensory stimuli. That is, it does not matter in which
order the observation inputs are provided. Moreover, their agent is not confused
even when the majority of the inputs are masked out.</p>

<h2 id="self-modifying-learning-algorithms">Self-Modifying Learning Algorithms</h2>

<p>Perhaps this concept is the pinnacle of meta-learning. What can be more
appealing than learning algorithms that modify themselves?
If updating a neural network during run was a <em>first-order</em> meta-learning,
then updating neural networks that update neural networks would be
already a <em>second-order</em>. And by induction, neural networks that update their own
weights, as is done for example by <a href="https://arxiv.org/abs/2202.05780">Irie and colleagues</a>, can be
considered an <em>infinite-order</em> meta-learning.</p>

<p>Don't agree? Then shoot me an email!</p>

<h2 id="conclusion">Conclusion</h2>

<p>The family of meta-learning concepts is vast. This article has
touched a few notable examples. However, as the field continues to
evolve, more interesting work is to appear.
I don't know how far we are from the so-called &quot;artificial general
intelligence&quot;, however, its distinctive characteristic is extreme adaptivity.
And adaptivity is something that is currently explored by meta-learning
researchers.</p>

<h2 id="citation">Citation</h2>

<pre>
@article{penkovsky2023meta,
 title   = "Meta-Learning",
 author  = "Penkovsky, Bogdan",
 journal = "penkovsky.com",
 year    = "2023",
 month   = "December",
 url     = "https://penkovsky.com/neural-networks/meta-learning/"
}
</pre>

<!-- https://www.bbc.com/news/technology-65855333 https://archive.is/BUSha -->

<!--

-->

<!-- https://archive.is/J2QPq -->
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1"><p>I believe, a truly &quot;intelligent&quot; machine has to be adaptive at least.
And probably it has—in some sense—to be <em>embodied</em> as well.
What we witness today with ChatGPT is a <em>text-to-text</em> engine. It's more
about machine translation than intelligence. Once I asked it to create a
personal training plan for swimming. I've got some result, quite far from
what I specified. Does ChatGPT even <em>know</em> about how to be immersed in the
water and what it may feel like? Certainly not. It managed to
&quot;translate&quot; existing articles and compile a training plan of some kind.
I doubt it could ever substitute a coach who can actually swim.</p>
 <a class="footnote-return" href="#fnref:fn-1"><sup>^</sup></a></li>
</ol>
</div>
    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/deep-learning/">Deep Learning</a>
  
</div>




    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/neural-networks/beyond/">Day 10: Beyond Supervised Learning</a></li>
        
        <li><a href="/neural-networks/day9/">Day 9: Roaming The Latent Space</a></li>
        
        <li><a href="/neural-networks/day8/">Day 8: Model Uncertainty Estimation</a></li>
        
        <li><a href="/neural-networks/day7/">Day 7: Real World Deep Learning</a></li>
        
        <li><a href="/publication/stochastic-binary-cnn/">Hardware-Efficient Stochastic Binary CNN Architectures for Near-Sensor Computing</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      <a rel="me" href="https://sigmoid.social/@penkovsky">&copy; Bogdan Penkovsky 2023</a> &middot; 

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/search.json";
      const i18n = {
        'placeholder': "Search...",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>

