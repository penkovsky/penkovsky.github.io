<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="So far we have explored neural networks almost in the vacuum. Although we have provided some illustrations for better clarity, relying an existing framework would allow us to benefit from the knowledge of previous contributors. One such framework is called Hasktorch. Among the practical reasons to use Hasktorch is relying on a mature Torch Tensor library. Another good reason is strong GPU acceleration, which is necessary for almost any serious deep learning project.">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/neural-networks/day7/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/neural-networks/day7/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/neural-networks/day7/">
  <meta property="og:title" content="Day 7: Real World Deep Learning | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="So far we have explored neural networks almost in the vacuum. Although we have provided some illustrations for better clarity, relying an existing framework would allow us to benefit from the knowledge of previous contributors. One such framework is called Hasktorch. Among the practical reasons to use Hasktorch is relying on a mature Torch Tensor library. Another good reason is strong GPU acceleration, which is necessary for almost any serious deep learning project."><meta property="og:image" content="https://penkovsky.com/img/posts/mnist/mnist-sky-blue.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2022-04-18T22:55:00&#43;02:00">
  
  <meta property="article:modified_time" content="2022-04-18T22:55:00&#43;02:00">
  

  

  

  <title>Day 7: Real World Deep Learning | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>AI</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#talks">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/posts/mnist/mnist-sky-blue.png" class="article-banner" itemprop="image">
  

  
</div>



  <div class="article-container">

    <h1 itemprop="name">Day 7: Real World Deep Learning</h2>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2022-04-18 22:55:00 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2022-04-18 22:55:00 &#43;0200 CEST" itemprop="dateModified">
      Apr 18, 2022
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    9 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://penkovsky.com/categories/10-days-of-grad/">10 Days Of Grad</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>So far we have explored neural networks almost in the vacuum. Although we have
provided some illustrations for better clarity, relying an existing framework
would allow us to benefit from the knowledge of previous contributors. One such
framework is called <a href="https://github.com/hasktorch/hasktorch">Hasktorch</a>. Among
the practical reasons to use Hasktorch is relying on a mature <a href="https://pytorch.org/docs/stable/torch.html">Torch</a>
Tensor library. Another good reason is strong GPU acceleration, which is
necessary for almost any serious deep learning project. Finally, standard
interfaces rather than reinventing the wheel will help to reduce the
boilerplate.</p>

<blockquote>
<p>Fun fact: one of Hasktorch
<a href="https://github.com/hasktorch/hasktorch/graphs/contributors">contributors</a> is
Adam Paszke, the original author of Pytorch.</p>
</blockquote>

<hr />

<p><strong>Today's post is also based on</strong></p>

<ul>
<li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
<li><a href="/neural-networks/day4/">Day 4: The Importance Of Batch Normalization</a></li>
<li><a href="/neural-networks/day5/">Day 5: Convolutional Neural Networks Tutorial</a></li>
</ul>

<p>The source code from this post is available <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day7">on Github</a>.</p>

<hr />

<h2 id="the-basics">The Basics</h2>

<p>The easiest way <a href="https://github.com/hasktorch/hasktorch#getting-started">to start</a>
with Hasktorch is via <a href="https://www.docker.com/get-started/">Docker</a>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-shell" data-lang="shell">  docker run --gpus all -it --rm -p <span style="color:#ae81ff">8888</span>:8888 <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    -v <span style="color:#66d9ef">$(</span>pwd<span style="color:#66d9ef">)</span>:/home/ubuntu/data <span style="color:#ae81ff">\
</span><span style="color:#ae81ff"></span>    htorch/hasktorch-jupyter:latest-cu11</code></pre></div>

<p>Now, you may open <code>localhost:8888</code> in your browser to access <a href="https://jupyter.org/">Jupyterlab</a>
notebooks. Note that you need to select <code>Haskell</code> kernel when creating a new notebook.</p>

<p>If you have never used Torch library before, you may also want to review this
<a href="https://hasktorch.github.io/tutorial/02-tensors.html">tutorial</a>.</p>

<h2 id="mnist-example">MNIST Example</h2>

<p>Let's take the familiar MNIST example and see how it can be implemented
<a href="https://github.com/hasktorch/hasktorch/tree/master/examples/mnist-mlp">in Hasktorch</a>.</p>

<h3 id="imports">Imports</h3>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#75715e">{-# LANGUAGE DeriveAnyClass #-}</span>
<span style="color:#75715e">{-# LANGUAGE DeriveGeneric #-}</span>
<span style="color:#75715e">{-# LANGUAGE MultiParamTypeClasses #-}</span>
<span style="color:#75715e">{-# LANGUAGE RecordWildCards #-}</span>
<span style="color:#75715e">{-# LANGUAGE ScopedTypeVariables #-}</span>

<span style="color:#66d9ef">import</span> Control.Exception.Safe
  ( <span style="color:#66d9ef">SomeException</span> (<span style="color:#f92672">..</span>),
    <span style="color:#a6e22e">try</span>,
  )
<span style="color:#66d9ef">import</span> Control.Monad ( <span style="color:#a6e22e">forM_</span>, <span style="color:#a6e22e">when</span>, (<span style="color:#f92672">&lt;=&lt;</span>) )
<span style="color:#66d9ef">import</span> Control.Monad.Cont ( <span style="color:#66d9ef">ContT</span> (<span style="color:#f92672">..</span>) )
<span style="color:#66d9ef">import</span> GHC.Generics
<span style="color:#66d9ef">import</span> Pipes <span style="color:#66d9ef">hiding</span> ( (<span style="color:#f92672">~&gt;</span>) )
<span style="color:#66d9ef">import</span> <span style="color:#66d9ef">qualified</span> Pipes.Prelude <span style="color:#66d9ef">as</span> P
<span style="color:#66d9ef">import</span> Torch
<span style="color:#66d9ef">import</span> Torch.Serialize
<span style="color:#66d9ef">import</span> Torch.Typed.Vision ( <span style="color:#a6e22e">initMnist</span> )
<span style="color:#66d9ef">import</span> <span style="color:#66d9ef">qualified</span> Torch.Vision <span style="color:#66d9ef">as</span> V
<span style="color:#66d9ef">import</span> Prelude <span style="color:#66d9ef">hiding</span> ( <span style="color:#a6e22e">exp</span> )</code></pre></div>

<p>The most notable import is the <code>Torch</code> module itself. There are also related
helpers such <code>Torch.Vision</code> to handle image data. The function <code>initMnist</code> has
type</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">initMnist</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">String</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">IO</span> (<span style="color:#66d9ef">MnistData</span>, <span style="color:#66d9ef">MnistData</span>)</code></pre></div>

<p>The function is loading MNIST train and test datasets, similar to <code>loadMNIST</code>
from previous posts.</p>

<p>It might be also useful to pay attention to
<a href="https://hackage.haskell.org/package/pipes"><code>Pipes</code></a> module. It is an
alternative to previously used <code>Streamly</code>, which also allows building
streaming components.</p>

<p>We also import functions from <code>Control.Monad</code>, which are useful for IO
operations.</p>

<p>Finally, we hide <code>exp</code> function in favor of Torch <code>exp</code>, which operates on
tensors (arrays)<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup> rather than floating point scalars:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">Torch</span><span style="color:#f92672">.</span><span style="color:#a6e22e">exp</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Tensor</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Tensor</span></code></pre></div>

<h3 id="defining-neural-network-architecture">Defining Neural Network Architecture</h3>

<p>First we define a neural network data structure that contains trained
parameters (neural network weights). In the simplest case, it can be a
multilayer perceptron (MLP).</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">data</span> <span style="color:#66d9ef">MLP</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">MLP</span>
  { <span style="color:#a6e22e">fc1</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Linear</span>,
    <span style="color:#a6e22e">fc2</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Linear</span>,
    <span style="color:#a6e22e">fc3</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Linear</span>
  }
  <span style="color:#66d9ef">deriving</span> (<span style="color:#66d9ef">Generic</span>, <span style="color:#66d9ef">Show</span>, <span style="color:#66d9ef">Parameterized</span>)</code></pre></div>

<p>This MLP contains three linear layers. Next, we may define a data structure
that specifies the number of neurons in each layer:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">data</span> <span style="color:#66d9ef">MLPSpec</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">MLPSpec</span>
  { <span style="color:#a6e22e">i</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Int</span>,
    <span style="color:#a6e22e">h1</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Int</span>,
    <span style="color:#a6e22e">h2</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Int</span>,
    <span style="color:#a6e22e">o</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Int</span>
  }
  <span style="color:#66d9ef">deriving</span> (<span style="color:#66d9ef">Show</span>, <span style="color:#66d9ef">Eq</span>)</code></pre></div>

<p>Now, we can define a neural network as a function, similar as we did on
<a href="/neural-networks/day5/">Day 5</a> with a &quot;reversed&quot; composition operator
<code>(~&gt;)</code>.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">(<span style="color:#f92672">~&gt;</span>) <span style="color:#f92672">::</span> (<span style="color:#a6e22e">a</span> <span style="color:#f92672">-&gt;</span> <span style="color:#a6e22e">b</span>) <span style="color:#f92672">-&gt;</span> (<span style="color:#a6e22e">b</span> <span style="color:#f92672">-&gt;</span> <span style="color:#a6e22e">c</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#a6e22e">a</span> <span style="color:#f92672">-&gt;</span> <span style="color:#a6e22e">c</span>
<span style="color:#a6e22e">f</span> <span style="color:#f92672">~&gt;</span> <span style="color:#a6e22e">g</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">g</span><span style="color:#f92672">.</span> <span style="color:#a6e22e">f</span>

<span style="color:#a6e22e">mlp</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">MLP</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Tensor</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Tensor</span>
<span style="color:#a6e22e">mlp</span> <span style="color:#66d9ef">MLP</span> {<span style="color:#f92672">..</span>} <span style="color:#f92672">=</span>
  <span style="color:#75715e">-- Layer 1</span>
  <span style="color:#a6e22e">linear</span> <span style="color:#a6e22e">fc1</span>
  <span style="color:#f92672">~&gt;</span> <span style="color:#a6e22e">relu</span>

  <span style="color:#75715e">-- Layer 2</span>
  <span style="color:#f92672">~&gt;</span> <span style="color:#a6e22e">linear</span> <span style="color:#a6e22e">fc2</span>
  <span style="color:#f92672">~&gt;</span> <span style="color:#a6e22e">relu</span>

  <span style="color:#75715e">-- Layer 3</span>
  <span style="color:#f92672">~&gt;</span> <span style="color:#a6e22e">linear</span> <span style="color:#a6e22e">fc3</span>
  <span style="color:#f92672">~&gt;</span> <span style="color:#a6e22e">logSoftmax</span> (<span style="color:#66d9ef">Dim</span> <span style="color:#ae81ff">1</span>)</code></pre></div>

<p>We finish by a (log) softmax layer reducing the tensor's dimension 1 (<code>Dim 1</code>).
Derivatives of <code>linear</code>, <code>relu</code>, and <code>logSoftmax</code> are already handled by Torch
library.</p>

<h3 id="initial-weights">Initial Weights</h3>

<p>How do we generate initial random weights? As you may remember from
<a href="/neural-networks/day5/">Day 5</a>, we could create a function such as this one:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">randNetwork</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">do</span>
  <span style="color:#66d9ef">let</span> [<span style="color:#a6e22e">i</span>, <span style="color:#a6e22e">h1</span>, <span style="color:#a6e22e">h2</span>, <span style="color:#a6e22e">o</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">784</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">10</span>]
  <span style="color:#a6e22e">fc1</span> <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">randLinear</span> (<span style="color:#66d9ef">Sz2</span> <span style="color:#a6e22e">i</span> <span style="color:#a6e22e">h1</span>)
  <span style="color:#a6e22e">fc2</span> <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">randLinear</span> (<span style="color:#66d9ef">Sz2</span> <span style="color:#a6e22e">h1</span> <span style="color:#a6e22e">h2</span>)
  <span style="color:#a6e22e">fc3</span> <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">randLinear</span> (<span style="color:#66d9ef">Sz2</span> <span style="color:#a6e22e">h2</span> <span style="color:#a6e22e">o</span>)
  <span style="color:#a6e22e">return</span> <span style="color:#f92672">$</span>
     <span style="color:#66d9ef">MLP</span> {  <span style="color:#a6e22e">fc1</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">fc1</span>
          , <span style="color:#a6e22e">fc2</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">fc2</span>
          , <span style="color:#a6e22e">fc3</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">fc3</span>
          }</code></pre></div>

<p>In our example we do almost the same, except we benefit from
<a href="http://learnyouahaskell.com/functors-applicative-functors-and-monoids">applicative functors</a> and
<a href="https://github.com/hasktorch/hasktorch/blob/0269df6b3c7fdfa3f25a8c8e315b6188214c57ca/hasktorch/src/Torch/NN.hs#L201"><code>Randomizable</code></a>.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">instance</span> <span style="color:#66d9ef">Randomizable</span> <span style="color:#66d9ef">MLPSpec</span> <span style="color:#66d9ef">MLP</span> <span style="color:#66d9ef">where</span>
  <span style="color:#a6e22e">sample</span> <span style="color:#66d9ef">MLPSpec</span> {<span style="color:#f92672">..</span>} <span style="color:#f92672">=</span>
    <span style="color:#66d9ef">MLP</span>
      <span style="color:#f92672">&lt;$&gt;</span> <span style="color:#a6e22e">sample</span> (<span style="color:#66d9ef">LinearSpec</span> <span style="color:#a6e22e">i</span> <span style="color:#a6e22e">h1</span>)
      <span style="color:#f92672">&lt;*&gt;</span> <span style="color:#a6e22e">sample</span> (<span style="color:#66d9ef">LinearSpec</span> <span style="color:#a6e22e">h1</span> <span style="color:#a6e22e">h2</span>)
      <span style="color:#f92672">&lt;*&gt;</span> <span style="color:#a6e22e">sample</span> (<span style="color:#66d9ef">LinearSpec</span> <span style="color:#a6e22e">h2</span> <span style="color:#a6e22e">o</span>)</code></pre></div>

<p>We say above that <code>MLP</code> is an instance of the <code>Randomizable</code> typeclass,
parametrized by <code>MLPSpec</code>. All we needed to define this instance was to
implement a <code>sample</code> function. To generate initial MLP weights, later we can
simply write</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">let</span> <span style="color:#a6e22e">spec</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">MLPSpec</span> <span style="color:#ae81ff">784</span> <span style="color:#ae81ff">64</span> <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">10</span>
<span style="color:#a6e22e">net</span> <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">sample</span> <span style="color:#a6e22e">spec</span></code></pre></div>

<h3 id="train-loop">Train Loop</h3>

<p>The core of the neural network training is <code>trainLoop</code>, which enables a single
training &quot;epoch&quot;. Let us first inspect its type signature.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">trainLoop</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Optimizer</span> <span style="color:#a6e22e">o</span> <span style="color:#f92672">=&gt;</span> <span style="color:#66d9ef">MLP</span> <span style="color:#f92672">-&gt;</span> <span style="color:#a6e22e">o</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">ListT</span> <span style="color:#66d9ef">IO</span> (<span style="color:#66d9ef">Tensor</span>, <span style="color:#66d9ef">Tensor</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">IO</span> <span style="color:#66d9ef">MLP</span></code></pre></div>

<p>This signifies that the function accepts an initial neural network
configuration, an optimizer, and a dataset. The optimizer can be a gradient
descent (GD), Adam, or other
<a href="https://hasktorch.github.io/hasktorch/html/Torch-Optim.html">optimizer</a>.
The result of the function is a new MLP configuration, as a result of IO call.
IO is necessary for instance if we want to print the loss after each iteration.
Now, let's take a look at the implementation:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">trainLoop</span> <span style="color:#a6e22e">model</span> <span style="color:#a6e22e">optimizer</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">P</span><span style="color:#f92672">.</span><span style="color:#a6e22e">foldM</span> <span style="color:#a6e22e">step</span> <span style="color:#a6e22e">begin</span> <span style="color:#a6e22e">done</span><span style="color:#f92672">.</span> <span style="color:#a6e22e">enumerateData</span></code></pre></div>

<p>First, we enumerate the dataset with <code>enumerateData</code>. Then, we iterate over (fold)
the batches. The <code>step</code> function is an analogy to a step in the gradient descent
algorithm:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#66d9ef">where</span>
    <span style="color:#a6e22e">step</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">MLP</span> <span style="color:#f92672">-&gt;</span> ((<span style="color:#66d9ef">Tensor</span>, <span style="color:#66d9ef">Tensor</span>), <span style="color:#66d9ef">Int</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">IO</span> <span style="color:#66d9ef">MLP</span>
    <span style="color:#a6e22e">step</span> <span style="color:#a6e22e">model</span> ((<span style="color:#a6e22e">input</span>, <span style="color:#a6e22e">label</span>), <span style="color:#a6e22e">iter</span>) <span style="color:#f92672">=</span> <span style="color:#66d9ef">do</span>
      <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">loss</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">nllLoss&#39;</span> <span style="color:#a6e22e">label</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">mlp</span> <span style="color:#a6e22e">model</span> <span style="color:#a6e22e">input</span>
      <span style="color:#75715e">-- Print loss every 50 batches</span>
      <span style="color:#a6e22e">when</span> (<span style="color:#a6e22e">iter</span> `<span style="color:#a6e22e">mod</span>` <span style="color:#ae81ff">50</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>) <span style="color:#f92672">$</span> <span style="color:#66d9ef">do</span>
        <span style="color:#a6e22e">putStrLn</span> <span style="color:#f92672">$</span> <span style="color:#e6db74">&#34;Iteration: &#34;</span> <span style="color:#f92672">++</span> <span style="color:#a6e22e">show</span> <span style="color:#a6e22e">iter</span> <span style="color:#f92672">++</span> <span style="color:#e6db74">&#34; | Loss: &#34;</span> <span style="color:#f92672">++</span> <span style="color:#a6e22e">show</span> <span style="color:#a6e22e">loss</span>
      (<span style="color:#a6e22e">newParam</span>, <span style="color:#66d9ef">_</span>) <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">runStep</span> <span style="color:#a6e22e">model</span> <span style="color:#a6e22e">optimizer</span> <span style="color:#a6e22e">loss</span> <span style="color:#ae81ff">1e-3</span>
      <span style="color:#a6e22e">return</span> <span style="color:#a6e22e">newParam</span></code></pre></div>

<p>We calculate a
<a href="https://hasktorch.github.io/hasktorch/html/Torch-Functional.html#v:nllLoss-39-">negative log likelihood loss</a>
<code>nllLoss'</code> between the ground truth label and the output
of our MLP. Note that <code>model</code> is the parameter, i.e. weights of the MLP
network. Then, we take advantage of the iteration number <code>iter</code> to print the
loss every 50 iterations. Finally, we perform a gradient descent step using our
optimizer via
<code>runStep :: ... =&gt; model -&gt; optimizer -&gt; Loss -&gt; LearningRate -&gt; IO (model, optimizer)</code>
and keep only new model <code>newParam</code>. The learning rate here is <code>1e-3</code>, but can
be eventually changed.</p>

<p>The <code>done</code> function is (trivial in this case) finalization of <code>foldM</code>
iterations over the MLP model and <code>begin</code> are the initial weights (we use <code>pure</code>
to satisfy the type
<a href="https://hackage.haskell.org/package/pipes-4.3.16/docs/src/Pipes.Prelude.html#foldM"><code>m x</code></a>
requirement).</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">    <span style="color:#a6e22e">done</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">pure</span>
    <span style="color:#a6e22e">begin</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">pure</span> <span style="color:#a6e22e">model</span></code></pre></div>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p>The remaining part is simple. We load the data into batches,
specify the number of neurons in our MLP, choose an optimizer,
and initialize the random weights.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">main</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">do</span>
  (<span style="color:#a6e22e">trainData</span>, <span style="color:#a6e22e">testData</span>) <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">initMnist</span> <span style="color:#e6db74">&#34;data&#34;</span>
  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">trainMnist</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">V</span><span style="color:#f92672">.</span><span style="color:#66d9ef">MNIST</span> {<span style="color:#a6e22e">batchSize</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>, <span style="color:#a6e22e">mnistData</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">trainData</span>}
      <span style="color:#a6e22e">testMnist</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">V</span><span style="color:#f92672">.</span><span style="color:#66d9ef">MNIST</span> {<span style="color:#a6e22e">batchSize</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, <span style="color:#a6e22e">mnistData</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">testData</span>}
      <span style="color:#a6e22e">spec</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">MLPSpec</span> <span style="color:#ae81ff">784</span> <span style="color:#ae81ff">64</span> <span style="color:#ae81ff">32</span> <span style="color:#ae81ff">10</span>
      <span style="color:#a6e22e">optimizer</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">GD</span>
  <span style="color:#a6e22e">net</span> <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">sample</span> <span style="color:#a6e22e">spec</span></code></pre></div>

<p>Then, we train the network for 5 epochs:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#a6e22e">net&#39;</span> <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">foldLoop</span> <span style="color:#a6e22e">net</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">\model</span> <span style="color:#66d9ef">_</span> <span style="color:#f92672">-&gt;</span>
      <span style="color:#a6e22e">runContT</span> (<span style="color:#a6e22e">streamFromMap</span> (<span style="color:#a6e22e">datasetOpts</span> <span style="color:#ae81ff">2</span>) <span style="color:#a6e22e">trainMnist</span>) <span style="color:#f92672">$</span> <span style="color:#a6e22e">trainLoop</span> <span style="color:#a6e22e">model</span> <span style="color:#a6e22e">optimizer</span><span style="color:#f92672">.</span> <span style="color:#a6e22e">fst</span></code></pre></div>

<p>Finally, we may examine the model on test images</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#a6e22e">forM_</span> [<span style="color:#ae81ff">0</span> <span style="color:#f92672">..</span> <span style="color:#ae81ff">10</span>] <span style="color:#f92672">$</span> <span style="color:#a6e22e">displayImages</span> <span style="color:#a6e22e">net&#39;</span> <span style="color:#f92672">&lt;=&lt;</span> <span style="color:#a6e22e">getItem</span> <span style="color:#a6e22e">testMnist</span></code></pre></div>

<p>For this purpose may use a function such as</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">displayImages</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">MLP</span> <span style="color:#f92672">-&gt;</span> (<span style="color:#66d9ef">Tensor</span>, <span style="color:#66d9ef">Tensor</span>) <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">IO</span> ()
<span style="color:#a6e22e">displayImages</span> <span style="color:#a6e22e">model</span> (<span style="color:#a6e22e">testImg</span>, <span style="color:#a6e22e">testLabel</span>) <span style="color:#f92672">=</span> <span style="color:#66d9ef">do</span>
  <span style="color:#66d9ef">V</span><span style="color:#f92672">.</span><span style="color:#a6e22e">dispImage</span> <span style="color:#a6e22e">testImg</span>
  <span style="color:#a6e22e">putStrLn</span> <span style="color:#f92672">$</span> <span style="color:#e6db74">&#34;Model        : &#34;</span> <span style="color:#f92672">++</span> (<span style="color:#a6e22e">show</span><span style="color:#f92672">.</span> <span style="color:#a6e22e">argmax</span> (<span style="color:#66d9ef">Dim</span> <span style="color:#ae81ff">1</span>) <span style="color:#66d9ef">RemoveDim</span><span style="color:#f92672">.</span> <span style="color:#a6e22e">exp</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">mlp</span> <span style="color:#a6e22e">model</span> <span style="color:#a6e22e">testImg</span>)
  <span style="color:#a6e22e">putStrLn</span> <span style="color:#f92672">$</span> <span style="color:#e6db74">&#34;Ground Truth : &#34;</span> <span style="color:#f92672">++</span> <span style="color:#a6e22e">show</span> <span style="color:#a6e22e">testLabel</span></code></pre></div>

<h3 id="running">Running</h3>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">Iteration: 0 | Loss: Tensor Float []  12.3775   
Iteration: 50 | Loss: Tensor Float []  1.0952   
Iteration: 100 | Loss: Tensor Float []  0.5626   
Iteration: 150 | Loss: Tensor Float []  0.6660   
Iteration: 200 | Loss: Tensor Float []  0.4771   
Iteration: 0 | Loss: Tensor Float []  0.5012   
Iteration: 50 | Loss: Tensor Float []  0.4058   
Iteration: 100 | Loss: Tensor Float []  0.3095   
Iteration: 150 | Loss: Tensor Float []  0.4237   
Iteration: 200 | Loss: Tensor Float []  0.3433   
Iteration: 0 | Loss: Tensor Float []  0.3671   
Iteration: 50 | Loss: Tensor Float []  0.3206   
Iteration: 100 | Loss: Tensor Float []  0.2467   
Iteration: 150 | Loss: Tensor Float []  0.3420   
Iteration: 200 | Loss: Tensor Float []  0.2737   
Iteration: 0 | Loss: Tensor Float []  0.3054   
Iteration: 50 | Loss: Tensor Float []  0.2779   
Iteration: 100 | Loss: Tensor Float []  0.2161   
Iteration: 150 | Loss: Tensor Float []  0.2933   
Iteration: 200 | Loss: Tensor Float []  0.2289   
Iteration: 0 | Loss: Tensor Float []  0.2693   
Iteration: 50 | Loss: Tensor Float []  0.2530   
Iteration: 100 | Loss: Tensor Float []  0.1979   
Iteration: 150 | Loss: Tensor Float []  0.2616   
Iteration: 200 | Loss: Tensor Float []  0.1986   
              
              
              
              
   #%%*****   
      ::: %   
         %:   
        :%    
        #:    
       :%     
       %.     
      #=      
     :%.      
     =#       
Model        : Tensor Int64 [1] [ 7]
Ground Truth : Tensor Int64 [1] [ 7]
              
              
     %%%#     
    %#  %     
    .  #%     
      :%:     
      %+      
     *%       
     %=       
    %%        
    %%%%++%%%=
     ==%%=.   
              
              
Model        : Tensor Int64 [1] [ 2]
Ground Truth : Tensor Int64 [1] [ 2]
              
              
        .-    
        =     
        %     
       .#     
       =:     
       @      
       #      
      ++      
      %:      
      %       
              
              
Model        : Tensor Int64 [1] [ 1]
Ground Truth : Tensor Int64 [1] [ 1]
              
              
       %.     
      *%-     
     %%%%#    
    :%%+:%-   
    %%   -%.  
    %    .@+  
    %    %%.  
    %   #%*   
    %%%%%%    
    :%%%-     
              
              
Model        : Tensor Int64 [1] [ 0]
Ground Truth : Tensor Int64 [1] [ 0]
              
              
              
     =    +   
     %    %   
    +.    %   
    %    %:   
    +    %    
    %--=*%    
     :: +%    
        =%    
        =%    
        *     
              
Model        : Tensor Int64 [1] [ 4]
Ground Truth : Tensor Int64 [1] [ 4]
              
              
              
        %@    
        @:    
       =@     
       @%     
       @      
      :@      
      %#      
      @       
      @       
      +       
              
Model        : Tensor Int64 [1] [ 1]
Ground Truth : Tensor Int64 [1] [ 1]
              
              
              
     %     %  
    %     %   
   +#    -+   
   +%*::*%    
    :%==%+    
        %     
       ++     
       %      
       %-+    
       *      
              
Model        : Tensor Int64 [1] [ 4]
Ground Truth : Tensor Int64 [1] [ 4]
              
              
              
      +       
     %%+      
    .%*%%     
    -: *%     
    -#-%%.    
     %% =#    
         %    
         .%   
          #.  
           %  
              
Model        : Tensor Int64 [1] [ 9]
Ground Truth : Tensor Int64 [1] [ 9]
              
              
         ..=. 
      .%%%%%% 
     ::%+:    
    %         
   %          
   %=         
   %%%%%%+    
     :%%%%    
      %%%%    
       %#     
              
              
Model        : Tensor Int64 [1] [ 6]
Ground Truth : Tensor Int64 [1] [ 5]
              
              
              
              
      +%%%#   
    +%*  .%%  
   :%.  .#%+  
    %@%%%%*   
       +%-    
      -%#     
      %%      
     %%       
     %=       
     @        
Model        : Tensor Int64 [1] [ 9]
Ground Truth : Tensor Int64 [1] [ 9]
              
              
       ==:    
     %%**%%   
    .%    %:  
    *-    +#  
    %     :#  
    #     :#  
   -#     +#  
   -#    .%   
    #   +%:   
    #%%%%=    
              
              
Model        : Tensor Int64 [1] [ 0]
Ground Truth : Tensor Int64 [1] [ 0]</code></pre></div>

<p>See the complete project on <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day7">Github</a>. For suggestions about the content
feel free to open a
<a href="https://github.com/penkovsky/10-days-of-grad/issues">new issue</a>.</p>

<h2 id="summary">Summary</h2>

<p>Today we have learned the basics of Hasktorch library. The most important is that
the principles from our previous days still apply. Therefore, the transition to
the new library was quite straightforward. With a few minor changes, this example
could be run on a <a href="/neural-networks/day8/">graphics processing unit</a> accelerator.</p>

<h2 id="further-reading">Further Reading</h2>

<p>Hasktorch:</p>

<ul>
<li><a href="https://hasktorch.github.io/tutorial/02-tensors.html">Hasktorch tutorial</a></li>
<li><a href="https://github.com/hasktorch/hasktorch/tree/master/examples">Hasktorch examples</a></li>
<li><a href="http://hasktorch.org/docs.html">Hasktorch documentation</a></li>
<li><a href="http://learnyouahaskell.com/functors-applicative-functors-and-monoids">Applicative functors</a></li>
</ul>

<p>Docker containers:</p>

<ul>
<li><a href="https://www.docker.com/get-started/">Getting started with Docker</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1"><a href="https://en.wikipedia.org/wiki/Tensor">Tensors</a> are represented by n-dimensional arrays.
 <a class="footnote-return" href="#fnref:fn-1"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/deep-learning/">Deep Learning</a>
  
  <a class="label label-default" href="https://penkovsky.com/tags/haskell/">Haskell</a>
  
</div>




    
    <div class="article-widget">
      Next: <a href="https://penkovsky.com/neural-networks/day8/">Day 8: Model Uncertainty Estimation</a>
    </div>
    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/neural-networks/day6/">Day 6: Saving Energy with Binarized Neural Networks</a></li>
        
        <li><a href="/neural-networks/day5/">Day 5: Convolutional Neural Networks Tutorial</a></li>
        
        <li><a href="/neural-networks/day4/">Day 4: The Importance Of Batch Normalization</a></li>
        
        <li><a href="/neural-networks/day3/">Day 3: Haskell Guide To Neural Networks</a></li>
        
        <li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; Bogdan Penkovsky 2024 

      <a rel="me" href="https://sigmoid.social/@penkovsky"><big>&sigma;</big></a>

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    
    

  </body>
</html>

