<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="Which purpose do neural networks serve for? Neural networks are learnable models. Their ultimate goal is to approach or even surpass human cognitive abilities. As Richard Sutton puts it, &#39;The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective&#39;. In his essay, Sutton argues that only models without encoded human-knowledge can outperform human-centeric approaches. Indeed, neural networks are general enough and they leverage computation.">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/neural-networks/day4/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/neural-networks/day4/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/neural-networks/day4/">
  <meta property="og:title" content="Day 4: The Importance Of Batch Normalization | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="Which purpose do neural networks serve for? Neural networks are learnable models. Their ultimate goal is to approach or even surpass human cognitive abilities. As Richard Sutton puts it, &#39;The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective&#39;. In his essay, Sutton argues that only models without encoded human-knowledge can outperform human-centeric approaches. Indeed, neural networks are general enough and they leverage computation."><meta property="og:image" content="https://penkovsky.com/img/posts/mnist/mnist-sky-blue.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-06-29T15:01:00&#43;02:00">
  
  <meta property="article:modified_time" content="2019-06-29T15:01:00&#43;02:00">
  

  

  

  <title>Day 4: The Importance Of Batch Normalization | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>Neural Networks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/posts/mnist/mnist-sky-blue.png" class="article-banner" itemprop="image">
  

  <span class="article-header-caption">Handwritten digits from the <a href="http://yann.lecun.com/exdb/mnist">MNIST dataset</a></span>
</div>



  <div class="article-container">

    <h1 itemprop="name">Day 4: The Importance Of Batch Normalization</h2>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2019-06-29 15:01:00 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2019-06-29 15:01:00 &#43;0200 CEST" itemprop="dateModified">
      Jun 29, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://penkovsky.com/categories/10-days-of-grad/">10 Days Of Grad</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>Which purpose do neural networks serve for? Neural networks are learnable
models. Their ultimate goal is to approach or even surpass human
cognitive abilities. As Richard Sutton puts it,
<em>'The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective'</em>.
In <a href="http://incompleteideas.net/IncIdeas/BitterLesson.html">his essay</a>, Sutton argues that only models without encoded
human-knowledge can outperform human-centeric approaches. Indeed, neural
networks are general enough and they leverage computation. Then, it is not
surprising how they can exhibit millions of learnable degrees of freedom.</p>

<p>The biggest challenge with neural networks is two-fold: (1) how to train those
millions of parameters, and (2) how to interpret them. Batch normalization
(shortly <em>batchnorm</em>) was <a href="https://arxiv.org/abs/1502.03167">introduced</a> as an attempt to
make training more efficient. The method can dramatically reduce the number of
training epochs. Moreover, batchnorm is perhaps the key ingredient that made
possible training of certain architectures such as
<a href="/neural-networks/day6/">binarized neural networks</a>. Finally, batchnorm is
one of the most recent neural network advances<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>

<hr />

<p><strong>Previous posts</strong></p>

<ul>
<li><a href="/neural-networks/day1/">Day 1: Learning Neural Networks The Hard Way</a></li>
<li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
<li><a href="/neural-networks/day3/">Day 3: Haskell Guide To Neural Networks</a></li>
</ul>

<p>The source code from this post is available <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day4">on Github</a>.</p>

<hr />

<h2 id="batch-normalization-in-short">Batch Normalization In Short</h2>

<h3 id="what-is-a-batch">What Is A Batch?</h3>

<p>Until now, we have looked on toy datasets. These were so small that they could
completely fit into the memory. However, in the real world exist huge databases
occupying hundreds of gigabytes of memory, such as
<a href="http://www.image-net.org/">Imagenet</a> for example. Those often would not fit
into the memory. In that case, it makes more sense to split a dataset into
smaller <em>mini-batches</em>. During a forward/backward pass, only one batch is
typically processed.</p>

<p>As the name suggests, batchnorm transformation is acting on individual batches
of data. The outputs of linear layers may cause activation function
saturation/'dead neurons'. For instance, in case of ReLU (rectified linear
unit) $f(x)=\max(0, x)$ activation, all negative values will result in zero
activations.  Therefore, it is a good idea to normalize those values by
subtracting the batch mean $\mu$. Similarly, division by standard deviation
$\sqrt{\text{var}}$ scales the amplitudes, which is especially beneficial for
<a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a>-like activations.</p>

<h3 id="training-and-batchnorm">Training And Batchnorm</h3>

<p>The batch normalization procedure differs between the training and inference
phases.  During the training, for each layer where we want to apply batchnorm
we first compute the mini-batch mean:</p>

<p>$$
\begin{equation}
\mu = \langle \mathbf{X} \rangle = \frac{1}{m} \sum_{i=1}^m \mathbf{X}_i,
\end{equation}
$$</p>

<p>where $\mathbf{X}_i$ is the $i$th feature-vector coming from the previous
layer; $i = 1 \dots m$, where $m &gt; 1$ is the batch size. We also obtain the
mini-batch variance:</p>

<p>$$
\begin{equation}
\text{var} = \frac{1}{m} \sum_{i=1}^m (\mathbf{X}_i - \mu)^2.
\end{equation}
$$</p>

<p>Now, the batchnorm's heart, normalization itself:</p>

<p>$$
\begin{equation}
\hat {\mathbf{X}_i} = \frac{\mathbf{X}_i - \mu}{\sqrt{\text{var} + \epsilon}},
\end{equation}
$$</p>

<p>where a small constant $\epsilon$ is added for numerical stability.  What if
normalization of the given layer was harmful? The algorithm provides two
learnable parameters that in the worst case scenario can undo the effect of
batch normalization: scaling parameter $\gamma$ and shift $\beta.$ After
(optionally) applying those, we obtain the output of the batchnorm layer:</p>

<p>$$
\begin{equation}
\mathbf{Y}_i = \gamma * \hat {\mathbf{X}_i} + \beta.
\end{equation}
$$</p>

<p>Please note that both mean $\mu$ and variance $\text{var}$ are vectors
of as many elements as neurons in a given hidden layer.
Operator $*$ denotes element-wise multiplication.</p>

<h3 id="inference">Inference</h3>

<p>In the inference phase it is perfectly normal to have one data sample at a
time. So how to calculate batch mean if the whole batch is a single sample? To
properly handle this, during training we estimate mean and variance
($E[\mathbf{X}]$ and $\text{Var}[\mathbf{X}]$) over all training
set. Those vectors will replace $\mu$ and $\text{var}$ during inference,
avoiding thus the problem of normalizing a singleton batch.</p>

<h2 id="how-efficient-is-batchnorm">How Efficient Is Batchnorm</h2>

<p><img src="/img/posts/mnist/mnist-five.png" alt="Handwritten digit" /></p>

<p>So far we have played with tasks that provided low-dimensional input features.
Now, we are going to test neural networks on a bit more interesting challenge.
We will apply our skills to automatically recognize human-written digits on a
famous <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>. This challenge came from the need to have
<a href="http://yann.lecun.com/exdb/publis/pdf/matan-92.pdf">zip-code machine reading</a>
for more efficient postal services.</p>

<p>We construct two neural networks, each having two fully-connected hidden layers
(300 and 50 neurons). Both networks receive $28 \times 28 = 784$ inputs, the
number of image pixels, and give back 10 outputs, the number of recognized
classes (digits).  As in-between layer activations we apply ReLU $f(x) =
\max(0,x)$.  To obtain the classification probabilities vector in the result,
we use <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> activation $\sigma(\mathbf{x})_i =
\frac{\exp{x_i}}{\sum_{j=1}\exp{x_j}}.$ One of the networks in addition
performs batch normalization before ReLUs.  Then, we train those using
stochastic gradient descent<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup> with learning rate $\alpha = 0.01$<sup class="footnote-ref" id="fnref:fn-3"><a href="#fn:fn-3">3</a></sup>
and batch size $m = 100$.</p>

<figure>

<img src="/img/posts/mnist/mnist-training.png" alt="Training with batchnorm (blue) leads to high accuracies faster than without batchnorm (orange)." width="450px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Neural network training on MNIST data.</h4>
  <p>
    Training with batchnorm (blue) leads to high accuracies faster than without batchnorm (orange).
    
    
    
  </p> 
</figcaption>

</figure>

<p>From the figure above we see that the neural network with batch
normalization reaches about $98\%$ accuracy in ten epochs, whereas the other
one is struggling to reach comparable performance in fifty epochs!
Similar results can be obtained for other architectures.</p>

<p>It is worth mentioning that we still may lack understanding how exactly does
batchnorm help. In its <a href="https://arxiv.org/abs/1502.03167">original paper</a>, it was hypothesized that
batchnorm is reducing internal covariate shift. Recently, <a href="https://arxiv.org/abs/1805.11604">it was
shown</a> that that is not necessary true. The best up-to-date
<a href="https://arxiv.org/abs/1805.11604">explanation</a> is that batchnorm makes optimization landscape
smooth, thus making gradient descent training more efficient. This, in its
turn, allows <a href="https://arxiv.org/abs/1502.03167">using higher learning rates</a> than without batchnorm!</p>

<h2 id="implementing-batchnorm">Implementing batchnorm</h2>

<p>We will base our effort on the code previously introduced on Day 2<sup class="footnote-ref" id="fnref:fn-4"><a href="#fn:fn-4">4</a></sup>.
First, we will redefined the <code>Layer</code> data structure making it more granular:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">data</span> <span style="color:#66d9ef">Layer</span> <span style="color:#a6e22e">a</span> <span style="color:#f92672">=</span> <span style="color:#75715e">-- Linear layer with weights and biases</span>
               <span style="color:#66d9ef">Linear</span> (<span style="color:#66d9ef">Matrix</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>)
               <span style="color:#75715e">-- Same as Linear, but without biases</span>
               <span style="color:#f92672">|</span> <span style="color:#66d9ef">Linear&#39;</span> (<span style="color:#66d9ef">Matrix</span> <span style="color:#a6e22e">a</span>)
               <span style="color:#75715e">-- Batchnorm with running mean, variance, and two</span>
               <span style="color:#75715e">-- learnable affine parameters</span>
               <span style="color:#f92672">|</span> <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>)
               <span style="color:#75715e">-- Usually non-linear element-wise activation</span>
               <span style="color:#f92672">|</span> <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">FActivation</span></code></pre></div>

<p>Amazing! Now we can distinguish between several kinds of layers: affine
(linear), activation, and batchnorm. Since batchnorm already compensates for a
bias, we do not actually need biases in the subsequent linear layers. That is
why we define a <code>Linear'</code> layer without biases. We also extend
<code>Gradients</code> to accommodate our new layers structure:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">data</span> <span style="color:#66d9ef">Gradients</span> <span style="color:#a6e22e">a</span> <span style="color:#f92672">=</span> <span style="color:#75715e">-- Weight and bias gradients</span>
                   <span style="color:#66d9ef">LinearGradients</span> (<span style="color:#66d9ef">Matrix</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>)
                   <span style="color:#75715e">-- Weight gradients</span>
                   <span style="color:#f92672">|</span> <span style="color:#66d9ef">Linear&#39;Gradients</span> (<span style="color:#66d9ef">Matrix</span> <span style="color:#a6e22e">a</span>)
                   <span style="color:#75715e">-- Batchnorm parameters and gradients</span>
                   <span style="color:#f92672">|</span> <span style="color:#66d9ef">BN1</span> (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>)
                   <span style="color:#75715e">-- No learnable parameters</span>
                   <span style="color:#f92672">|</span> <span style="color:#66d9ef">NoGrad</span></code></pre></div>

<p>Next, we want to extend the neural network propagation function <code>_pass</code>,
depending on the layer.  That is easy with <a href="http://learnyouahaskell.com/syntax-in-functions"><em>pattern
matching</em></a>.
Here is how we match the <code>Batchnorm1d</code> layer and its parameters:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">    <span style="color:#a6e22e">_pass</span> <span style="color:#a6e22e">inp</span> (<span style="color:#66d9ef">Batchnorm1d</span> <span style="color:#a6e22e">mu</span> <span style="color:#a6e22e">variance</span> <span style="color:#a6e22e">gamma</span> <span style="color:#a6e22e">beta</span><span style="color:#66d9ef">:</span><span style="color:#a6e22e">layers</span>)
        <span style="color:#f92672">=</span> (<span style="color:#a6e22e">dX</span>, <span style="color:#a6e22e">pred</span>, <span style="color:#66d9ef">BN1</span> <span style="color:#a6e22e">batchMu</span> <span style="color:#a6e22e">batchVariance</span> <span style="color:#a6e22e">dGamma</span> <span style="color:#a6e22e">dBeta</span><span style="color:#66d9ef">:</span><span style="color:#a6e22e">t</span>)
      <span style="color:#66d9ef">where</span></code></pre></div>

<p>As previously, the <code>_pass</code> function receives an input <code>inp</code> and layer
parameters. The second argument is the pattern we are matching against, making
our algorithm specific in this case for <code>(Batchnorm1D ...)</code>.  We will also
specify <code>_pass</code> for other kinds of <code>Layer</code>.  Thus, we have obtained a
<em>polymorphic</em> <code>_pass</code> function with respect to the layers.  Finally, the
equation results in a tuple of three: gradients to back propagate <code>dX</code>,
predictions <code>pred</code>, and prepended list <code>t</code> with values <code>BN1</code> computed in this
layer (batch mean <code>batchMu</code>, variance <code>batchVariance</code>, and learnable parameters
gradients).</p>

<p>The forward pass as illustrated in <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">this post</a>:
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">        <span style="color:#75715e">-- Forward</span>
        <span style="color:#a6e22e">eps</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-12</span>
        <span style="color:#a6e22e">b</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">br</span> (<span style="color:#a6e22e">rows</span> <span style="color:#a6e22e">inp</span>)  <span style="color:#75715e">-- Broadcast (replicate) rows from 1 to batch size</span>
        <span style="color:#a6e22e">m</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">recip</span> <span style="color:#f92672">$</span> (<span style="color:#a6e22e">fromIntegral</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">rows</span> <span style="color:#a6e22e">inp</span>)

        <span style="color:#75715e">-- Step 1: mean from Equation (1)</span>
        <span style="color:#a6e22e">batchMu</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Vector</span> <span style="color:#66d9ef">Float</span>
        <span style="color:#a6e22e">batchMu</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">m</span> `<span style="color:#a6e22e">_scale</span>` (<span style="color:#a6e22e">_sumRows</span> <span style="color:#a6e22e">inp</span>)

        <span style="color:#75715e">-- Step 2: mean subtraction</span>
        <span style="color:#a6e22e">xmu</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
        <span style="color:#a6e22e">xmu</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">inp</span> <span style="color:#f92672">.-</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">batchMu</span>

        <span style="color:#75715e">-- Step 3</span>
        <span style="color:#a6e22e">sq</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">xmu</span> <span style="color:#f92672">.^</span> <span style="color:#ae81ff">2</span>

        <span style="color:#75715e">-- Step 4: variance, Equation (2)</span>
        <span style="color:#a6e22e">batchVariance</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Vector</span> <span style="color:#66d9ef">Float</span>
        <span style="color:#a6e22e">batchVariance</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">m</span> `<span style="color:#a6e22e">_scale</span>` (<span style="color:#a6e22e">_sumRows</span> <span style="color:#a6e22e">sq</span>)

        <span style="color:#75715e">-- Step 5</span>
        <span style="color:#a6e22e">sqrtvar</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">sqrtA</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">batchVariance</span> `<span style="color:#a6e22e">addC</span>` <span style="color:#a6e22e">eps</span>

        <span style="color:#75715e">-- Step 6</span>
        <span style="color:#a6e22e">ivar</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">map</span> <span style="color:#a6e22e">recip</span> <span style="color:#a6e22e">sqrtvar</span>

        <span style="color:#75715e">-- Step 7: normalize, Equation (3)</span>
        <span style="color:#a6e22e">xhat</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">xmu</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">ivar</span>

        <span style="color:#75715e">-- Step 8: rescale</span>
        <span style="color:#a6e22e">gammax</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">gamma</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">xhat</span>

        <span style="color:#75715e">-- Step 9: translate, Equation (4)</span>
        <span style="color:#a6e22e">out0</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
        <span style="color:#a6e22e">out0</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">gammax</span> <span style="color:#f92672">.+</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">beta</span></code></pre></div></p>

<p>As discussed on <a href="/neural-networks/day2/">Day 2</a>,
there is a recurrent call obtaining gradients from the next layer,
neural network prediction <code>pred</code> and computed values tail <code>t</code>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">        (<span style="color:#a6e22e">dZ</span>, <span style="color:#a6e22e">pred</span>, <span style="color:#a6e22e">t</span>) <span style="color:#f92672">=</span> <span style="color:#a6e22e">_pass</span> <span style="color:#a6e22e">out</span> <span style="color:#a6e22e">layers</span></code></pre></div>

<p>I prefer to keep the backward pass without any simplifications.
That makes clear which step corresponds to which:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">        <span style="color:#75715e">-- Backward</span>

        <span style="color:#75715e">-- Step 9</span>
        <span style="color:#a6e22e">dBeta</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">_sumRows</span> <span style="color:#a6e22e">dZ</span>

        <span style="color:#75715e">-- Step 8</span>
        <span style="color:#a6e22e">dGamma</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">_sumRows</span> (<span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">dZ</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">xhat</span>)
        <span style="color:#a6e22e">dxhat</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
        <span style="color:#a6e22e">dxhat</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">dZ</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">gamma</span>

        <span style="color:#75715e">-- Step 7</span>
        <span style="color:#a6e22e">divar</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">_sumRows</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">dxhat</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">xmu</span>
        <span style="color:#a6e22e">dxmu1</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">dxhat</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">ivar</span>

        <span style="color:#75715e">-- Step 6</span>
        <span style="color:#a6e22e">dsqrtvar</span> <span style="color:#f92672">=</span> (<span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">map</span> (<span style="color:#a6e22e">negate</span><span style="color:#f92672">.</span> <span style="color:#a6e22e">recip</span>) (<span style="color:#a6e22e">sqrtvar</span> <span style="color:#f92672">.^</span> <span style="color:#ae81ff">2</span>)) <span style="color:#f92672">.*</span> <span style="color:#a6e22e">divar</span>

        <span style="color:#75715e">-- Step 5</span>
        <span style="color:#a6e22e">dvar</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span> `<span style="color:#a6e22e">_scale</span>` <span style="color:#a6e22e">ivar</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">dsqrtvar</span>

        <span style="color:#75715e">-- Step 4</span>
        <span style="color:#a6e22e">dsq</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">m</span> `<span style="color:#a6e22e">_scale</span>` <span style="color:#a6e22e">dvar</span>

        <span style="color:#75715e">-- Step 3</span>
        <span style="color:#a6e22e">dxmu2</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> `<span style="color:#a6e22e">_scale</span>` <span style="color:#a6e22e">xmu</span> <span style="color:#f92672">.*</span> <span style="color:#a6e22e">b</span> <span style="color:#a6e22e">dsq</span>

        <span style="color:#75715e">-- Step 2</span>
        <span style="color:#a6e22e">dx1</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">dxmu1</span> <span style="color:#f92672">.+</span> <span style="color:#a6e22e">dxmu2</span>
        <span style="color:#a6e22e">dmu</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">map</span> <span style="color:#a6e22e">negate</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">_sumRows</span> <span style="color:#a6e22e">dx1</span>

        <span style="color:#75715e">-- Step 1</span>
        <span style="color:#a6e22e">dx2</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">b</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">compute</span> (<span style="color:#a6e22e">m</span> `<span style="color:#a6e22e">_scale</span>` <span style="color:#a6e22e">dmu</span>)

        <span style="color:#a6e22e">dX</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">dx1</span> <span style="color:#f92672">.+</span> <span style="color:#a6e22e">dx2</span></code></pre></div>

<p>Note that often we need to perform operations like mean subtraction
$\mathbf{X} - \mathbf{\mu}$, where in practice we have a matrix
$\mathbf{X}$ and a vector $\mathbf{\mu}$.
How do you subtract a vector from a matrix?  Right,
<em>you don't</em>.  You can subtract only two matrices. Libraries like <code>Numpy</code> may
have a <a href="https://docs.scipy.org/doc/numpy/user/theory.broadcasting.html"><em>broadcasting</em></a> magic that would implicitly convert a
vector to a matrix<sup class="footnote-ref" id="fnref:fn-5"><a href="#fn:fn-5">5</a></sup>. This broadcasting might be useful, but might also
obscure different kinds of bugs. We instead perform explicit vector to matrix
transformations. For our convenience, we have a shortcut <code>b = br (rows inp)</code>
that will expand a vector to the same number of rows as in <code>inp</code>.
Where function <code>br</code> ('broadcast') is:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">br</span> <span style="color:#a6e22e">rows&#39;</span> <span style="color:#a6e22e">v</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">expandWithin</span> <span style="color:#66d9ef">Dim2</span> <span style="color:#a6e22e">rows&#39;</span> <span style="color:#a6e22e">const</span> <span style="color:#a6e22e">v</span></code></pre></div>

<p>Here is an example how <code>br</code> works. First, we start
an interactive Haskell session and load <code>NeuralNetwork.hs</code> module:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4">$ stack exec ghci
GHCi, version 8.2.2: http://www.haskell.org/ghc/  :? for help
Prelude&gt; :l src/NeuralNetwork.hs</pre></div>

<p>Then, we test <code>br</code> function on a vector <code>[1, 2, 3, 4]</code>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#f92672">*</span><span style="color:#66d9ef">NeuralNetwork</span><span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">a</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">fromList</span> <span style="color:#66d9ef">Par</span> [<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>,<span style="color:#ae81ff">4</span>] <span style="color:#f92672">::</span> <span style="color:#66d9ef">Vector</span> <span style="color:#66d9ef">Float</span>
<span style="color:#f92672">*</span><span style="color:#66d9ef">NeuralNetwork</span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">a</span>
<span style="color:#66d9ef">Array</span> <span style="color:#66d9ef">U</span> <span style="color:#66d9ef">Par</span> (<span style="color:#66d9ef">Sz1</span> <span style="color:#ae81ff">4</span>)
  [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]

<span style="color:#f92672">*</span><span style="color:#66d9ef">NeuralNetwork</span><span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">b</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">br</span> <span style="color:#ae81ff">3</span> <span style="color:#a6e22e">a</span>
<span style="color:#f92672">*</span><span style="color:#66d9ef">NeuralNetwork</span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">b</span>
<span style="color:#66d9ef">Array</span> <span style="color:#66d9ef">D</span> <span style="color:#66d9ef">Seq</span> (<span style="color:#66d9ef">Sz</span> (<span style="color:#ae81ff">3</span> <span style="color:#66d9ef">:.</span> <span style="color:#ae81ff">4</span>))
  [ [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]
  , [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]
  , [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]
  ]</code></pre></div>

<p>As we can see, a new matrix with three identical rows has been obtained.
Note that <code>a</code> has type <code>Array U Seq</code>, meaning that
data are stored in an unboxed array. Whereas the result is of type <code>Array D Seq</code>,
a so-called <em>delayed array</em>. This delayed array is not an actual
array, but rather a <em>promise to compute</em><sup class="footnote-ref" id="fnref:fn-6"><a href="#fn:fn-6">6</a></sup> an array in the future.
In order to obtain an actual array residing in memory, use <code>compute</code>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#f92672">*</span><span style="color:#66d9ef">NeuralNetwork</span><span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">compute</span> <span style="color:#a6e22e">b</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
<span style="color:#66d9ef">Array</span> <span style="color:#66d9ef">U</span> <span style="color:#66d9ef">Seq</span> (<span style="color:#66d9ef">Sz</span> (<span style="color:#ae81ff">3</span> <span style="color:#66d9ef">:.</span> <span style="color:#ae81ff">4</span>))
  [ [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]
  , [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]
  , [ <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">3.0</span>, <span style="color:#ae81ff">4.0</span> ]
  ]</code></pre></div>

<p>You will find more information about manipulating arrays in
<a href="http://hackage.haskell.org/package/massiv/docs/Data-Massiv-Array.html"><code>massiv</code> documentation</a>. Similarly to <code>br</code>, there exist several more convenience
functions, <code>rowsLike</code> and <code>colsLike</code>. Those are useful in conjunction with
<code>_sumRows</code> and <code>_sumCols</code>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#75715e">-- | Sum values in each column and produce a delayed 1D Array</span>
<span style="color:#a6e22e">_sumRows</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Array</span> <span style="color:#66d9ef">D</span> <span style="color:#66d9ef">Ix1</span> <span style="color:#66d9ef">Float</span>
<span style="color:#a6e22e">_sumRows</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">foldlWithin</span> <span style="color:#66d9ef">Dim2</span> (<span style="color:#f92672">+</span>) <span style="color:#ae81ff">0.0</span>

<span style="color:#75715e">-- | Sum values in each row and produce a delayed 1D Array</span>
<span style="color:#a6e22e">_sumCols</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Array</span> <span style="color:#66d9ef">D</span> <span style="color:#66d9ef">Ix1</span> <span style="color:#66d9ef">Float</span>
<span style="color:#a6e22e">_sumCols</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">foldlWithin</span> <span style="color:#66d9ef">Dim1</span> (<span style="color:#f92672">+</span>) <span style="color:#ae81ff">0.0</span></code></pre></div>

<p>Here is an example of <code>_sumCols</code> and <code>colsLike</code> when computing
<a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> activation
$\sigma(\mathbf{x})_i = \frac{\exp{x_i}}{\sum_{j=1}\exp{x_j}}$:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">softmax</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
<span style="color:#a6e22e">softmax</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">=</span>
  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">x0</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">expA</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
      <span style="color:#a6e22e">x1</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> (<span style="color:#a6e22e">_sumCols</span> <span style="color:#a6e22e">x0</span>) <span style="color:#f92672">::</span> <span style="color:#66d9ef">Vector</span> <span style="color:#66d9ef">Float</span>
      <span style="color:#a6e22e">x2</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">x1</span> `<span style="color:#a6e22e">colsLike</span>` <span style="color:#a6e22e">x</span>
  <span style="color:#66d9ef">in</span> (<span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">x0</span> <span style="color:#f92672">./</span> <span style="color:#a6e22e">x2</span>)</code></pre></div>

<p>Note that softmax is different from element-wise activations. Instead, softmax
acts as a fully-connected layer that receives a vector and outputs a vector.
Finally, we define our neural network with two hidden linear layers and batch
normalization as:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">net</span> <span style="color:#f92672">=</span> [ <span style="color:#66d9ef">Linear&#39;</span> <span style="color:#a6e22e">w1</span>
            , <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h1</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h1</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h1</span>) (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h1</span>)
            , <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">Relu</span>
            , <span style="color:#66d9ef">Linear&#39;</span> <span style="color:#a6e22e">w2</span>
            , <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h2</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h2</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h2</span>) (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h2</span>)
            , <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">Relu</span>
            , <span style="color:#66d9ef">Linear&#39;</span> <span style="color:#a6e22e">w3</span>
            ]</code></pre></div>

<p>The number of inputs is the total number of $28 \times 28 = 784$
image pixels and the number of outputs is the number of classes (ten digits).
We randomly generate the initial weights <code>w1</code>, <code>w2</code>, and <code>w3</code>.
And set initial batchnorm layer parameters as follows:
means to zeroes,
variances to ones,
scaling parameters to ones,
and translation parameters to zeroes:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#66d9ef">let</span> [<span style="color:#a6e22e">i</span>, <span style="color:#a6e22e">h1</span>, <span style="color:#a6e22e">h2</span>, <span style="color:#a6e22e">o</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">784</span>, <span style="color:#ae81ff">300</span>, <span style="color:#ae81ff">50</span>, <span style="color:#ae81ff">10</span>]
  (<span style="color:#a6e22e">w1</span>, <span style="color:#a6e22e">b1</span>) <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">genWeights</span> (<span style="color:#a6e22e">i</span>, <span style="color:#a6e22e">h1</span>)
  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">n</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">replicate</span> <span style="color:#66d9ef">Par</span> (<span style="color:#66d9ef">Sz1</span> <span style="color:#a6e22e">n</span>) <span style="color:#ae81ff">1</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Vector</span> <span style="color:#66d9ef">Float</span>
      <span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">n</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">replicate</span> <span style="color:#66d9ef">Par</span> (<span style="color:#66d9ef">Sz1</span> <span style="color:#a6e22e">n</span>) <span style="color:#ae81ff">0</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Vector</span> <span style="color:#66d9ef">Float</span>
  (<span style="color:#a6e22e">w2</span>, <span style="color:#a6e22e">b2</span>) <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">genWeights</span> (<span style="color:#a6e22e">h1</span>, <span style="color:#a6e22e">h2</span>)
  (<span style="color:#a6e22e">w3</span>, <span style="color:#a6e22e">b3</span>) <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">genWeights</span> (<span style="color:#a6e22e">h2</span>, <span style="color:#a6e22e">o</span>)</code></pre></div>

<p>Remember that the number of batchnorm parameters equals to the number of neurons.
It is a common practice to put batch normalization before activations, however
this sequence is not strict: one can put batch normalization after activations too.
For comparison, we also specify a neural network with two hidden layers without
batch normalization.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">net2</span> <span style="color:#f92672">=</span> [ <span style="color:#66d9ef">Linear</span> <span style="color:#a6e22e">w1</span> <span style="color:#a6e22e">b1</span>
             , <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">Relu</span>
             , <span style="color:#66d9ef">Linear</span> <span style="color:#a6e22e">w2</span> <span style="color:#a6e22e">b2</span>
             , <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">Relu</span>
             , <span style="color:#66d9ef">Linear</span> <span style="color:#a6e22e">w3</span> <span style="color:#a6e22e">b3</span>
             ]</code></pre></div>

<p>In both cases the output softmax activation is omitted as it is computed
together with loss gradients in the final recursive call in <code>_pass</code>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">    <span style="color:#a6e22e">_pass</span> <span style="color:#a6e22e">inp</span> <span style="color:#66d9ef">[]</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">loss&#39;</span>, <span style="color:#a6e22e">pred</span>, <span style="color:#66d9ef">[]</span>)
      <span style="color:#66d9ef">where</span>
        <span style="color:#a6e22e">pred</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">softmax</span> <span style="color:#a6e22e">inp</span>
        <span style="color:#a6e22e">loss&#39;</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span> <span style="color:#f92672">$</span> <span style="color:#a6e22e">pred</span> <span style="color:#f92672">.-</span> <span style="color:#a6e22e">tgt</span></code></pre></div>

<p>Here, <code>[]</code> on the left-hand side signifies an empty list of input layers,
and <code>[]</code> on the right-hand side is the empty tail of computed values in the
beginning of the backward pass.</p>

<p>The complete project is available on <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day4">Github</a>.
I recommend playing with different neuron networks architectures
and parameters. Have fun!</p>

<h2 id="batchnorm-pitfalls">Batchnorm Pitfalls</h2>

<p>There are several potential traps when using batchnorm. First,
batchnorm is different
<a href="https://github.com/penkovsky/10-days-of-grad/blob/389ee62a89a31946bab226731432345bdeaf3288/day4/src/NeuralNetwork.hs#L305">during training</a>
and
<a href="https://github.com/penkovsky/10-days-of-grad/blob/389ee62a89a31946bab226731432345bdeaf3288/day4/src/NeuralNetwork.hs#L355">during inference</a>.
That makes your implementation more complicated.
Second, batchnorm may <a href="https://www.alexirpan.com/2017/04/26/perils-batch-norm.html">fail</a> when training data come from different datasets.
To avoid the second pitfall, it is
essential to ensure that every batch represents the whole dataset, i.e. it has
data coming from the same distribution as the machine learning task you are
trying to solve. <a href="https://www.alexirpan.com/2017/04/26/perils-batch-norm.html">Read more</a> about that.</p>

<h2 id="summary">Summary</h2>

<p>Despite its pitfalls, batchnorm is an important concept and remains a popular
method in the context of deep neural networks. Batchnorm's power is that it can
substantially reduce the number of training epochs or even help achieving
better neural network accuracy. After discussing this, we are prepared for
hot subjects such as <a href="/neural-networks/day5/">convolutional neural networks</a>.
Stay tuned!</p>

<h2 id="further-reading">Further Reading</h2>

<h3 id="ai-and-neural-networks">AI And Neural Networks</h3>

<ul>
<li><a href="http://incompleteideas.net/IncIdeas/BitterLesson.html">Richard Sutton. The Bitter Lesson</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap1.html">Using neural nets to recognize handwritten digits</a></li>
<li><a href="https://arxiv.org/abs/1502.03167">Batch normalization paper</a></li>
<li><a href="https://arxiv.org/abs/1805.11604">How Does Batch Normalization Help Optimization?</a></li>
<li><a href="https://www.alexirpan.com/2017/04/26/perils-batch-norm.html">On The Perils of Batch Norm</a></li>
</ul>

<h3 id="haskell">Haskell</h3>

<ul>
<li><a href="http://www.philipzucker.com/why-i-as-of-june-22-2019-think-haskell-is-the-best-general-purpose-language-as-of-june-22-2019/">Why I think Haskell is the best general purpose language</a></li>
<li><a href="https://github.com/theindigamer/not-a-blog/blob/master/opinionated-haskell-guide-2019.md">An opinionated beginner's guide to Haskell in mid 2019</a></li>
<li><a href="http://hackage.haskell.org/package/massiv">Massiv Array Library</a></li>
<li><a href="https://www.youtube.com/watch?v=AAx2a0bUsxA">Alexey's presentation about massiv</a></li>
</ul>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">Most of the currently used neural network techniques date back to 80s. They became widely popular only now because of faster hardware and large datasets availability. The distinctively new methods are few: long short-term memory (1997), generative adversarial networks (2014), batchnorm (2015).
 <a class="footnote-return" href="#fnref:fn-1"><sup>^</sup></a></li>
<li id="fn:fn-2">The difference of stochastic gradient descent is that it approximates the true gradient by a gradient over a mini-batch.
 <a class="footnote-return" href="#fnref:fn-2"><sup>^</sup></a></li>
<li id="fn:fn-3">We have previously called learning rate $\gamma$. In this post we call learning rate $\alpha$ since $\gamma$ is a scaling parameter in the original <a href="https://arxiv.org/abs/1502.03167">batch normalization paper</a>.
 <a class="footnote-return" href="#fnref:fn-3"><sup>^</sup></a></li>
<li id="fn:fn-4">There are some technical differences, though. Namely, we use <a href="http://hackage.haskell.org/package/massiv">massiv</a> library supporting multidimensional arrays and parallelism. For the reference, <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/massiv">here</a> are the first two days reimplemented with <code>massiv</code>. Those multidimensional arrays are crucial when dealing with convolutional neural networks for image processing.
 <a class="footnote-return" href="#fnref:fn-4"><sup>^</sup></a></li>
<li id="fn:fn-5">In general, broadcasting is about making multidimensional arrays with different shapes compatible, not only vector to matrix translation.
 <a class="footnote-return" href="#fnref:fn-5"><sup>^</sup></a></li>
<li id="fn:fn-6">These delayed arrays are useful for computing optimizations such as <em>fusion</em>.
 <a class="footnote-return" href="#fnref:fn-6"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/deep-learning/">Deep Learning</a>
  
  <a class="label label-default" href="https://penkovsky.com/tags/haskell/">Haskell</a>
  
</div>




    
    <div class="article-widget">
      Next: <a href="https://penkovsky.com/neural-networks/day5/">Day 5: Convolutional Neural Networks Tutorial</a>
    </div>
    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/neural-networks/day3/">Day 3: Haskell Guide To Neural Networks</a></li>
        
        <li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
        
        <li><a href="/neural-networks/day1/">Day 1: Learning Neural Networks The Hard Way</a></li>
        
        <li><a href="/publication/stochastic-bnn/">Stochastic Computing for Hardware Implementation of Binarized Neural Networks</a></li>
        
        <li><a href="/talk/ictp2019/">Coupled Delay Systems For Brain-Inspired Computing</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; Bogdan Penkovsky 2025 &middot; 

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    
    

  </body>
</html>

