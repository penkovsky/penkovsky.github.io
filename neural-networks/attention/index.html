<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="The attention mechanism is the driving force behind modern neural network
architectures, such as transformers. Understanding attention is the first
step towards understand the inner workings of ChatGPT.">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/neural-networks/attention/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/neural-networks/attention/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/neural-networks/attention/">
  <meta property="og:title" content="Soft Actor-Critic with Attention | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="The attention mechanism is the driving force behind modern neural network
architectures, such as transformers. Understanding attention is the first
step towards understand the inner workings of ChatGPT."><meta property="og:image" content="https://penkovsky.com/img/posts/neural-networks/attention/policy-with-attention2.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2025-01-22T22:18:09&#43;01:00">
  
  <meta property="article:modified_time" content="2025-01-22T22:18:09&#43;01:00">
  

  

  

  <title>Soft Actor-Critic with Attention | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>Neural Networks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/posts/neural-networks/attention/policy-with-attention2.png" class="article-banner" itemprop="image">
  

  <span class="article-header-caption">Policy network with attention</span>
</div>



  <div class="article-container">

    <h1 itemprop="name">Soft Actor-Critic with Attention</h2>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2025-01-22 22:18:09 &#43;0100 CET" itemprop="datePublished">
    <time datetime="2025-01-22 22:18:09 &#43;0100 CET" itemprop="dateModified">
      Jan 22, 2025
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  

  
  
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>The <em>attention</em> mechanism is the driving force behind modern neural network
architectures, such as <a href="https://arxiv.org/abs/1706.03762">transformers</a>. Understanding attention is the first
step towards understand the inner workings of ChatGPT.</p>

<hr />

<p>The accompanying code is available on <a href="https://github.com/penkovsky/attention">Github</a>.</p>

<hr />

<h2 id="attention-mechanism-basics">Attention Mechanism Basics</h2>

<p>There are several variations on the attention mechanism, yet they are
conceptually similar. Here we consider a scaled dot-product
<a href="https://arxiv.org/abs/1706.03762">attention</a> that is used in transformers.</p>

<p>$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
$$</p>

<p>where $Q := W^{(Q)} Q_0$, $K := W^{(K)} K_0$, $V := W^{(V)} V_0$.
Tensors $Q_0$, $K_0$, and $V_0$ are conventionally called <em>queries</em>, <em>keys</em>, and
<em>values</em> (in our case they all are equal to the input vector $\mathbf{x}$ cast
as a matrix). Matrices $W^{(i)}$ are trainable linear transformations. For
convenience, I omit biases assuming that input (feature) vectors are augmented
with a constant feature. The first layer's attention mechanism is illustrated
schematically in the cover image.</p>

<p>The output of the <em>softmax</em> transformation is known as <em>attention weights</em>.
Those are created dynamically from the context (the input). Those
weights will be applied to transform values $V$.
Note that the output vector is a linear combination of the input features.
Last but not least, the input scaling factor $\sqrt{d_k}$
is used to deal with exponentially small gradients of <em>softmax</em> for large inputs.
The logic behind this constant: if we assume that the elements of query and key
vectors are independent random variables with zero mean and unit variance,
the variance of the dot product is $d_k$. Hence dividing by $\sqrt d_k$,
the standard deviation.</p>

<h2 id="visualizing-attention">Visualizing Attention</h2>

<p>Let us take an example from natural language processing. In order to interpret a sentence,
one needs to correlate the words in it. Sometimes, the related words are not neighbors.
This is the <a href="https://arxiv.org/abs/1409.0473">initial motivation</a> of the attention mechanism,
without using recurrent neural networks.
To illustrate the concept, we reproduce a figure from the transformer <a href="https://arxiv.org/abs/1706.03762">paper</a>.</p>

<figure>

<img src="/img/posts/neural-networks/attention/Vaswani2017_Fig5.png" alt="Visualizing attention from the [transformer article](https://arxiv.org/pdf/1706.03762)." width="700px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4></h4>
  <p>
    Visualizing attention from the <a href="https://arxiv.org/pdf/1706.03762">transformer article</a>.
    
    
    
  </p> 
</figcaption>

</figure>

<p>The visualization might remind of fully connected layers. However, there is a
difference. Attention weights adjust dynamically, depending on the actual
input! Also attention is non-negative because of softmax transformation.</p>

<h2 id="benchmarks">Benchmarks</h2>

<h3 id="lunar-lander">Lunar Lander</h3>

<figure>

<img src="/img/posts/neural-networks/attention/lunar.png" alt="Lunar lander environment: The goal is to safely land on the flat area between the flags." width="600px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4></h4>
  <p>
    Lunar lander environment: The goal is to safely land on the flat area between the flags.
    
    
    
  </p> 
</figcaption>

</figure>

<p>We train an AI agent to land on the lunar surface using
<a href="/neural-networks/beyond/">deep reinforcement learning</a> approach.
We will solve the <a href="https://gymnasium.farama.org/environments/box2d/lunar_lander/">Lunar lander</a> <a href="https://gymnasium.farama.org">Gymnasium</a>
environment. The agent will learn to control a space vehicle in a 2D setting.
The agent observes eight environmental features: X and Y coordinates of the
lander, X and Y velocities, lander's angle and angular velocity, and two flags
indicating whether each lander's leg is in contact with the ground. We would
like to see how the agent attends to these features in order to get some
insights what it has learned.</p>

<figure>

<img src="/img/posts/neural-networks/attention/policy-with-attention.png" alt="Policy network with attention: attention layer and a linear layer." width="700px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4></h4>
  <p>
    Policy network with attention: attention layer and a linear layer.
    
    
    
  </p> 
</figcaption>

</figure>

<p>Let us consider a simple scenario: a two-layer neural network, i.e. a network
with a single hidden layer. This network will represent the control policy,
that is, it will transform the observations from the environment into
actions. The network will consist of an <em>attention</em> layer and a linear layer.
We will use <a href="https://spinningup.openai.com/en/latest/algorithms/sac.html">soft actor-critic</a> for training, but many other
algorithms would do. Then, once we have trained the agent, we will visualize
how attention weights change during an episode. To push things even further,
we will consider the attention layer with only one attention <a href="https://arxiv.org/abs/1706.03762">head</a>.
Will this be enough?</p>



<video width="600" height="700" autoplay loop controls>
  <source src="/vid/LunarLanderContinuous-v3__sac_continuous_action__3__1737753957.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>


<p align="right">
<small><a href="/vid/LunarLanderContinuous-v3__sac_continuous_action__3__1737753957.mp4" title="Download">&#x1f4e5;&#xfe0e; Video</a></small>
</p>



<video width="600" height="700" autoplay loop controls>
  <source src="/vid/LunarLanderContinuous-v3__sac_continuous_action__6__1737753916.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>


<p align="right">
<small><a href="/vid/LunarLanderContinuous-v3__sac_continuous_action__6__1737753916.mp4" title="Download">&#x1f4e5;&#xfe0e; Video</a></small>
</p>

<p>By observing the evolution of the attention weights during the episode,
we may suggest that the agent has learned several distinctive phases.</p>

<p>The <strong>first phase</strong> when the lander is high, the agent pays special
attention to its vertical position Y. While the lander descends,
the agent's attention gradually changes.</p>

<p>During the <strong>second phase</strong> when the lander is close to the ground,
the agent attends also to the horizontal velocity and angular velocity omega.
This information can be useful to learn how land vertically.</p>

<p>During the <strong>final phase</strong>, when the ground contact flags are high,
the agent pays significant attention to lander's angle.
This state usually precedes the end of the episode.</p>

<h2 id="bipedal-walker">Bipedal Walker</h2>

<p>It might interesting to look how our attention-enabled architecture performs on
other tasks. <a href="https://gymnasium.farama.org/environments/box2d/bipedal_walker/">Bipedal Walker</a> is yet another 2D Gymnasium
environment. This time, the observation space contains information about 24
variables:
hull angle speed, angular velocity, horizontal speed, vertical speed, position
of joints and joints angular speed, legs contact with ground, and 10 lidar
rangefinder measurements.</p>



<video width="600" height="700" autoplay loop controls>
  <source src="/vid/BipedalWalker-v3__sac_continuous_action__2__1737753153.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

<p align="right">
<small><a href="/vid/BipedalWalker-v3__sac_continuous_action__2__1737753153.mp4" title="Download">&#x1f4e5;&#xfe0e; Video</a></small>
</p>


<p>We see that attention is periodically switched. This is likely related to the
agent's walking pattern (left leg? right leg?).</p>

<h2 id="from-attention-to-transformer">From Attention To Transformer</h2>

<p>It seems somewhat surprising that a policy network with very few
hidden neurons was sufficient to learn an efficient control strategy.
On the other hand, this hints on the power of the attention mechanism as it is<sup class="footnote-ref" id="fnref:fn-note"><a href="#fn:fn-note">1</a></sup>.
In the actual transformer architecture, there are implemented many other tricks to
teach the neural network some very impressive feats. For instance, instead of a
single attention mechanism in each layer there are several of them (so-called
<em>heads</em>) analyzing information in parallel. <em>Layer normalization</em> is used
to improve training efficiency. Position information is encoded
using positional encodings (e.g. sine and cosine functions). To enforce
causality, masking is performed (to prevent from attending to future word
tokens during training). These are well described in
<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>.</p>

<h2 id="citation">Citation</h2>

<pre>
@article{penkovsky2025attn,
 title   = "Soft Actor-Critic with Attention",
 author  = "Penkovsky, Bogdan",
 journal = "penkovsky.com",
 year    = "2025",
 month   = "January",
 url     = "https://penkovsky.com/neural-networks/attention/"
}
</pre>

<h2 id="learn-more">Learn More</h2>

<ul>
<li><a href="https://spinningup.openai.com/en/latest/algorithms/sac.html">Soft Actor-Critic</a></li>
<li><a href="https://arxiv.org/pdf/1409.0473">Neural machine translation by jointly learning to align and translate</a></li>
<li><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need (transformer paper)</a></li>
<li><a href="https://www.bishopbook.com">Deep Learning - Foundations and Concepts</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-note">A bit more work is required to show this more rigorously: Train several baseline linear models and compare the average rewards over several episodes with average rewards given by models with attention.
 <a class="footnote-return" href="#fnref:fn-note"><sup>^</sup></a></li>
</ol>
</div>
    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/deep-learning/">Deep Learning</a>
  
</div>




    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/neural-networks/meta-learning/">Meta-Learning</a></li>
        
        <li><a href="/neural-networks/beyond/">Day 10: Beyond Supervised Learning</a></li>
        
        <li><a href="/neural-networks/day9/">Day 9: Roaming The Latent Space</a></li>
        
        <li><a href="/neural-networks/day8/">Day 8: Model Uncertainty Estimation</a></li>
        
        <li><a href="/neural-networks/day7/">Day 7: Real World Deep Learning</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; Bogdan Penkovsky 2025 &middot; 

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    
    

  </body>
</html>

