<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="Now that we have seen how neural networks work, we realize that understanding of the gradients flow is essential for survival. Therefore, we will revise our strategy on the lowest level. However, as neural networks become more complicated, calculation of gradients by hand becomes a murky business. Yet, fear not young padawan, there is a way out! I am very excited that today we will finally get acquainted with automatic differentiation, an essential tool in your deep learning arsenal.">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/neural-networks/day3/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/neural-networks/day3/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/neural-networks/day3/">
  <meta property="og:title" content="Day 3: Haskell Guide To Neural Networks | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="Now that we have seen how neural networks work, we realize that understanding of the gradients flow is essential for survival. Therefore, we will revise our strategy on the lowest level. However, as neural networks become more complicated, calculation of gradients by hand becomes a murky business. Yet, fear not young padawan, there is a way out! I am very excited that today we will finally get acquainted with automatic differentiation, an essential tool in your deep learning arsenal.">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-02-17T01:10:00&#43;02:00">
  
  <meta property="article:modified_time" content="2019-02-17T01:10:00&#43;02:00">
  

  

  

  <title>Day 3: Haskell Guide To Neural Networks | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>Neural Networks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  


  <div class="article-container">

    <h1 itemprop="name">Day 3: Haskell Guide To Neural Networks</h2>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2019-02-17 01:10:00 &#43;0200 &#43;0200" itemprop="datePublished">
    <time datetime="2019-02-17 01:10:00 &#43;0200 &#43;0200" itemprop="dateModified">
      Feb 17, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    8 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://penkovsky.com/categories/10-days-of-grad/">10 Days Of Grad</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>Now that we have seen how neural networks work, we realize that understanding
of the gradients flow is essential for survival.  Therefore, we will revise
our strategy on the lowest level. However, as neural networks become more complicated,
calculation of gradients by hand becomes a murky business. Yet, fear not young
<em>padawan</em>, there is a way out! I am very excited that today we will finally get
acquainted with automatic differentiation, an essential tool in your deep
learning arsenal.  This post was largely inspired by <a href="http://karpathy.github.io/neuralnets/">Hacker's guide to Neural
Networks</a>. For comparison, see also
<a href="https://github.com/urwithajit9/HG_NeuralNetwork">Python version</a>.</p>

<p>Before jumping ahead, you may also want to check the previous posts:</p>

<ul>
<li><a href="/neural-networks/day1/">Day 1: Learning Neural Networks The Hard Way</a></li>
<li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
</ul>

<p>The source code from this guide is available <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day3">on
Github</a>. The
guide is written in literate Haskell, so it can be safely compiled.</p>

<h2 id="why-random-local-search-fails">Why Random Local Search Fails</h2>

<p>Following Karpathy's <a href="http://karpathy.github.io/neuralnets/">guide</a>, we
first consider a simple multiplication circuit.  Well, Haskell is not JavaScript,
so the definition is pretty straightforward:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">forwardMultiplyGate</span> <span style="color:#f92672">=</span> (<span style="color:#f92672">*</span>)</code></pre></div>

<p>Or we could have written</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">forwardMultiplyGate</span> <span style="color:#a6e22e">x</span> <span style="color:#a6e22e">y</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">y</span></code></pre></div>

<p>to make the function look more intuitively $f(x,y) = x \cdot y$. Anyway,</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">forwardMultiplyGate</span> (<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>) <span style="color:#ae81ff">3</span></code></pre></div>

<p>returns -6. Exciting.</p>

<p>Now, the question: is it possible to change the input $(x,y)$ slightly in order
to increase the output?  One way would be to perform local random search.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">_search</span> <span style="color:#a6e22e">tweakAmount</span> (<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">bestOut</span>) <span style="color:#f92672">=</span> <span style="color:#66d9ef">do</span>
  <span style="color:#a6e22e">x_try</span> <span style="color:#f92672">&lt;-</span> (<span style="color:#a6e22e">x</span> <span style="color:#f92672">+</span> )<span style="color:#f92672">.</span> (<span style="color:#a6e22e">tweakAmount</span> <span style="color:#f92672">*</span>) <span style="color:#f92672">&lt;$&gt;</span> <span style="color:#a6e22e">randomDouble</span>
  <span style="color:#a6e22e">y_try</span> <span style="color:#f92672">&lt;-</span> (<span style="color:#a6e22e">y</span> <span style="color:#f92672">+</span> )<span style="color:#f92672">.</span> (<span style="color:#a6e22e">tweakAmount</span> <span style="color:#f92672">*</span>) <span style="color:#f92672">&lt;$&gt;</span> <span style="color:#a6e22e">randomDouble</span>
  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">out</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">forwardMultiplyGate</span> <span style="color:#a6e22e">x_try</span> <span style="color:#a6e22e">y_try</span>
  <span style="color:#a6e22e">return</span> <span style="color:#f92672">$</span> <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">out</span> <span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">bestOut</span>
               <span style="color:#66d9ef">then</span> (<span style="color:#a6e22e">x_try</span>, <span style="color:#a6e22e">y_try</span>, <span style="color:#a6e22e">out</span>)
               <span style="color:#66d9ef">else</span> (<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">bestOut</span>)</code></pre></div>

<p>Not surprisingly, the function above represents a single iteration of a
&quot;for&quot;-loop. What it does, it randomly selects points around initial $(x, y)$
and checks if the output has increased.  If yes, then it updates the best known
inputs and the maximal output. To iterate, we can use <code>foldM :: (b -&gt; a -&gt; IO
b) -&gt; b -&gt; [a] -&gt; IO b</code>. This function is convenient since we anticipate some
interaction with the &quot;external world&quot; in the form of random numbers generation:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">localSearch</span> <span style="color:#a6e22e">tweakAmount</span> (<span style="color:#a6e22e">x0</span>, <span style="color:#a6e22e">y0</span>, <span style="color:#a6e22e">out0</span>) <span style="color:#f92672">=</span>
 <span style="color:#a6e22e">foldM</span> (<span style="color:#a6e22e">searchStep</span> <span style="color:#a6e22e">tweakAmount</span>) (<span style="color:#a6e22e">x0</span>, <span style="color:#a6e22e">y0</span>, <span style="color:#a6e22e">out0</span>) [<span style="color:#ae81ff">1</span><span style="color:#f92672">..</span><span style="color:#ae81ff">100</span>]</code></pre></div>

<p>What the code essentially tells us is that we seed the algorithm with some
initial values of <code>x0</code>, <code>y0</code>, and <code>out0</code> and iterate from 1 till 100.  The core
of the algorithm is <code>searchStep</code>:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">searchStep</span> <span style="color:#a6e22e">ta</span> <span style="color:#a6e22e">xyz</span> <span style="color:#66d9ef">_</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">_search</span> <span style="color:#a6e22e">ta</span> <span style="color:#a6e22e">xyz</span></code></pre></div>

<p>which is a convenience function that glues those two pieces together.  It
simply ignores the iteration number and calls <code>_search</code>. Now, we would like to
have a random number generator within the range of [-1; 1). From the
<a href="http://hackage.haskell.org/package/random-1.1/docs/System-Random.html">documentation</a>,
we know that <code>randomIO</code> produces a number between 0 and 1.  Therefore, we scale
the value by multiplying by 2 and subtracting 1:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">randomDouble</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">IO</span> <span style="color:#66d9ef">Double</span>
<span style="color:#a6e22e">randomDouble</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">subtract</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">.</span> (<span style="color:#f92672">*</span><span style="color:#ae81ff">2</span>) <span style="color:#f92672">&lt;$&gt;</span> <span style="color:#a6e22e">randomIO</span></code></pre></div>

<p>The <code>&lt;$&gt;</code> function is a synonym to <code>fmap</code>.  What it essentially does is
attaching the pure function <code>subtract 1. (*2)</code> which has type <code>Double -&gt;
Double</code>, to the &quot;external world&quot; action <code>randomIO</code>, which has type <code>IO Double</code>
(yes, IO = input/output)<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup>.</p>

<p>A hack for a numerical minus infinity:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">inf_</span> <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">0</span></code></pre></div>

<p>Now, we run <code>localSearch 0.01 (-2, 3, inf_)</code> several times:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">(<span style="color:#f92672">-</span><span style="color:#ae81ff">1.7887454910045664</span>,<span style="color:#ae81ff">2.910160042416705</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">5.205535653974539</span>)
(<span style="color:#f92672">-</span><span style="color:#ae81ff">1.7912166830200635</span>,<span style="color:#ae81ff">2.89808308735154</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">5.19109477484237</span>)
(<span style="color:#f92672">-</span><span style="color:#ae81ff">1.8216809458018006</span>,<span style="color:#ae81ff">2.8372869694452523</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">5.168631610010152</span>)</code></pre></div>

<p>In fact, we see that the outputs have increased from -6 to about -5.2.
But the improvement is only about 0.8/100 = 0.008 units per iteration.
That is an extremely inefficient method. The problem with random
search is that each time it attempts to change the inputs in random
directions. If the algorithm makes a mistake, it has to discard
the result and start again from the previously known best position.
Wouldn't it be nice if instead each iteration would improve the result
at least by a little bit?</p>

<h2 id="automatic-differentiation">Automatic Differentiation</h2>

<p>Instead of random search in random direction, we can make use of the precise
direction and amount to change the input so that the output would improve. And
that is exactly what the <a href="https://en.wikipedia.org/wiki/Gradient">gradient</a>
tells us.  Instead of manually computing the gradient every time, we can employ
some clever algorithm. There exist multiple approaches: numerical, symbolic,
and automatic differentiation.  In his
<a href="https://idontgetoutmuch.wordpress.com/2013/10/13/backpropogation-is-just-steepest-descent-with-automatic-differentiation-2/">article</a>,
Dominic Steinitz explains the differences between them.  The last approach,
automatic differentiation is exactly what we need: accurate gradients with
minimal overhead.  Here, we will briefly explain the concept.</p>

<p>The idea behind automatic differentiation is that we explicitly define
gradients only for elementary, basic operators. Then, we exploit the
<a href="https://en.wikipedia.org/wiki/Chain_rule">chain rule</a> combining those
operators into neural networks or whatever we like. That strategy will infer
the necessary gradients by itself. Let us illustrate the method with an
example.</p>

<p>Below we define both multiplication operator and its gradient using the chain
rule, i.e. $\frac {d} {dt} x(t) y(t) = x(t) y'(t) + x'(t) y(t)$:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">(<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">x&#39;</span>) <span style="color:#f92672">*.</span> (<span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">y&#39;</span>) <span style="color:#f92672">=</span> (<span style="color:#a6e22e">x</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">x</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">y&#39;</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">x&#39;</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">y</span>)</code></pre></div>

<p>The same can be done with addition, subtraction, division, and exponent:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">(<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">x&#39;</span>) <span style="color:#f92672">+.</span> (<span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">y&#39;</span>) <span style="color:#f92672">=</span> (<span style="color:#a6e22e">x</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">x&#39;</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">y&#39;</span>)

<span style="color:#a6e22e">x</span> <span style="color:#f92672">-.</span> <span style="color:#a6e22e">y</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">+.</span> (<span style="color:#a6e22e">negate1</span> <span style="color:#a6e22e">y</span>)

<span style="color:#a6e22e">negate1</span> (<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">x&#39;</span>) <span style="color:#f92672">=</span> (<span style="color:#a6e22e">negate</span> <span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">negate</span> <span style="color:#a6e22e">x&#39;</span>)

(<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">x&#39;</span>) <span style="color:#f92672">/.</span> (<span style="color:#a6e22e">y</span>, <span style="color:#a6e22e">y&#39;</span>) <span style="color:#f92672">=</span> (<span style="color:#a6e22e">x</span> <span style="color:#f92672">/</span> <span style="color:#a6e22e">y</span>, (<span style="color:#a6e22e">y</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">x&#39;</span> <span style="color:#f92672">-</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">y&#39;</span>) <span style="color:#f92672">/</span> <span style="color:#a6e22e">y</span><span style="color:#f92672">^</span><span style="color:#ae81ff">2</span>)

<span style="color:#a6e22e">exp1</span> (<span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">x&#39;</span>) <span style="color:#f92672">=</span> (<span style="color:#a6e22e">exp</span> <span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">x&#39;</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">exp</span> <span style="color:#a6e22e">x</span>)</code></pre></div>

<p>We also have <code>constOp</code> for constants:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">constOp</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Double</span> <span style="color:#f92672">-&gt;</span> (<span style="color:#66d9ef">Double</span>, <span style="color:#66d9ef">Double</span>)
<span style="color:#a6e22e">constOp</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">=</span> (<span style="color:#a6e22e">x</span>, <span style="color:#ae81ff">0.0</span>)</code></pre></div>

<p>Finally, we can define our favourite sigmoid $\sigma(x)$
combining the operators above:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">sigmoid1</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">constOp</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/.</span> (<span style="color:#a6e22e">constOp</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">+.</span> <span style="color:#a6e22e">exp1</span> (<span style="color:#a6e22e">negate1</span> <span style="color:#a6e22e">x</span>))</code></pre></div>

<p>Now, let us compute a neuron $f(x, y) = \sigma(a x + b y + c)$, where $x$
and $y$ are inputs and $a$, $b$, and $c$ are parameters</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">neuron1</span> [<span style="color:#a6e22e">a</span>, <span style="color:#a6e22e">b</span>, <span style="color:#a6e22e">c</span>, <span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">y</span>] <span style="color:#f92672">=</span> <span style="color:#a6e22e">sigmoid1</span> ((<span style="color:#a6e22e">a</span> <span style="color:#f92672">*.</span> <span style="color:#a6e22e">x</span>) <span style="color:#f92672">+.</span> (<span style="color:#a6e22e">b</span> <span style="color:#f92672">*.</span> <span style="color:#a6e22e">y</span>) <span style="color:#f92672">+.</span> <span style="color:#a6e22e">c</span>)</code></pre></div>

<p>Now, we can obtain the gradient of <code>a</code> in the point
where $a = 1$, $b = 2$, $c = -3$, $x = -1$, and $y = 3$:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">abcxy1</span> <span style="color:#f92672">::</span> [(<span style="color:#66d9ef">Double</span>, <span style="color:#66d9ef">Double</span>)]
<span style="color:#a6e22e">abcxy1</span> <span style="color:#f92672">=</span> [(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)]</code></pre></div>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">neuron1</span> <span style="color:#a6e22e">abcxy1</span>
(<span style="color:#ae81ff">0.8807970779778823</span>,<span style="color:#f92672">-</span><span style="color:#ae81ff">0.1049935854035065</span>)</code></pre></div>

<p>Here, the first number is the result of the neuron's output and the second
one is the gradient with respect to <code>a</code> ($\frac d {da}$). Let us verify the
math behind the result:</p>

<p>$$
\begin{equation}
\sigma(ax + by + c) | _{a=(a,1), b=(b,0), c=(c,0), x=(x,0), y=(y,0)} = \\<br />
\sigma[(a, 1) (x, 0) + (b, 0) (y, 0) + (c, 0)] = \\<br />
\sigma[(ax, a \cdot 0 + 1 \cdot x) + (by, 0 \cdot b + 0 \cdot y) + (c, 0)] = \\<br />
\sigma[(ax + by + c, x)] = \\<br />
\frac {(1, 0)} {(1, 0) + \exp \left[ -(ax + by + c, x) \right]} = \\<br />
\frac {(1, 0)} {(1, 0) + \exp \left[ -ax - by - c, -x) \right]} = \\<br />
\frac {(1, 0)} {(1, 0) + (\exp (-ax - by - c), -x \exp (-ax - by - c))} = \\<br />
\frac {(1, 0)} {(1 + \exp(-ax - by - c), -x \exp(-ax - by - c))} = \\<br />
\left( \sigma(ax + by + c), \frac {x \exp(-ax - by -c)} {(1 + \exp(-ax - by -c))^2} \right).
\end{equation}
$$</p>

<p>The first expression is the result of neuron's computation
and the second one is the exact analytic expression
for $\frac d {da}$. That is all the magic behind
automatic differentiation!
In a similar way, we can obtain the rest of the gradients:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">neuron1</span> [(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)]
(<span style="color:#ae81ff">0.8807970779778823</span>,<span style="color:#ae81ff">0.3149807562105195</span>)

<span style="color:#a6e22e">neuron1</span> [(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)]
(<span style="color:#ae81ff">0.8807970779778823</span>,<span style="color:#ae81ff">0.1049935854035065</span>)

<span style="color:#a6e22e">neuron1</span> [(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)]
(<span style="color:#ae81ff">0.8807970779778823</span>,<span style="color:#ae81ff">0.1049935854035065</span>)

<span style="color:#a6e22e">neuron1</span> [(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">1</span>)]
(<span style="color:#ae81ff">0.8807970779778823</span>,<span style="color:#ae81ff">0.209987170807013</span>)</code></pre></div>

<h2 id="introducing-backprop-library">Introducing backprop library</h2>

<p>The <a href="https://backprop.jle.im/">backprop library</a> was specifically designed for
<a href="https://www.quora.com/What-is-Differentiable-Programming">differentiable
programming</a>.  It
provides combinators to reduce our mental overhead.  In addition, the most
useful operations such as arithmetics and trigonometry, have already been
defined in the library. See also
<a href="http://hackage.haskell.org/package/hmatrix-backprop">hmatrix-backprop</a> for
linear algebra.  So all you need for differentiable programming now is to
define some functions:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">neuron</span>
  <span style="color:#f92672">::</span> <span style="color:#66d9ef">Reifies</span> <span style="color:#a6e22e">s</span> <span style="color:#66d9ef">W</span>
  <span style="color:#f92672">=&gt;</span> [<span style="color:#66d9ef">BVar</span> <span style="color:#a6e22e">s</span> <span style="color:#66d9ef">Double</span>] <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">BVar</span> <span style="color:#a6e22e">s</span> <span style="color:#66d9ef">Double</span>
<span style="color:#a6e22e">neuron</span> [<span style="color:#a6e22e">a</span>, <span style="color:#a6e22e">b</span>, <span style="color:#a6e22e">c</span>, <span style="color:#a6e22e">x</span>, <span style="color:#a6e22e">y</span>] <span style="color:#f92672">=</span> <span style="color:#a6e22e">sigmoid</span> (<span style="color:#a6e22e">a</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">b</span> <span style="color:#f92672">*</span> <span style="color:#a6e22e">y</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">c</span>)

<span style="color:#a6e22e">sigmoid</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">+</span> <span style="color:#a6e22e">exp</span> (<span style="color:#f92672">-</span><span style="color:#a6e22e">x</span>))</code></pre></div>

<p>Here <code>BVar s</code> wrapper signifies that our function is
differentiable. Now, the forward pass is:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">forwardNeuron</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">BP</span><span style="color:#f92672">.</span><span style="color:#a6e22e">evalBP</span> (<span style="color:#a6e22e">neuron</span><span style="color:#f92672">.</span> <span style="color:#66d9ef">BP</span><span style="color:#f92672">.</span><span style="color:#a6e22e">sequenceVar</span>)</code></pre></div>

<p>We use <code>sequenceVar</code> isomorphism to convert a <code>BVar</code> of a list into a list of
<code>BVar</code>s, as required by our <code>neuron</code> equation. And the backward pass is</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">backwardNeuron</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">BP</span><span style="color:#f92672">.</span><span style="color:#a6e22e">gradBP</span> (<span style="color:#a6e22e">neuron</span><span style="color:#f92672">.</span> <span style="color:#66d9ef">BP</span><span style="color:#f92672">.</span><span style="color:#a6e22e">sequenceVar</span>)

<span style="color:#a6e22e">abcxy0</span> <span style="color:#f92672">::</span> [<span style="color:#66d9ef">Double</span>]
<span style="color:#a6e22e">abcxy0</span> <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, (<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>), (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), <span style="color:#ae81ff">3</span>]

<span style="color:#a6e22e">forwardNeuron</span> <span style="color:#a6e22e">abcxy0</span>
<span style="color:#75715e">-- 0.8807970779778823</span>

<span style="color:#a6e22e">backwardNeuron</span> <span style="color:#a6e22e">abcxy0</span>
<span style="color:#75715e">-- [-0.1049935854035065,0.3149807562105195,0.1049935854035065,0.1049935854035065,0.209987170807013]</span></code></pre></div>

<p>Note that all the gradients are in one list, the type of the first <code>neuron</code>
argument.</p>

<h2 id="summary">Summary</h2>

<p>Modern neural networks tend to be complex beasts.  Writing backpropagation
gradients by hand can easily become a tedious task.  In this post we have seen
how automatic differentiation can face this problem.</p>

<p>In the next posts we will apply automatic differentiation to real neural
networks. We will talk about
<a href="/neural-networks/day4/">batch normalization</a>,
another crucial method in modern deep learning.  And we will ramp it up to
<a href="/neural-networks/day5/">convolutional networks</a> allowing us
to solve some interesting challenges.
Stay tuned!</p>

<h2 id="further-reading">Further reading</h2>

<ul>
<li><a href="https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/">Visual guide to neural networks</a></li>
<li><a href="https://backprop.jle.im/01-getting-started.html">Backprop documentation</a></li>
<li><a href="https://idontgetoutmuch.wordpress.com/2013/10/13/backpropogation-is-just-steepest-descent-with-automatic-differentiation-2/">Article on backpropagation by Dominic Steinitz</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">In fact, 64 bit double precision is not necessary for neural networks, if not an overkill. In practice you would prefer to use a 32 bit <code>Float</code> type.
 <a class="footnote-return" href="#fnref:fn-1"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/deep-learning/">Deep Learning</a>
  
  <a class="label label-default" href="https://penkovsky.com/tags/haskell/">Haskell</a>
  
</div>




    
    <div class="article-widget">
      Next: <a href="https://penkovsky.com/neural-networks/day4/">Day 4: The Importance Of Batch Normalization</a>
    </div>
    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
        
        <li><a href="/neural-networks/day1/">Day 1: Learning Neural Networks The Hard Way</a></li>
        
        <li><a href="/talk/icee2018/">Towards Binarized Neural Networks Hardware</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; Bogdan Penkovsky 2025 &middot; 

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    
    

  </body>
</html>

