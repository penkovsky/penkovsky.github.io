<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="Last week Apple has acquired XNOR.ai startup for amazing $200 million. The startup is known for promoting binarized neural network algorithms to save the energy and computational resources. That is definitely a way to go for mobile devices, and Apple just acknowledged that it is a great deal for them too. I feel now is a good time to explain what binarized neural networks are so that you can better appreciate their value for the industry.">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/neural-networks/day6/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/neural-networks/day6/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/neural-networks/day6/">
  <meta property="og:title" content="Day 6: Saving Energy with Binarized Neural Networks | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="Last week Apple has acquired XNOR.ai startup for amazing $200 million. The startup is known for promoting binarized neural network algorithms to save the energy and computational resources. That is definitely a way to go for mobile devices, and Apple just acknowledged that it is a great deal for them too. I feel now is a good time to explain what binarized neural networks are so that you can better appreciate their value for the industry."><meta property="og:image" content="https://penkovsky.com/img/posts/neural-networks/bnn-solving-xor.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2020-01-22T07:30:00&#43;02:00">
  
  <meta property="article:modified_time" content="2020-01-22T07:30:00&#43;02:00">
  

  

  

  <title>Day 6: Saving Energy with Binarized Neural Networks | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>AI</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/posts/neural-networks/bnn-solving-xor.png" class="article-banner" itemprop="image">
  

  
</div>



  <div class="article-container">

    <h1 itemprop="name">Day 6: Saving Energy with Binarized Neural Networks</h2>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2020-01-22 07:30:00 &#43;0200 &#43;0200" itemprop="datePublished">
    <time datetime="2020-01-22 07:30:00 &#43;0200 &#43;0200" itemprop="dateModified">
      Jan 22, 2020
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    7 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://penkovsky.com/categories/10-days-of-grad/">10 Days Of Grad</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      

<p>Last week Apple <a href="https://techcrunch.com/2020/01/15/apple-buys-edge-based-ai-startup-xnor-ai-for-a-reported-200m/">has acquired</a>
XNOR.ai startup for amazing $200 million.
The startup is known for promoting binarized neural network
algorithms to save the energy and computational resources.
That is definitely a way to go for mobile devices, and Apple
just acknowledged that it is a great deal for them too.
I feel now is a good time to explain what
binarized neural networks are so that you can better
appreciate their value for the industry.</p>

<hr />

<p><strong>Today's post is based on</strong></p>

<ul>
<li><a href="/neural-networks/day1/">Day 1: Learning Neural Networks The Hard Way</a></li>
<li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
<li><a href="/neural-networks/day4/">Day 4: The Importance Of Batch Normalization</a></li>
</ul>

<p>The source code from this post is available <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day6">on Github</a>.</p>

<hr />

<h2 id="the-basics">The Basics</h2>

<p>Knowing what a (real-weight) <a href="/neural-networks/day1/">neural network is</a> you already
have the most essential information. In contrast,
a binarized neural network (BNN) has its weights and activations
limited to a single bit precision.
Meaning that those have values of either $+1$ or $-1$.
Below is a comparison table.</p>

<figure>

<img src="/img/posts/neural-networks/bnn.png" alt="Binarized neural network vs conventional neural network with real weights" width="590px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4></h4>
  <p>
    Binarized neural network vs conventional neural network with real weights
    
    
    
  </p> 
</figcaption>

</figure>

<p>Having single-bit values can drastically reduce hardware
requirements to operate those networks.
As you remember from <a href="/neural-networks/day1/">Day 1</a>,
neural networks are built around activations of dot products.
When every value is $+1$ or $-1$, multiplication can be replaced
with $\text{XNOR}$ binary operation, thus eliminating the need in hardware
multipliers. The dot product sum is then replaced by a simple
bit counting operation ($\text{Popcount}$).
And finally the activation is merely a thresholding ($\text{Sign}$) function.</p>

<p>Overall, the architecture can <a href="http://arxiv.org/abs/1602.02830">speed up</a> conventional
hardware or even better, drastically reduce the area of dedicated
<a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASICs</a>.
Reduced area means smaller manufacturing cost.
Not less important is another implication, reduced energy consumption.
That is crucial for <a href="/project/edge-ai/">edge</a> devices.
To further save the energy one can even merge computation and memory
giving rise to emerging <a href="/project/edge-ai/">in-memory computing</a>
technology.</p>

<p>As we have <a href="/neural-networks/day4/">discussed before</a>, batch normalization is
helpful to prevent neurons from saturation and to faster train the network.
It turns out that for binarized networks batchnorm is actually indispensable.
Indeed, neurons with $\text{Sign}$ activations tend to be useless (&quot;dead&quot;)
otherwise.
Moreover, for an efficient training, to make the error landscape smooth,
it is advisable to also include batch normalization
immediately before the softmax layer.
Yes, the most important lesson to learn here is is actually how to
<em>train</em> a BNN. I attempt to answer the most frequently encountered questions
in the following section.</p>

<h2 id="binarized-neural-networks-faq">Binarized Neural Networks FAQ</h2>

<p>Here are some typical questions that people tend to ask.</p>

<p><strong>Q: Do I need to provide binarized inputs to a BNN?</strong></p>

<p><strong>A:</strong> No, BNN inputs can remain non-binarized (real).
That results in the first layer slightly different
from the others: having binary weights (+1 and -1), yet producing real
pre-activation values. After activation, those values are
converted to binary ones. Here is a hint:</p>

<blockquote>
<p>It is relatively easy to handle continuous-valued inputs as fixed point numbers, with $m$ bits of precision.
<small>Courbariaux <em>et al.</em> Binarized Neural Networks: Training Deep Neural Networks... (2016)</small></p>
</blockquote>

<p>You should also be aware that indeed there exist
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8731879">methods</a> to have binary inputs as well.</p>

<p><strong>Q: Do I need more neurons?</strong></p>

<p><strong>A:</strong> Yes, typically one needs 3 to 4 times more <em>binary</em> neurons to
compensate for information loss.</p>

<p><strong>Q: Do I need a softmax layer?</strong></p>

<p><strong>A:</strong> Only for BNN training. For inference (and simpler hardware) it can be removed.</p>

<p><strong>Q: Do binary neurons have learnable biases?</strong></p>

<p><strong>A:</strong> No, biases are redundant as normally you
want to use batch normalization layers. Those layers already learn
bias-equivalent parameters.</p>

<p><strong>Q: Why BNNs are sometimes called &quot;integer networks&quot;?</strong></p>

<p><strong>A:</strong> Because bit count (before activation) results in an integer value.</p>

<p><strong>Q: $\text{Sign}$ activation gradient is zero</strong></p>

<p><strong>A:</strong> We approximate $\text{Sign}(x)$ derivative with
$\text{Hardtanh}(x)$<sup class="footnote-ref" id="fnref:fn-1"><a href="#fn:fn-1">1</a></sup> derivative, i.e. $ 1_{|x| \le 1} $.</p>

<p><strong>Q: Are gradients binarized in backprop training?</strong></p>

<p><strong>A:</strong> No, during the training one typically deals with
real gradients and thus, real weights.
Therefore, it is only in the forward pass where the network applies
its binarized weights.</p>

<p><strong>Q: So what is the interest then?</strong></p>

<p><strong>A:</strong> BNNs are interesting to develop dedicated hardware hosting pretrained networks
for inference, i.e. performing only the forward pass.
Think about handwriting recognition-based automated systems as a use case.
Think about energy-harvesting sensors. Solar-powered smart cameras. Drones.
Smart <a href="/publication/medical-bnn/">wearable devices</a> and implants.
Binarized networks facilitate smaller, faster, and more energy-efficient
inference devices. Though, there is also an effort towards (on-chip)
binarized networks training.</p>

<h2 id="implementing-binarized-neural-networks-in-haskell">Implementing Binarized Neural Networks in Haskell</h2>

<p>There is no better way to understand a binarized network
than to create one ourselves.
This easy demo is built
<a href="https://github.com/penkovsky/10-days-of-grad/commit/95b08aa50abe3a0051662f79134545ba7e99ffdd">on top of Day 4</a>.
Here are a few code highlights.</p>

<p>First of all, the layers types we will use</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#66d9ef">data</span> <span style="color:#66d9ef">Layer</span> <span style="color:#a6e22e">a</span> <span style="color:#f92672">=</span> <span style="color:#75715e">-- A linear layer (BNN training)</span>
               <span style="color:#f92672">|</span> <span style="color:#66d9ef">BinarizedLinear</span> (<span style="color:#66d9ef">Matrix</span> <span style="color:#a6e22e">a</span>)
               <span style="color:#75715e">-- Batch normalization with running mean, variance, and two</span>
               <span style="color:#75715e">-- learnable affine parameters</span>
               <span style="color:#f92672">|</span> <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>) (<span style="color:#66d9ef">Vector</span> <span style="color:#a6e22e">a</span>)
               <span style="color:#f92672">|</span> <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">FActivation</span></code></pre></div>

<p>Second, we provide the $\text{Sign}$ activation</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">sign</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span> <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
<span style="color:#a6e22e">sign</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">computeMap</span> <span style="color:#a6e22e">f</span>
  <span style="color:#66d9ef">where</span>
    <span style="color:#a6e22e">f</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0</span>
             <span style="color:#66d9ef">then</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
             <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">1</span></code></pre></div>

<p>and its gradient approximation:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell"><span style="color:#a6e22e">sign&#39;</span> <span style="color:#f92672">::</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
      <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
      <span style="color:#f92672">-&gt;</span> <span style="color:#66d9ef">Matrix</span> <span style="color:#66d9ef">Float</span>
<span style="color:#a6e22e">sign&#39;</span> <span style="color:#a6e22e">x</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">compute</span><span style="color:#f92672">.</span> <span style="color:#66d9ef">A</span><span style="color:#f92672">.</span><span style="color:#a6e22e">zipWith</span> <span style="color:#a6e22e">f</span> <span style="color:#a6e22e">x</span>
  <span style="color:#66d9ef">where</span>
    <span style="color:#a6e22e">f</span> <span style="color:#a6e22e">x0</span> <span style="color:#a6e22e">dy0</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">x0</span> <span style="color:#f92672">&gt;</span> (<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) <span style="color:#f92672">&amp;&amp;</span> (<span style="color:#a6e22e">x0</span> <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">1</span>)
                  <span style="color:#66d9ef">then</span> <span style="color:#a6e22e">dy0</span>
                  <span style="color:#66d9ef">else</span> <span style="color:#ae81ff">0</span></code></pre></div>

<p>Finally, we accommodate the forward and backward passes of the network
through the <code>BinarizedLinear</code> layer.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#a6e22e">_pass</span> <span style="color:#a6e22e">inp</span> (<span style="color:#66d9ef">BinarizedLinear</span> <span style="color:#a6e22e">w</span> <span style="color:#66d9ef">:</span> <span style="color:#a6e22e">layers</span>) <span style="color:#f92672">=</span> (<span style="color:#a6e22e">dX</span>, <span style="color:#a6e22e">pred</span>, <span style="color:#66d9ef">BinarizedLinearGradients</span> <span style="color:#a6e22e">dW</span> <span style="color:#66d9ef">:</span> <span style="color:#a6e22e">t</span>)
   <span style="color:#66d9ef">where</span>
    <span style="color:#75715e">-- Binarize the weights (!)</span>
    <span style="color:#a6e22e">wB</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">sign</span> <span style="color:#a6e22e">w</span>

    <span style="color:#75715e">-- Forward</span>
    <span style="color:#a6e22e">lin</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">maybe</span> (<span style="color:#a6e22e">error</span> <span style="color:#e6db74">&#34;Incompatible shapes&#34;</span>) <span style="color:#a6e22e">compute</span> (<span style="color:#a6e22e">inp</span> <span style="color:#f92672">|*|</span> <span style="color:#a6e22e">wB</span>)

    (<span style="color:#a6e22e">dZ</span>, <span style="color:#a6e22e">pred</span>, <span style="color:#a6e22e">t</span>) <span style="color:#f92672">=</span> <span style="color:#a6e22e">_pass</span> <span style="color:#a6e22e">lin</span> <span style="color:#a6e22e">layers</span>

    <span style="color:#75715e">-- Backward</span>
    <span style="color:#a6e22e">dW</span>            <span style="color:#f92672">=</span> <span style="color:#a6e22e">linearW&#39;</span> <span style="color:#a6e22e">inp</span> <span style="color:#a6e22e">dZ</span>
    <span style="color:#75715e">-- Gradient w.r.t. wB (!)</span>
    <span style="color:#a6e22e">dX</span>            <span style="color:#f92672">=</span> <span style="color:#a6e22e">linearX&#39;</span> <span style="color:#a6e22e">wB</span> <span style="color:#a6e22e">dZ</span></code></pre></div>

<p>In the <code>main</code> function we define the architecture, starting with the
number of neurons in each layer.
Good news: on the MNIST handwriting recognition benchmark
we can achieve accuracy comparable to those of a 32 bit network.
However, we may need more neurons (by <code>infl</code> factor) in the hidden layers.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">infl</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>

  <span style="color:#66d9ef">let</span> [<span style="color:#a6e22e">i</span>, <span style="color:#a6e22e">h1</span>, <span style="color:#a6e22e">h2</span>, <span style="color:#a6e22e">o</span>] <span style="color:#f92672">=</span> [<span style="color:#ae81ff">784</span>, <span style="color:#a6e22e">infl</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">300</span>, <span style="color:#a6e22e">infl</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">50</span>, <span style="color:#ae81ff">10</span>]</code></pre></div>

<p>Here is the network specification. Note the <code>BinarizedLinear</code> layers
defined above. The first <code>BinarizedLinear</code> has real inputs, however every subsequent
one receives binary inputs coming from <code>Sign</code> activation.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-haskell" data-lang="haskell">  <span style="color:#66d9ef">let</span> <span style="color:#a6e22e">net</span> <span style="color:#f92672">=</span>
        [ <span style="color:#66d9ef">BinarizedLinear</span> <span style="color:#a6e22e">w1</span>
        , <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h1</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h1</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h1</span>) (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h1</span>)
        , <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">Sign</span>

        , <span style="color:#66d9ef">BinarizedLinear</span> <span style="color:#a6e22e">w2</span>
        , <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h2</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h2</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">h2</span>) (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">h2</span>)
        , <span style="color:#66d9ef">Activation</span> <span style="color:#66d9ef">Sign</span>

        , <span style="color:#66d9ef">BinarizedLinear</span> <span style="color:#a6e22e">w3</span>
        <span style="color:#75715e">-- NB this batchnorm (!)</span>
        , <span style="color:#66d9ef">Batchnorm1d</span> (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">o</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">o</span>) (<span style="color:#a6e22e">ones</span> <span style="color:#a6e22e">o</span>) (<span style="color:#a6e22e">zeros</span> <span style="color:#a6e22e">o</span>)
        ]</code></pre></div>

<p>A final technical note is that dividing by the batch variance (and rescaling by a learnable parameter <code>gamma</code>)
<a href="https://github.com/penkovsky/10-days-of-grad/blob/master/day6/src/NeuralNetwork.hs#L294-L311">in batchnorm</a>
actually makes little sense since division (multiplication) by a positive constant does not change the sign.
If you would like to get your hands dirty with the code, that is a good place to optimize.
Another suggestion is to transfer your newly acquired skills to
a <a href="/neural-networks/day5/">convolutional network</a> architecture<sup class="footnote-ref" id="fnref:fn-2"><a href="#fn:fn-2">2</a></sup>.</p>

<p>See the complete project on <a href="https://github.com/penkovsky/10-days-of-grad/tree/master/day6">Github</a>.
If you have any questions, remarks, or any typos found
please send me an email. For suggestions about the code,
feel free to open a <a href="https://github.com/penkovsky/10-days-of-grad/issues">new issue</a>.</p>

<h2 id="summary">Summary</h2>

<p>Binarized neural networks (BNNs) are valuable for low-power edge devices.
Starting with energy-harvesting sensors and finishing with
smart wearable medical devices and implants.
Binarized networks facilitate smaller, faster, and more energy-efficient
hardware.</p>

<p>We have illustrated how to train a BNN solving a handwritten digits recognition task.
Our binarized network can achieve accuracy comparable
to a full-precision 32 bit network provided
that we moderately increase the number of binary neurons.
To better grasp today's concept, train your own BNNs.
Pay attention to small details. Do not forget the batch normalization before
the softmax. And good luck!</p>

<h2 id="further-reading">Further Reading</h2>

<p>Binarized neural networks:</p>

<ul>
<li><a href="http://arxiv.org/abs/1602.02830">Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1</a></li>
<li><a href="https://arxiv.org/abs/1603.05279">XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks</a></li>
<li><a href="/project/edge-ai/">Emerging technology: Edge AI</a></li>
<li><a href="/publication/medical-bnn/">In-Memory Resistive RAM Implementation of Binarized Neural Networks for Medical Applications</a></li>
</ul>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-1">$\text{Hardtanh}(x)=\max(-1, \min(1,x))$
 <a class="footnote-return" href="#fnref:fn-1"><sup>^</sup></a></li>
<li id="fn:fn-2">In convolutional layers you may also want to increase the number convolutional filters.
 <a class="footnote-return" href="#fnref:fn-2"><sup>^</sup></a></li>
</ol>
</div>

    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/deep-learning/">Deep Learning</a>
  
  <a class="label label-default" href="https://penkovsky.com/tags/haskell/">Haskell</a>
  
</div>




    
    <div class="article-widget">
      Next: <a href="https://penkovsky.com/neural-networks/day7/">Day 7: Real World Deep Learning</a>
    </div>
    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/neural-networks/day5/">Day 5: Convolutional Neural Networks Tutorial</a></li>
        
        <li><a href="/neural-networks/day4/">Day 4: The Importance Of Batch Normalization</a></li>
        
        <li><a href="/neural-networks/day3/">Day 3: Haskell Guide To Neural Networks</a></li>
        
        <li><a href="/neural-networks/day2/">Day 2: What Do Hidden Layers Do?</a></li>
        
        <li><a href="/neural-networks/day1/">Day 1: Learning Neural Networks The Hard Way</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; Bogdan Penkovsky 2024 

      <a rel="me" href="https://sigmoid.social/@penkovsky"><big>&sigma;</big></a>

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    
    

  </body>
</html>

