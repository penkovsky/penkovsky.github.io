<!DOCTYPE html>
<html lang="en-us">
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-YZ04D85XM2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-YZ04D85XM2');
  </script>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 2.4.0">
  <meta name="generator" content="Hugo 0.53" />
  <meta name="author" content="Bogdan Penkovsky">

  
  
  
  
    
  
  <meta name="description" content="Why quantum machine learning or QML might be worth exploring?


Quantum models are inherently generative.
Quantum parallelism over bits in superposition.
In certain cases, quantum models can be more interpretable than classical models.
Learning from quantum data. Data generated by quantum processes.
This is the very beginning of the QML field, so there are many opportunities for breakthroughs.
">

  
  <link rel="alternate" hreflang="en-us" href="https://penkovsky.com/post/qml/">

  


  

  
  
  
  <meta name="theme-color" content="#0095eb">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/abap.min.css" crossorigin="anonymous">
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700%7cRoboto:400,400italic,700%7cRoboto&#43;Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  <link rel="feed" href="https://penkovsky.com/index.xml" type="application/rss+xml" title="Bogdan Penkovsky, PhD">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://penkovsky.com/post/qml/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Bogdan Penkovsky, PhD">
  <meta property="og:url" content="https://penkovsky.com/post/qml/">
  <meta property="og:title" content="Gentle Introduction to Quantum Machine Learning | Bogdan Penkovsky, PhD">
  <meta property="og:description" content="Why quantum machine learning or QML might be worth exploring?


Quantum models are inherently generative.
Quantum parallelism over bits in superposition.
In certain cases, quantum models can be more interpretable than classical models.
Learning from quantum data. Data generated by quantum processes.
This is the very beginning of the QML field, so there are many opportunities for breakthroughs.
"><meta property="og:image" content="https://penkovsky.com/img/posts/qml/qml-cover.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2024-08-11T11:45:48&#43;02:00">
  
  <meta property="article:modified_time" content="2024-09-18T21:46:00&#43;02:00">
  

  

  

  <title>Gentle Introduction to Quantum Machine Learning | Bogdan Penkovsky, PhD</title>

</head>
<body id="top" data-spy="scroll" data-target="#toc" data-offset="71" >

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
      <a class="navbar-brand" href="/">Bogdan Penkovsky, PhD</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      
      <ul class="nav navbar-nav navbar-right">
        

        
        
        
        
        

        <li class="nav-item">
          <a href="/">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/post">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/neural-networks">
            
            <span>Neural Networks</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
            
          
        

        <li class="nav-item">
          <a href="https://scholar.google.co.uk/citations?user=NrD1h9QAAAAJ" target="_blank" rel="noopener">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        
        
        
        
        
          
        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
            
          </a>
        </li>

        
        
      

      
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
<div class="article-header">
  
  
    <img src="/img/posts/qml/qml-cover.png" class="article-banner" itemprop="image">
  

  <span class="article-header-caption">Artwork: Fidelities of measuring states |0&gt; and |1&gt; depending on the input data</span>
</div>



  <div class="article-container">
    <h1 itemprop="name">Gentle Introduction to Quantum Machine Learning</h1>

    

<div class="article-metadata">

  
  
  
  <div>
    
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Bogdan Penkovsky</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
        Last updated on
    
    <meta content="2024-08-11 11:45:48 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2024-09-18 21:46:00 &#43;0200 CEST" itemprop="dateModified">
      Sep 18, 2024
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Bogdan Penkovsky">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    19 min read
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="https://penkovsky.com/categories/quantum-computing/">Quantum Computing</a>
    
  </span>
  
  

  
  

  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>Why quantum machine learning or QML might be worth exploring?</p>

<ul>
<li>Quantum models are inherently generative.</li>
<li><a href="/post/qc/">Quantum parallelism</a> over bits in superposition.</li>
<li>In certain cases, quantum models can be more interpretable than classical models.</li>
<li>Learning from quantum data. Data generated by quantum processes.</li>
<li>This is the very beginning of the QML field, so there are many opportunities for breakthroughs.</li>
</ul>

<hr />

<p><strong>Edit 21/08/2024:</strong> Added results running the ML model on a real quantum computer.</p>

<hr />

<p>It is probably worth mentioning that by quantum machine learning we usually mean
a form of machine learning where a <em>part of</em> algorithm is executed on a quantum
computer.
In certain cases (small number of qubits or non-entangled states),
a quantum algorithm is efficiently simulated on a classical machine.
Such as your laptop.</p>

<p><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning</a> refers to
algorithms learning by examples. That is, learning from data.
The difference between machine learning and other subfields of
<a href="https://en.wikipedia.org/wiki/Artificial_intelligence">AI</a> is that machine
learning heavily relies on statistics. Unlike symbolic approaches for instance.
The most popular application of machine learning, apparently, is
being able to tell if there is a cat in a given picture or not.
At least that would be quite hard with a symbolic approach.
Obviously, in quantum machine learning, the cat would be
<a href="https://sciencing.com/wavefunctions-definition-properties-equation-signs-w-diagrams-13722576.html">Schrödinger's</a>.</p>

<p>Disclaimer: The article assumes some familiarity with quantum mechanics and
statistics. If you are new to quantum computing, I recommend starting
with the <a href="/post/qc/">Quantum Computing</a> post.
You may also safely ignore the blue boxes providing unnecessary mathematical
details in this article.</p>

<h2 id="quantum-model-as-an-expectation-value">Quantum Model as an Expectation Value</h2>

<p>A quantum model $f_{\theta}$ parameterized by $\theta$ can be represented as an
expectation value<sup class="footnote-ref" id="fnref:fn-expval"><a href="#fn:fn-expval">1</a></sup> of an observable operator $\mathcal{M}$:</p>

<p>$$ f_{\theta} = \langle 0 | \mathcal{M} | 0 \rangle,$$</p>

<p>where $| 0 \rangle$ is the ground
state of the quantum system. The expectation value of $f_{\theta}$ is a real
number since $\mathcal{M}$ is a Hermitian operator. It <em>must</em> be
Hermitian because physically we can only measure real numbers (see also
the blue box &quot;Expectation Values&quot;).</p>

<div class="alert alert-info">
<h4>Hermitian Operators</h4>
An operator $\mathcal{M}$ is Hermitian if

$$ \langle \Psi_1 | \mathcal{M} \Psi_2 \rangle = \langle \mathcal{M} \Psi_1 | \Psi_2 \rangle $$

for arbitrary operators $\Psi_1$ and $\Psi_2$.

Or, in other words,

$$ \int dx \Psi_1^{*}(x) \mathcal{M} \Psi_2(x) = \int dx (\mathcal{M} \Psi_1(x))^{*} \Psi_2(x), $$

where $\Psi_i(x)$ is a
<a href="https://en.wikipedia.org/wiki/Wave_function">wave function</a>.

</div>

<p>For instance, consider a quantum circuit measured in the computational basis:</p>

<p>$$ f_{\theta} = \langle 0 | U^{\dagger} \sigma_z U | 0 \rangle,$$</p>

<p>where $U$ is a unitary operator that represents the quantum circuit,
$U^{\dagger}$ is the adjoint of $U$, and $\sigma_z$ is the Pauli-Z operator.
The expectation value of $f_{\theta}$ is
then between -1 and 1. -1 corresponds to the state $| 1 \rangle$, 1 corresponds
to the state $| 0 \rangle$, and 0 corresponds to the superposition state $(| 0
\rangle + | 1 \rangle) / \sqrt{2}$.</p>

<div class="alert alert-info">
<h4>Expectation Values</h4>

Given that $dx \left| \Psi(x) \right|^{2} = \Psi^{*}(x) \Psi(x) dx$
is the probability of finding the particle in the interval $[x, x + dx]$,
the expectation value of the position operator $\hat x$:

$$ \langle \hat x \rangle = \int x \Psi^{*}(x) \Psi(x) dx. $$

In general, for operator $\mathcal M$:

$$ \langle \mathcal M \rangle = \int dx \Psi^{*}(x) \mathcal M \Psi(x). $$

If operator $\mathcal M$ is Hermitian, then the expectation value

$$ \langle \mathcal M \rangle = \sum |\alpha_{i}|^2 q_i, $$

where $|\alpha_{i}|^2$ are the probabilities of measuring the eigenvalues $q_i$
of the operator $\mathcal M$.

For a more detailed explanation, see the
<a href="https://www.youtube.com/watch?v=XQKV-hpsurs&list=PLUl4u3cNGP60cspQn3N9dYRPiyVWDd80G&index=38">lectures</a>
by Barton Zwiebach on the topic.

</div>

<h2 id="the-first-taste-of-quantum-machine-learning">The First Taste of Quantum Machine Learning</h2>

<p>Let us take a look at an example from <a href="https://link.springer.com/book/10.1007/978-3-030-83098-4">Schuld and Petruccione</a> where
a parametrized quantum circuit computes a unitary transformation $U$:</p>

<p>$$U =  \text{Rot}(\theta_1, \theta_2, \theta_3) R_x(x),$$</p>

<p>where $R_x(x)$ is the rotation operator around the x-axis by an angle $x$, and
$\text{Rot}(\theta_1, \theta_2, \theta_3)$ is a general rotation operator.</p>

<p>The circuit is schematized below:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">                                                             
    ┌────┐    ┌───────┐   ┌───────────────┐                  
    │|0&gt; ├────│ Rx(x) ├───┤ Rot(θ1,θ2,θ3) ├──── Measurement  
    └────┘    └───────┘   └───────────────┘                  
                                                             </code></pre></div>

<p>Initially, the qubit is in the ground state $|0\rangle$.
Then, it is rotated around $X$ axis ($R_x(x)$ operator). And then rotated again.
This time, by a general qubit
rotation $\text{Rot}(\theta_1, \theta_2, \theta_3)$. Finally, we have the measurement operation.</p>

<div class="alert alert-info">
<h4>Rotation Operators</h4>

Commonly used operators that perform rotation
around $X$, $Y$, and $Z$ axes in the Bloch sphere:

\begin{aligned}
R_x(\phi) &= \begin{pmatrix} \cos(\phi / 2) & -i \sin(\phi / 2) \\\\ -i \sin(\phi / 2) &  \cos(\phi / 2) \end{pmatrix}, \\\\
\end{aligned}

\begin{aligned}
R_y(\phi) &= \begin{pmatrix} \cos(\phi / 2) & -\sin(\phi / 2) \\\\ \sin(\phi / 2) & \cos(\phi / 2) \end{pmatrix}, \\\\
\end{aligned}

\begin{aligned}
R_z(\phi) &= \begin{pmatrix} e^{-i \phi / 2} & 0 \\\\ 0 & e^{i \phi / 2} \end{pmatrix}.
\end{aligned}

If you remember the Hadamard operator $H$ from this
<a href="/post/qc/">post</a>,
it is a special case of $R_y$ operator: $H = R_y(\pi / 2)$.

Feel free to derive the arbitrary qubit rotation operator
$\text{Rot}(\theta_1, \theta_2, \theta_3)$
as an exercise.
</div>

<p>Therefore, the corresponding quantum model is mathematically described as:</p>

<p>$$ f_{\theta}(x) = \langle 0 | R_x(x)^\dagger \text{Rot}(\theta_1, \theta_2, \theta_3)^\dagger \sigma_z \text{Rot}(\theta_1, \theta_2, \theta_3) R_x(x) | 0 \rangle.$$</p>

<p>The variable $x$ is the input data point and $\theta$ are parameters
that we need to optimize such that $f_{\theta}(x)$ approximates
some <em>ground truth</em> function
$\hat y$.
We don't know what $\hat y$ is, otherwise we would not need
<em>learning</em> it. All we have, is a bunch of data points
and we want our model to reasonably represent them.</p>

<p>Consider the table below:</p>

<table>
<thead>
<tr>
<th>data point $x$</th>
<th>$y$ value</th>
</tr>
</thead>

<tbody>
<tr>
<td>-0.8238</td>
<td>-0.56</td>
</tr>

<tr>
<td>-0.3204</td>
<td>-0.033</td>
</tr>

<tr>
<td>0.3083</td>
<td>0.03</td>
</tr>
</tbody>
</table>

<p>We want to build a model on the interval $[-1, 1]$,
such that when we have a new value e.g. $x = 0.2$, we can predict
what $f(0.2)$ would be.</p>

<p><strong>Challenge #1</strong>: Ambiguity of the model. The model is not unique.</p>

<p>Perhaps, we can approximate
the ground truth with $f(x) = x^3$. But we don't know that.
For example, $f_\theta(x) = \sin(5 x + \theta)$
where $\theta = \pi / 2$ would fit equally well!
Therefore, we have an ambiguity.
We could try to reduce the ambiguity by collecting more data points.</p>

<figure>

<img src="/img/posts/qml/fit1.png" width="400px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Machine learning models are not unique. There exist an infinite number of ways to fit the data.</h4>
  
</figcaption>

</figure>

<p>Let's imagine that now we have worked hard to collect more data:</p>

<table>
<thead>
<tr>
<th>data point</th>
<th>$y$ value</th>
</tr>
</thead>

<tbody>
<tr>
<td>-0.8238</td>
<td>-0.56</td>
</tr>

<tr>
<td>-0.3204</td>
<td>-0.033</td>
</tr>

<tr>
<td>-0.2</td>
<td>-0.54</td>
</tr>

<tr>
<td><strong>0</strong></td>
<td><strong>-0.2</strong></td>
</tr>

<tr>
<td>0.3083</td>
<td>0.03</td>
</tr>

<tr>
<td>0.5</td>
<td>-0.8</td>
</tr>
</tbody>
</table>

<p><strong>Challenge #2</strong>: Noise in the data.</p>

<p>Indeed,
the $f(x) = \sin(5 x + \pi / 2)$ model
would have perfectly fit the data points if not for the noise.
Note the value of $f(0) = -0.2$, whereas $\sin(\pi / 2) = 1$.
We realize that our data are probably corrupted by noise.
That is $y = \hat y + \epsilon$,
where $\epsilon$ is a random noise.
Therefore, it is very hard to tell whether $f(0) = -0.2$ is an
outlier or a genuine data point.</p>

<p>One way to resolve the issue is to collect even more data points.
Which often might be very costly or even impossible.
Imagine that every data point is a result of a complex forty-step
chemical reaction. You can't just run the reaction
a thousand times to get more data points.</p>

<p>Another way to resolve the issue is to ignore the outliers.
But this might lead to a biased model.</p>

<p><strong>Challenge #3</strong>: Overfitting.</p>

<p>Finally, we can use a more complex model, so that
it can fit to the $f(0) = -0.2$ point.
On the other hand, a more complex model might capture the <em>noise</em>
in the data, and not the <em>signal</em>.
This would lead to what we call <em>overfitting</em>.
That is, the model would perform well on the training data
but make poor predictions on new data.</p>

<p><strong>Challenge #4</strong>: Verification.</p>

<p>To resolve the issue, we may want to <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularize</a> or use a
simpler model.
But how do we actually know that the model makes good predictions?
We need to validate it on data unseen during training.
In practice this means that we
train the model on a part of the data and validate it on the
remaining part. This gives some approximation of how well
the model generalizes to new data.</p>

<h2 id="neural-networks-universal-function-approximators-and-quantum-models">Neural Networks, Universal Function Approximators, and Quantum Models</h2>

<p>The power of neural networks comes from their ability to approximate any
reasonable function. As we have seen in this
<a href="/neural-networks/day2/">post</a>, we need <em>nonlinear</em>
transformations over subsequent layers for this to work. However, quantum
transformations are <em>linear</em>.
So how can we approximate any function with a quantum model
if it is linear by nature? The good news is
that we do not need to mimic the exact behavior of neural networks.
<a href="https://arxiv.org/abs/2008.08605">Schuld et al.</a> showed that quantum models can approximate any
square-integrable function, which is a broad class of functions. The basic idea
was to show that quantum models $f_{\mathbf{\theta}}$ parametrized by
$\mathbf{\theta}$ can be represented as a Fourier-type sum</p>

<p>$$ f_{\mathbf{\theta}} = \sum_{\mathbf{\omega} \in \Omega} c_{\mathbf{\omega}}(\mathbf{\theta}) e^{i \mathbf{\omega x}}, $$</p>

<p>where $\Omega$ is the frequency spectrum and $\mathbf{\omega x}$
is the inner product.
And the nonlinearity $e^{i \mathbf{\omega x}}$
comes from <em>encoding</em> the classical data into the quantum states.
Note also that the functions $f_{\mathbf{\theta}}$ are
periodic. In practice that means that the inputs need to be rescaled to avoid
periodicity.</p>

<p>Interestingly, the authors show that the frequency spectrum $\Omega$ is solely
determined by the eigenvalues of the data-encoding Hamiltonials, while the
design of the entire circuit (or <em>ansatz</em>) controls the coefficients $c_{\omega}$.</p>

<p>An intuitive connection can be established in the following figure:</p>

<figure>

<img src="/img/posts/qml/mlp.jpg" width="480px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Schuld and colleagues demonstrate the connection between neural networks and quantum models through Fourier formalism. The quantum circuit can be seen as a neural network with a single hidden layer.</h4>
  
</figcaption>

</figure>

<p>Now you should be able to understand why talking about &quot;deep quantum neural
networks&quot; is at least misleading. Quantum models are not deep in the sense that
they have multiple layers of nonlinear transformations. However, they are
still universal function approximators in the sense that they can approximate
any square-integrable function with a Fourier-type sum.</p>

<h2 id="somewhat-detailed-example">Somewhat Detailed Example</h2>

<blockquote>
<p>The associated code is on <a href="https://gitlab.com/quadrant27/qml">Gitlab</a> if you want to follow along.</p>
</blockquote>

<p>Let us consider a simple example where
we want to perform the classification task
(as in the &quot;cat/no cat&quot; task, remember?).
The algorithm we are going to demonstrate belong to the class of
<em>variational quantum algorithms</em>. It is called variational because we optimize
the parameters $\theta$ of the quantum model
by varying them. The goal of optimization is to
minimize a certain <em>loss</em> function related to the task at hand.</p>

<p>The loss (or cost) function we chose for this example is based on the <em>fidelity</em>
between the quantum state that depends on the data input, and the target state
$\psi_{target}$, which is either $|0\rangle$ or $|1\rangle$.
You may think about the target state as if it were a label $0$ or $1$
in classical machine learning. Fidelity is a measure of how close two
quantum states are. The fidelity between states $|\psi\rangle$ and
$|\phi\rangle$ is defined as their inner product squared:</p>

<p>$$ F(\psi, \phi) = |\langle \psi | \phi \rangle|^2. $$</p>

<p>Fidelity represents the overlap or &quot;closeness&quot; between the two states.
Therefore, we want to maximize the fidelity when measuring $|0\rangle$ given
that the input data is a &quot;no cat&quot;. Likewise, we want to maximize the fidelity
when measuring $|1\rangle$ given that the input data is a &quot;cat&quot;,
whatever that means in the context of the dataset.</p>

<p>The intuitive explanation of this particular
strategy is the following:
We want to find such parameters $\theta$ that after performing a
fixed number of rotations (proportional to <code>num_layers</code> variable) on the input
data, the obtained vector on the Bloch sphere is close to the initial vector
(ground state $|0\rangle$) for the &quot;no cat&quot; class and far from the
initial vector for the &quot;cat&quot; class. That is, if it actually is a representative
of the &quot;no cat&quot; class, after all the rotations, the state vector should
come back close to the initial state $|0\rangle$. Whereas if it is a
representative of the &quot;cat&quot; class, the state vector should come
close to the state $|1\rangle$.</p>

<p>We already explained that
the goal of machine learning is to find such a model that not only has a high
accuracy on the training data but also generalizes well to unseen data.
Therefore, we need at least two datasets: one for training and one for
validation.</p>

<p>As a benchmark, we will use the two spirals dataset from this
<a href="/neural-networks/day2/">post</a>. The dataset consists of two
spirals that are intertwined. The goal is to classify the points into two
classes (orange and blue). Imagine that blue points are cats
in some low-dimensional space, if you like.
It does not change the essence of the task.</p>

<figure>

<img src="/img/posts/qml/two-spirals.png" width="400px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Two spirals dataset.</h4>
  
</figcaption>

</figure>

<p>To get you motivated, the figure below illustrates what we are going to
achieve with a quantum model.</p>

<figure>

<img src="/img/posts/qml/two-spirals-optimized.png" width="700px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Classification before and after training. Having only 75 parameters, the model achieves perfect classification on both training and validation datasets.</h4>
  
</figcaption>

</figure>

<p>There are several meta-parameters (or <em>hyperparameters</em>)
that might impact the performance of the
model. Therefore, we might need to tune them to get reasonably good results.
Andrew Ng recommends using random sampling. This helps to deal
with the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>.</p>

<p>Here is a randomly generated plan of experiments for the three hyperparameters:</p>

<table>
<thead>
<tr>
<th>#</th>
<th>num_layers</th>
<th>learning_rate</th>
<th>batch_size</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td>12</td>
<td>0.0497</td>
<td>16</td>
</tr>

<tr>
<td>2</td>
<td>8</td>
<td>0.0147</td>
<td>32</td>
</tr>

<tr>
<td>3</td>
<td>7</td>
<td>0.0663</td>
<td>16</td>
</tr>

<tr>
<td>4</td>
<td>9</td>
<td>0.0047</td>
<td>64</td>
</tr>

<tr>
<td>5</td>
<td>7</td>
<td>0.0053</td>
<td>64</td>
</tr>

<tr>
<td>6</td>
<td>9</td>
<td>0.0177</td>
<td>32</td>
</tr>

<tr>
<td>7</td>
<td>14</td>
<td>0.0075</td>
<td>64</td>
</tr>

<tr>
<td>8</td>
<td>9</td>
<td>0.0889</td>
<td>16</td>
</tr>

<tr>
<td>9</td>
<td>15</td>
<td>0.0135</td>
<td>64</td>
</tr>

<tr>
<td>10</td>
<td>12</td>
<td>0.0398</td>
<td>16</td>
</tr>

<tr>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>

<tr>
<td>29</td>
<td>10</td>
<td>0.0472</td>
<td>64</td>
</tr>

<tr>
<td>30</td>
<td>9</td>
<td>0.0274</td>
<td>64</td>
</tr>
</tbody>
</table>

<p>Now we will train several models (we chose 30 as a reasonable number)
for a small number or epochs (see below) to select
candidates with the best performance (validation accuracy in this case).
Note, that for a more reliable result, one should average the accuracies
over several runs with the same <em>hyperparameters</em> but different initial
<em>model parameters</em> $\theta$.</p>

<p>Having a luxury of simulating the quantum model on a classical computer, we can
employ the gradient descent together with the
<a href="/neural-networks/day2/">Adam optimizer</a> to learn the
model parameters. On a real quantum computer, we would need to use something like
the <a href="https://arxiv.org/abs/1909.02108">Quantum Natural Gradient</a> or
<a href="https://pennylane.ai/qml/glossary/parameter_shift/">parameter shift rule</a>.</p>

<p>The goal is to minimize the average loss over the training dataset. Our square
of difference loss is minimized when the fidelity maximized (i.e. close to 1).</p>

<p>$$ \text{loss} = \frac{1}{N} \sum_{i=1}^{N} \left(1 - F(\psi_{\theta}(x_i), \psi_{target}) \right)^2, $$</p>

<p>where $\psi_{\theta}$ comes from the circuit, $x_i$ is the input data point,
and $N$ is the number of data points in the training set.
Finally, here is the circuit itself:</p>

<figure>

<img src="https://pennylane.ai/_images/universal_layers.png" alt="Credit: [pennylane.ai](https://pennylane.ai/qml/demos/tutorial_data_reuploading_classifier/)." width="550px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>The variational circuit scheme to learn the spirals dataset.</h4>
  <p>
    Credit: <a href="https://pennylane.ai/qml/demos/tutorial_data_reuploading_classifier/">pennylane.ai</a>.
    
    
    
  </p> 
</figcaption>

</figure>

<p>In our version, we perform a number of repetitions (<code>num_layers</code>) of two operators:
$u(x)$ and $u(\theta)$. The operator
$u(x) := \text{Rot}(x_1 \theta_1, x_2 \theta_2, 0)$ is parametrized by the input
data vector $x := (x_1, x_2)$ and two angles $\theta_{i_1}$ and $\theta_{i_2}$.
While the operator $u(\theta) := \text{Rot}(\theta_3, \theta_4, \theta_5)$ is
only parametrized by the angles $\theta_{i_3}$,
$\theta_{i_4}$, and $\theta_{i_5}$.</p>

<p>We train models by adapting parameters $\theta_{i=1..\text{num_layers}}$
with gradient descent and the Adam optimizer.
During the parameter search, we train the models on 200 train data samples for 10
epochs each. This is motivated by the need to <strong>reduce the amount of
computation</strong> during the hyperparameter search.
After training all 30 models, we analyze the results
by sorting the models by the validation accuracy.
The first model resulted in the validation accuracy of 0.8575 after
10 epochs of training. The second model results in the validation accuracy of only 0.7375.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">  accuracy_train  accuracy_val  num_layers  learning_rate  epochs  batch_size
        0.870        0.8575          12         0.0497      10          16
        0.775        0.7375          15         0.0633      10          32
        0.735        0.7100          12         0.0398      10          16
        0.665        0.6450          15         0.0276      10          16
        0.595        0.6050          10         0.0472      10          64</code></pre></div>

<p>Note that the best model might be different from run to run due to the random
initialization of the model parameters. Therefore, it is a good practice to
average the results over several runs with the same hyperparameters.</p>

<p>Then we can train our final models on more training data samples (if available)
and for more epochs.
In our case, it was 30 epochs on 400 data samples.
Validation was done on another 400 data samples, not seen during training.
What a good luck to use a synthetic dataset!</p>

<p>I tried the two top configurations from the list above,
and the best ultimate result was achieved with the second best configuration
(<code>num_layers = 15, learning_rate = 0.0633, batch_size = 32</code>).
Of course, the results might vary from run to run due to the random initialization
of the model parameters, which is something to keep in mind.</p>

<p>Here are the optimized parameters $\theta$ for the
architecture with <code>num_layers = 15</code> repetitions achieving perfect
prediction accuracy on both training and validation datasets.
Those are only $15 \cdot 5 = 75$ model parameters, therefore,
I can conveniently print them below.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-text" data-lang="text">[[ 0.29827245,  0.08220332,  1.42578576,  0.50208184, 1.5928177 ],
 [ 0.05869113,  0.26620008,  1.12377337,  0.46328539, 1.70133469],
 [ 1.18456692, -0.45356308,  0.74022415,  0.21701071, -0.29728818],
 [-0.73229329,  0.90770549,  0.36220917,  0.91383889, 0.04663861],
 [-0.54919759,  0.96878071,  1.39345259,  1.35545392, 0.38011819],
 [ 0.2557679 , -0.32282553,  0.57328026,  1.60893581, 1.21153585],
 [ 1.14139141,  1.09135017,  0.6102359 , -0.55232082, -0.47481543],
 [ 1.36793678,  1.19127662,  0.67343336,  0.34314725, -0.0578214 ],
 [ 1.47484026,  1.01214414,  0.76562758, -0.2116039 , 0.83520595],
 [ 0.47545624, -0.37081129,  0.96669664,  0.05487279, 0.24774081],
 [ 1.48150396, -1.89516311, -0.15641872, -0.09757299, 0.04472809],
 [ 1.76554667, -2.2032729 ,  0.34249646,  0.05324532, 0.62406002],
 [ 1.05227216, -0.94414955,  0.67808506,  0.91553152, 1.03300285],
 [ 0.60005659,  1.34494046, -0.31970111,  0.28673467, 1.3734422 ],
 [ 1.57574681,  0.40458179, -1.87592608, -0.88068899, 0.23475442]]</code></pre></div>

<p>Compare with 513, 1416, or 2049 parameters in the
<a href="/neural-networks/day2/">classical neural networks</a>
that achieve the same or inferior accuracy on validation data.</p>

<p>Now we can visualize the learned decision boundary:</p>

<figure>

<img src="/img/posts/qml/two-spirals-decision-boundary.png" width="450px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Learned decision boundary.</h4>
  
</figcaption>

</figure>

<p>Even more interesting is to check the fidelity of measuring one of the two
states.</p>

<figure>

<img src="/img/posts/qml/spirals-fidelity-1.png" width="450px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Fidelity of measuring $|1\rangle$.</h4>
  
</figcaption>

</figure>

<p>I would suggest that the model captures the essence of the dataset
quite well.</p>

<h2 id="testing-on-ibm-quantum">Testing on IBM Quantum</h2>

<p>This all is nice, but is probably worth not much if it does not work on a real
quantum machine. To confirm our results,
I ran a pretrained model on the IBM Brisbane quantum computer.
The characteristics of the QPU (quantum processing unit)
are shown below.</p>

<figure>

<img src="/img/posts/qml/ibm_brisbane_details.png" width="650px" />


</figure>

<figure>

<img src="/img/posts/qml/ibm_brisbane.png" width="650px" />


</figure>

<p>It is a 127-qubit Eagle type quantum processor. The native gates can be found
<a href="https://docs.quantum.ibm.com/guides/native-gates#eagle">here</a>.
As you can see, there are no universal rotations available.
Therefore, during <a href="/post/qc/#transpilation">transpilation</a>, the
gates are decomposed into the native gates.
That is, the universal rotations will be represented in terms
of $R_z$ (<a href="https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.RZGate">RZ</a>) and $\sqrt X$ (<a href="https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.library.SXGate">SX</a>) gates.</p>

<p>For the sake of test, I have validated the model on the first 20 data points
unseen during training.</p>

<figure>

<img src="/img/posts/qml/spirals_val.png" width="450px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Data points tested on a real quantum computer.</h4>
  
</figcaption>

</figure>

<p>Below are a few example circuits that were run on the IBM quantum computer. You
can easily verify that the first two have a high fidelity of measuring $|0\rangle$,
while the last two have a high fidelity of measuring $|1\rangle$.</p>

<figure>

<img src="/img/posts/qml/isa_circuits0-1.png" width="500px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Circuits representing the $|0\rangle$ target.</h4>
  
</figcaption>

</figure>

<figure>

<img src="/img/posts/qml/isa_circuits10-11.png" width="500px" />



<figcaption data-pre="Figure " data-post=":" >
  <h4>Circuits representing the $|1\rangle$ target.</h4>
  
</figcaption>

</figure>

<p>You might be surprised first that the circuits are so simple. However, there is nothing
wrong with that. To run a circuit, we have to provide all the parameters $\theta$ and $x$.
During the <a href="/post/qc/#transpilation">transpilation</a> process,
therefore, the circuit is simplified. And instead of 30 rotations, we have only
one. This single rotation is then represented in terms of the native gates
$R_z$ and $\sqrt X$.
Note also that we can only run a single circuit at a time.</p>

<p>Using 1024 shots per data point, the workload
consumed 7.6 seconds of the QPU time.
Below are the results summarized in a form of table.
Those are the probabilities of measuring the states $|0\rangle$
and $|1\rangle$ for the 20 data points.</p>

<table>
<thead>
<tr>
<th>#</th>
<th>Proba  0</th>
<th>Proba  1</th>
<th>$\hat y$ Label</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td><strong>0.91</strong></td>
<td>0.09</td>
<td>0</td>
</tr>

<tr>
<td>2</td>
<td><strong>0.93</strong></td>
<td>0.07</td>
<td>0</td>
</tr>

<tr>
<td>3</td>
<td><strong>0.83</strong></td>
<td>0.17</td>
<td>0</td>
</tr>

<tr>
<td>4</td>
<td><strong>0.84</strong></td>
<td>0.16</td>
<td>0</td>
</tr>

<tr>
<td>5</td>
<td><strong>0.86</strong></td>
<td>0.14</td>
<td>0</td>
</tr>

<tr>
<td>6</td>
<td><strong>0.83</strong></td>
<td>0.17</td>
<td>0</td>
</tr>

<tr>
<td>7</td>
<td><strong>0.92</strong></td>
<td>0.08</td>
<td>0</td>
</tr>

<tr>
<td>8</td>
<td><strong>0.74</strong></td>
<td>0.26</td>
<td>0</td>
</tr>

<tr>
<td>9</td>
<td><strong>0.88</strong></td>
<td>0.12</td>
<td>0</td>
</tr>

<tr>
<td>10</td>
<td><strong>0.91</strong></td>
<td>0.09</td>
<td>0</td>
</tr>

<tr>
<td>11</td>
<td>0.03</td>
<td><strong>0.97</strong></td>
<td>1</td>
</tr>

<tr>
<td>12</td>
<td>0.06</td>
<td><strong>0.94</strong></td>
<td>1</td>
</tr>

<tr>
<td>13</td>
<td>0.03</td>
<td><strong>0.97</strong></td>
<td>1</td>
</tr>

<tr>
<td>14</td>
<td>0.04</td>
<td><strong>0.96</strong></td>
<td>1</td>
</tr>

<tr>
<td>15</td>
<td>0.02</td>
<td><strong>0.98</strong></td>
<td>1</td>
</tr>

<tr>
<td>16</td>
<td>0.09</td>
<td><strong>0.91</strong></td>
<td>1</td>
</tr>

<tr>
<td>17</td>
<td>0.02</td>
<td><strong>0.98</strong></td>
<td>1</td>
</tr>

<tr>
<td>18</td>
<td>0.07</td>
<td><strong>0.93</strong></td>
<td>1</td>
</tr>

<tr>
<td>19</td>
<td>0.04</td>
<td><strong>0.96</strong></td>
<td>1</td>
</tr>

<tr>
<td>20</td>
<td>0.02</td>
<td><strong>0.98</strong></td>
<td>1</td>
</tr>
</tbody>
</table>

<p>We can see that the model performs well on the unseen data points,
despite the noise in the NISQ device. This is not surprising
since (1) the theoretical fidelities were close to 1, and (2) the model
was simplified to only five native gates during the transpilation process.
Less gates mean less noise.
To reproduce my experiments on a quantum computer,
you can run this <a href="https://gitlab.com/quadrant27/qml/blob/master/spirals-ibm.ipynb">notebook</a> (IBM account required).</p>

<h2 id="limitations">Limitations</h2>

<p>The article would be not complete without mentioning the limitations of quantum
computing in general and quantum machine learning in particular.</p>

<ul>
<li>The most important limitation is that currently available NISQ devices are <em>noisy</em>
and have <em>limited qubit count</em>. This means that we can't just run different
quantum algorithms and see what works best. In contrast to
graphical processing units availability,
which arguably led to success in deep learning.</li>
<li>Cryogenic cooling is required. So you can't just put a quantum computer on your desk.</li>
<li>Quantum machine learning is in its infancy. Therefore, there are not many
real-life problems solved with quantum machine learning that cannot be
efficiently solved with classical machine learning.</li>
<li>Quantum algorithms are <em>not always</em> faster than classical algorithms.</li>
<li>Classical machine learning proved to be incredibly successful to solve
real life problems. On the other hand, in today's quantum computing, problems
are <a href="https://arxiv.org/abs/2203.01340">carefully selected</a> to be provably difficult for classical computers.</li>
<li>Today's quantum computers are typically applied to <a href="https://arxiv.org/abs/2203.01340">well-structured</a> problems.</li>
</ul>

<h2 id="summary">Summary</h2>

<p>This post illustrates that even single-qubit quantum models can approximate
highly non-linear functions. And this is despite the linear nature of quantum
transformations. This is possible thanks to the encoding of classical data into
quantum states. The repetition of quantum transformations over the input data
increases the frequency spectrum of the quantum model, and therefore its
expressive power. The quantum universal approximation theorem states that
quantum models can approximate any square-integrable function.
Much like in classical neural networks, the universal approximation theorem
does neither provide a recipe how to construct a quantum circuit with sufficient
expressive power, nor how to obtain the parameters to approximate a given
function.</p>

<p>There are several opportunities to explore in quantum machine learning. One of
them is quantum machine learning with quantum data.
Another one is encoding classical datasets into superposition states.
To exploit, therefore, quantum parallelism and interference, which might be
beneficial for certain tasks.
Finally, the
benefits of entanglement in quantum machine learning are yet to be demonstrated.
The main challenge is to find quantum models that can outperform classical
models on real-world datasets. This is especially true in the NISQ era, where
the number of qubits is limited and the noise is high.</p>

<h2 id="citation">Citation</h2>

<pre>
@article{penkovsky2024qml,
 title   = "Gentle Introduction to Quantum Machine Learning",
 author  = "Penkovsky, Bogdan",
 journal = "penkovsky.com",
 year    = "2024",
 month   = "August",
 url     = "https://penkovsky.com/post/qml/"
}
</pre>

<h2 id="learn-more">Learn More</h2>

<ul>
<li><a href="https://arxiv.org/abs/2203.01340">Is quantum advantage the right goal for quantum machine learning?</a></li>
<li><a href="https://arxiv.org/abs/2008.08605">Effect of data encoding on the expressive power of variational quantum-machine-learning models</a></li>
<li><a href="https://link.springer.com/book/10.1007/978-3-030-83098-4">Machine Learning with Quantum Computers</a></li>
<li><a href="https://www.youtube.com/watch?v=XQKV-hpsurs&amp;list=PLUl4u3cNGP60cspQn3N9dYRPiyVWDd80G&amp;index=38">Expectation values of operators</a></li>
<li><a href="https://pennylane.ai/qml/demos/tutorial_data_reuploading_classifier/">Data re-uploading classifier</a></li>
</ul>
<div class="footnotes">

<hr />

<ol>
<li id="fn:fn-expval">The expectation value (marked by the angle brackets) of an operator is the average value of the observable in a given quantum state: $\langle \mathcal{M} \rangle = \langle \psi | \mathcal{M} | \psi \rangle,$ where $| \psi \rangle$ is the quantum state. In practice, you will need to measure a large number of copies of the quantum state and average the results to get the expectation value.
 <a class="footnote-return" href="#fnref:fn-expval"><sup>^</sup></a></li>
</ol>
</div>
    </div>

    


<div class="article-tags">
  
  <a class="label label-default" href="https://penkovsky.com/tags/qc/">QC</a>
  
</div>




    
    <div class="article-widget">
      Next: <a href="https://penkovsky.com/post/goals/">5/25 Rule</a>
    </div>
    

    
    
    <div class="article-widget">
      <div class="hr-light"></div>
      <h3>Related</h3>
      <ul>
        
        <li><a href="/post/qc/">Quantum Computing for Hackers</a></li>
        
      </ul>
    </div>
    

    


  </div>
</article>

<footer class="site-footer">
  <div class="container">

    

    <p class="powered-by">

      &copy; Bogdan Penkovsky 2025 &middot; 

      Powered by
      <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <button type="button" class="close btn-large" data-dismiss="modal">&times;</button>
        <h4 class="modal-title">Cite</h4>
      </div>
      <div>
        <pre><code class="modal-body tex"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-primary btn-outline js-copy-cite" href="#" target="_blank">
          <i class="fa fa-copy"></i> Copy
        </a>
        <a class="btn btn-primary btn-outline js-download-cite" href="#" target="_blank">
          <i class="fa fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.2.1/jquery.min.js" integrity="sha512-3P8rXCuGJdNZOnUx/03c1jOTnMn3rP63nBip5gOP2qmUh5YAdVAvFZ1E+QLZZbC1rtMrQb+mah3AfYW11RUrWA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/haskell.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    

    
    

    
    

  </body>
</html>

