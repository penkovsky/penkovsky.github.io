[{"authors":["Bogdan Penkovsky"],"categories":[],"content":"Breaking news! Artificial intelligence is taking over the world. Or it is not? Here is what you need to know about a deeper concept of meta-learning.\nMeta-learning is learning about learning. Learning how to learn belongs here too.\nTable of Contents\n Models Analyzing Other Models Updating Neural Network During Run Self-Modifying Learning Algorithms Conclusion  The first thing that pops to mind is AutoML, a search for the best model architecture. It often includes data preparation, feature engineering, and hyperparameter search. Companies like Google adore this approach: First, AutoML requires a lot of computational resources, and companies like Google have it. Second, AutoML can make machine learning accessible to non-experts, enabling companies like Google to sell their compute power to more customers.\nHowever, meta-learning is more than AutoML. It is a broad subject. Here are the articles that have marked my thinking.\nModels Analyzing Other Models In this work, Langosco and colleagues detect whether a neural network has a backdoor. A backdoor is a way to modify the input to force a network to produce an undesirable output. As an extreme example, imagine a self-driving car that crashes into another car after seeing a malicious drawing on the road. That drawing would be a neural network's input that was misinterpreted to cause a bad decision, which itself led to a crash. And a backdoor in this case could be a susceptibility of a neural network to a particular form or texture.\nTherefore, you see, it could be quite hard to manually design such an algorithm that would detect if a neural network has a backdoor in it. So it seems natural to use another neural network (which we call a meta-model) to analyze the weights of the first one and to detect whether there is a built-in backdoor or not.\nI think the importance of this work is not only technical, it also raises the awareness about the challenges that we will face when relying on AI. In 2023 AI is often defined as \u0026quot;allowing computers to learn and solve problems almost like a person\u0026quot; (BBC, 01/11/2023). Which, to be honest, has almost nothing to do with intelligence1. And even though there is no imminent Terminator danger, we need to beware of malicious human actors out there.\nHere are a few more examples of models analyzing other models.\nUpdating Neural Network During Run From the perspective of reinforcement learning, meta-learning is about agents that learn from ongoing experience. For instance, a robotic dog was trained to walk on the grass. Now, as it crosses a patch of sandy terrain, can it adapt its walking gait? Najarro and Risi suggest a recipe of an agent that modifies its behavior based on local learning. That is, based on the neural network layers' inputs and outputs. Using the Hebbian rule (\u0026quot;what fires together wires together\u0026quot;), the network is updated on the fly. Evolutionary techniques are employed to train such agents. In a remarkable demonstration, a robot manages to walk despite one of its legs being damaged. This would be hardly possible with neural networks having static weights (that is, \u0026quot;normal\u0026quot; neural networks).\nTo capture invariant object representations, Halvagal and Zenke propose to augment the Hebbian rule to include a predictive term. \u0026quot;It cancels when the neural activity does not change and, therefore, accurately predicts future activity,\u0026quot; the authors explain. The idea resonates with HTM neurons from thousand brains theory, originally conceptualized by Jeff Hawkins. Each HTM neuron predicts its activation. The hypothesis claims that these predictions of expected inputs allow brains to filter out what has changed in the environment and what is immediately important to notice. Like a branch cracking. Suddenly, a wild animal detects a threat in the jungle. Such alertness is essential for survival in a complex environment.\n\u0026quot;So how about transformers?\u0026quot; you might ask, \u0026quot;They use attention.\u0026quot; Yes, transformers technically belong to the category of dynamic neural networks, too. What happens with GPT models (T is for transformer of course) is that when you provide a text prompt, it serves as a context. The attention mechanism underpinning the transformer architecture is transforming the neural network weights, as a function of that context. And that is what makes transformers flexible.\nBy the way don't be misled thinking that the attention mechanism/dynamically updated weights is the reason why transformer models are called generative! Generative models already exist for decades. Other examples are generative adversarial networks (GANs), auto-encoders, variational auto-encoders (VAEs), Hidden Markov models, etc. The actual reason why all those models are called generative is learning a joint distribution. Combining a generative model with neural networks is now called Generative AI (not so elegant, as you can see). It just happens that the transformer architecture is popular, and is the first thing associated with \u0026quot;Generative AI\u0026quot; these days.\nComing back to transformers, Tang and Ha demonstrate what they call a sensory neuron. Based on the attention mechanism, their agents trained by reinforcement learning are able to quickly re-adapt to the change of order of sensory stimuli. That is, it does not matter in which order the observation inputs are provided. Moreover, their agent is not confused even when the majority of the inputs are masked out.\nSelf-Modifying Learning Algorithms Perhaps this concept is the pinnacle of meta-learning. What can be more appealing than learning algorithms that modify themselves? If updating a neural network during run was a first-order meta-learning, then updating neural networks that update neural networks would be already a second-order. And by induction, neural networks that update their own weights, as is done for example by Irie and colleagues, can be considered an infinite-order meta-learning.\nDon't agree? Then shoot me an email!\nConclusion The family of meta-learning concepts is vast. This article has touched a few notable examples. However, as the field continues to evolve, more interesting work is to appear. I don't know how far we are from the so-called \u0026quot;artificial general intelligence\u0026quot;, however, its distinctive characteristic is extreme adaptivity. And adaptivity is something that is currently explored by meta-learning researchers.\nCitation  @article{penkovsky2023meta, title = \"Meta-Learning\", author = \"Penkovsky, Bogdan\", journal = \"penkovsky.com\", year = \"2023\", month = \"December\", url = \"https://penkovsky.com/neural-networks/meta-learning/\" }   I believe, a truly \u0026quot;intelligent\u0026quot; machine has to be adaptive at least. And probably it has—in some sense—to be embodied as well. What we witness today with ChatGPT is a text-to-text engine. It's more about machine translation than intelligence. Once I asked it to create a personal training plan for swimming. I've got some result, quite far from what I specified. Does ChatGPT even know about how to be immersed in the water and what it may feel like? Certainly not. It managed to \u0026quot;translate\u0026quot; existing articles and compile a training plan of some kind. I doubt it could ever substitute a coach who can actually swim.\n^  ","date":1702732800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702732800,"objectID":"9984ae298abf373661b80f648c5e264d","permalink":"https://penkovsky.com/neural-networks/meta-learning/","publishdate":"2023-12-16T14:20:00+01:00","relpermalink":"/neural-networks/meta-learning/","section":"neural-networks","summary":"Breaking news! Artificial intelligence is taking over the world. Or it is not? Here is what you need to know about a deeper concept of meta-learning.\nMeta-learning is learning about learning. Learning how to learn belongs here too.\n","tags":["Deep Learning"],"title":"Meta-Learning","type":"neural-networks"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":"Ever wondered how machines defeated the best human Go player Lee Sedol in 2016? A historical moment for the game that was previously considered to be very tough. What is reinforcement learning that generates so much buzz recently? Superhuman performance playing arcade Atari games, real-time Tokamak plasma control, Google data center cooling, and autonomous chemical synthesis are all the recent achievements behind the approach. Let's dive to learn what empowers deep reinforcement learning.\nTable of Contents\n Introduction Reinforcement Learning Concepts Reinforcement Learning Hints REINFORCE Algorithm Proximal Policy Optimization AlphaGo vs Lee Sedol Goodbye? Learn More  Introduction Reinforcement learning (RL) is a machine learning technique that is all about repetitive decision making. Well, just like when you go camping. During your first trip you realize that you need more food and you lack some tools. Next time, you take some more cereals and dried fruit, a better knife, and a pot. And after yet several more attempts you invest in better shoes or a backpack and learn how to arrange your stuff in a neat way. The journeys become longer and more enjoyable. As you can see, you iterate. You learn from experience. And you get rewarded.\nNo matter how much you can relate yourself to the experiences above, they contain everything that reinforcement learning has. An agent is learning from experience (or from mistakes if you want) by interacting with its environment. The agent receives some sort of a reward signal in response to its actions. And that is basically the idea. An idea from psychology? Perhaps.\nIn a supervised learning setting1 you would be instructed what equipment to take with you. And you would need to act exactly as instructed. However, the real life is not like that. You might have watched a camping channel on YouTube and still struggle to start a fire. Trying different kinds of approaches yourself, however, improves your survival skills over time.\nYou may also notice that reinforcement learning is actually more general than supervised learning. Indeed, in supervised learning we usually talk about a single iteration and apply some sort of ground truth or target signal. However, let's take a look at the following example. Imagine, you are a director of AI at Tesla (those hipsterish cars, you know) and your team trains a self-driving car based on videos that were previously recorded. Sounds like a supervised problem with images and labels, right? The car performs well during the same day, but tomorrow it rains and the car refuses to drive well. What do you do? You say, perhaps the distribution of images has changed. Let's collect some more for a rainy weather. And the model training is repeated. The third day is very foggy and your team has to collect the data again. And train the model again. So what happens? Exactly! Iteration. You have an implicit reinforcement learning loop where it is you who acts as an agent trying to outsmart the weather conditions2.\nMore often than not present decisions will influence the situation, and thus they will influence future decisions too. This is perhaps the biggest difference from supervised learning. Supervised learning conveniently omits to confess that its main goal is to help making decisions in real world. The most boring vision task you have encountered, classifying MNIST images, at some point was actually useful for post offices to automate mail delivery. Similarly, predicting time series might be helpful getting an idea about climate change or predicting economic downturns. Whatever supervised learning benchmark you take, in its origin there is some sort of implication in the real world.\nUnlike supervised learning, reinforcement learning is all about decisions. It explicitly states the goal of achieving the best overall consequences -- by maximizing total reward. For instance, to arrive from point A to point B you may take a car. Depending on which route you will take and how frequently you will rest, this will affect your trip duration and how tired you will arrive. Those circumstances may further affect your plans. Decisions almost never occur in isolation. They will inevitably lead to new decisions. That is why real-life challenges are often accompanied by reinforcement learning in some form.\nA quick note. People often talk about deep reinforcement learning. This is to highlight the presence of neural networks. The fact does not change the essence of the method. These days almost everyone is doing deep reinforcement learning anyway, so I prefer to omit the term. Just wanted to make sure we stay on the same page. Now, we will focus our attention on the nitty-gritty details of reinforcement learning.\nReinforcement Learning Concepts By interacting with environment, an RL agent actually creates its own unique dataset, whereas in supervised learning the dataset is predefined. As the agent explores its environment and the dataset is being created we apply backprop to teach our agent similarly to supervised learning. It turns out there is a large variety of ways how to do that. The concepts below will give you a taste about the key ideas from reinforcement learning.\nPolicies, States, Observations A policy (denoted by $\\pi$) is simply a strategy that an agent uses when responding to changes in the environment. The following notation reads \u0026quot;a policy parametrized by parameters $\\phi$ to take an action $a$ given a state $s$\u0026quot;:\n$$ \\pi_{\\phi}(a|s). $$\nTypically $\\phi$ signifies weights in a neural network (the deep reinforcement learning story). In the notation above, by $s$ people usually mean \u0026quot;state\u0026quot;. And sometimes they actually mean \u0026quot;observations\u0026quot; $o$. The difference between the two is that observations are a projection of the complete state describing the environment. For example, when learning robotic manipulations from camera pixels, observations of a robotic hand are only a representation of the actual environment state from a certain angle. Sometimes people talk about \u0026quot;completely observable\u0026quot; environments, then $s = o$. The distinction between $s$ and $o$ does not affect the RL algorithms we are learning about today.\nOn-policy vs Off-policy Reinforcement learning algorithms can be split between on-policy and off-policy. The first ones use trajectories (experiences)3 generated by the policy itself, while the second ones use trajectories from some other policies. For instance, algorithms that use a replay buffer (such as deep Q networks playing Atari games) to store transitions are off-policy algorithms. Typically, off-policy algorithms have better sample efficiency compared to on-policy ones, which makes them attractive when it is costly to generate new experiences. For instance, when training a physical robot. On the other hand, on-policy algorithms are often used when acquiring new experiences are cheap. For instance, in simulation.\nOffline Reinforcement Learning Offline RL is a family of algorithms where an RL agent observes past experiences and tries to figure out a better policy without interaction with an environment. Of course this is a very challenging task. Nevertheless, it has many real-world applications where it could be dangerous or illegal to apply reinforcement learning experiments. For instance, finding the best treatment in the medical setting. A doctor would not experiment on a patient using different drugs because this can lead to health deterioration. Instead, they may use reinforcement learning techniques applied to previous records of other patients to figure out the best prescription (offline setting).\nDiscrete vs Continuous Actions Playing an arcade game using a manipulator with four buttons implies a discrete action setting (four possible actions). However, in reality there are settings with infinite or very large number of actions. For instance, robot joints positions. Certain algorithms can be applicable only to one action type (DQN can be applied only for discrete and DDPG only for continuous actions), whereas certain others can be used for both (e.g. A2C, PPO).\nDeterministic vs Stochastic Policies Certain policies inherently produce deterministic actions (DQN), while others sample from a learned distribution producing stochastic actions (PPO). Of course, it is possible to make a deterministic algorithm to produce stochastic actions (epsilon-greedy exploration) and vice versa. Why use a stochastic policy? Actually, under stochastic environments optimal policies are often stochastic. The simplest example is Rock-Paper-Scissors game.\nModel-free vs Model-based Model-based approaches assume a certain environment model. The advantage of this approach is better sample efficiency. On the other hand, model-free approaches may better generalize to unknown environments. In some cases, environment's model is learned.\nSample Efficiency By sample efficiency we mean using less examples for training.\nReinforcement Learning Hints The goal of reinforcement learning is typically to solve a real-life challenge. For that purpose you will need to define some interaction protocol between your agent and its environment and also a reward function. The reward function provides a score how well your agent performs. Typically, it is beneficial having a \u0026quot;dense\u0026quot; reward so that the agent gets some information at every time step. This is not always possible and there are methods to circumvent this issue. One way to look at reward shaping is that RL is your \u0026quot;compiler\u0026quot; and the reward is your understanding of the problem you are trying to solve.\nTo gain some expertise, you may want to start with existing environments4 and RL algorithms. If your goal is to train a physically embodied robot, you may want to create your own environment to try out some ideas in simulation. This will save you some time since running a simulator is typically faster than real-time robot training.\nThe more accurate is the simulator, the better the agent is likely to perform in real-world. For instance, in Sim-to-Real: Learning Agile Locomotion For Quadruped Robots the authors have obtained better results by creating accurate actuator models and performing latency modeling.\nHowever, there is always a difference between running an agent on any simulator and reality. The problem known as reality gap. There are different techniques how to reduce this difference. One of them is domain randomization. The idea is to train the agent using environments with randomized parameters. In active domain randomization, the active learning approach is combined: agents are trained more often on environments where they performed poorly.\nAnother technique domain adaptation suggests making real data look like those in simulation or vice versa. You may also want to perform parameters fine-tuning on your real robot.\nNote also that if you are training an agent in a simulated environment, the agent may \u0026quot;overfit\u0026quot; to the environment. That is, it may exhibit unrealistic (and often undesired) behaviors that still lead to high reward scores. The agent may try to exploit environment glitches if there are any.\nHaving those practical strategies in mind, we are going to the meat of RL. Below we are discussing an algorithm which constitutes the basis to many modern RL strategies such as TRPO and PPO.\nREINFORCE REINFORCE, also called Monte Carlo Policy Gradient, is the simplest from the policy gradient algorithms family. The advantage of policy gradients is that they can learn stochastic policies. This also addresses the exploration-exploitation dilemma.\nREINFORCE algorithm:\n Initialize policy parameters $\\phi$. Generate trajectories $s_0, a_0, r_1,...,s_{T-1}, a_{T-1}, r_T$ by interacting with environment. For every step $t = 0,1,...,T-1$, estimate returns $R_t = \\sum_{k=t+1}^T \\gamma^{k-t-1} r_k.$ Optimize agent parameters $\\phi \\leftarrow \\phi + \\alpha R_t \\nabla \\log \\pi_{\\phi}(a_t|s_t)$. Repeat steps 2-5 until convergence.  Note that when using universal function approximators (that is neural networks), convergence is not guaranteed. However, in practice you still have a good chance to train your agent.\nnumEpisodes = 400 main :: IO () main = do putStrLn $ \u0026#34;Num episodes \u0026#34; ++ show numEpisodes -- Seed Torch for reproducibility. -- Feel free to remove. manual_seed_L 10 let seed = 42 -- Environment seed -- Step 1: Initialize policy parameters agent \u0026lt;- mkAgent obsDim actionDim -- We also initialize the optimizer let trainer = mkTrainer agent -- Repeat steps 2-4 for the number of episodes (trajectories) (agent\u0026#39;, trainer\u0026#39;) \u0026lt;- foldM (\\at i -\u0026gt; (reinforce conf) seed at i) (agent, trainer) [1..numEpisodes] return () reinforce cfg@Config {..} seed (agent, trainer) i = do -- Step 2: Trajectories generation (rollout) let s0 = Env.reset (seed + i) (_, _, !rs, !logprobs_t) \u0026lt;- rollout maxSteps agent s0 let logprobs\u0026#39; = cat (Dim 0) logprobs_t putStrLn $ \u0026#34;Episode \u0026#34; ++ show i ++ \u0026#34; - Score \u0026#34; ++ show (sum rs) -- Step 3: Estimating returns let returns_t = asTensor $ returns γ rs returnsNorm = (returns_t - mean returns_t) / (std returns_t + 1e-8) -- Step4: Optimize optimize cfg logprobs\u0026#39; returnsNorm (agent, trainer) Trajectory Generation The agent generates a trajectory by interacting with the environment. We call this a rollout. By observing the function signature below, we can tell that the function consumes an integer number, an agent, and environment state. This integer is simply the maximal number of steps per episode. As a result, the function will provide the trajectory: observations, actions, and rewards. In addition, it also provides log probabilities, which we will use to compute our objective (or loss) before we can optimize parameters in Step 4.\nrollout :: Int -\u0026gt; Agent -\u0026gt; Env.State -\u0026gt; IO ([Observation], [[Int]], [Reward], [Tensor]) -- ^ Observations, actions, rewards, log probabilities Here is how we implement it: we benefit from the excellent unfoldM function that has type Monad m =\u0026gt; (s -\u0026gt; m (Maybe (a, s))) -\u0026gt; s -\u0026gt; m [a]. That is we iterate as long as the function in the first argument (s -\u0026gt; m (Maybe (a, s))) provides a Just value (and stop when Nothing).\n Haskell libraries provide lots of \u0026quot;pieces of code\u0026quot; or \u0026quot;programming templates\u0026quot;: functions like map, foldl, scanr, unfoldr, zip, zipWith, etc. All those are compact versions of loops! Some of those concepts gradually diffuse into imperative languages (such as C++ and Python).\n rollout _maxSteps agent s0 = L.unzip4 \u0026lt;$\u0026gt; unfoldM f (_maxSteps, agent, s0) where -- Reached max number of steps. Nothing = stop. f (0, _, _) = pure Nothing f (_maxSteps, _agent@Agent{..}, _s) = do if Env.isDone _s -- The environment is done: stop. then do pure Nothing else do let ob = Env.observations _s (ac@(Action ac_), logprob) \u0026lt;- getAction ϕ ob let (r, s\u0026#39;) = Env.step ac _s pure $ Just ((ob, ac_, r, logprob), (_maxSteps - 1, _agent, s\u0026#39;)) The heart of iteration is in the end: First, observe the environment and sample actions.\nlet ob = Env.observations _s (ac@(Action ac_), logprob) \u0026lt;- getAction ϕ ob Then, simulate the environment step:\nlet (r, s\u0026#39;) = Env.step ac _s And finally, prepare the next iteration step by reducing the maximal number of steps and passing the agent and the new environment state.\npure $ Just ((ob, ac_, r, logprob), (_maxSteps - 1, _agent, s\u0026#39;)) Here is how we get an action and a log probability.\ngetAction ϕ obs = do let obs_ = unsqueeze (Dim 0) $ asTensor obs (ac, logprob) \u0026lt;- evaluate ϕ obs_ Nothing -- Return a single discrete action return (Action [asValue ac], logprob) Evaluate the policy $\\pi_{\\phi}$: If no action provided, sample a new action from the learned distribution. Also get log probabilities for the action.\nevaluate ϕ obs a = do let probs = policy ϕ obs dist = Categorical.fromProbs probs action \u0026lt;- _getAct a dist let logProb = D.logProb dist action pure (action, logProb) where -- Sample from the categorical distribution: -- get a tensor of integer values (one sample per observation). _getAct Nothing dist = D.sample dist [head $ shape obs] _getAct (Just a\u0026#39;) _ = pure a\u0026#39; Estimating Returns After a rollout we (retrospectively) compute how good were our decisions during the whole trajectory. The trick is that we start from the last step. That is, our return $R$ at time step $t$ is our current reward plus a discounted future return.\n$$ R_t = r_t + \\gamma R_{t+1}. $$\nThis expands into\n$$ R_t = r_t + \\gamma (r_{t+1} + \\gamma (r_{t+2} + \\gamma( ... ))). $$\nwhere $r_t$ is the reward at time $t$. The meaning of this expression is the essence of policy gradient: We evaluate how good were our past decisions with respect to the future outcomes.\n An intriguing way to look at discount factor $\\gamma$ is by using the concept of terminal state (to put simply, death). At each future step, the agent can get a reward with probability $\\gamma$ and can die with probability $1 - \\gamma$. Therefore, discounting the reward is akin to incorporating the \u0026quot;fear of death\u0026quot; into RL. In his lectures, S. Levine gives an example of receiving 1000 dollars today or in million years. In the latter case, the reward is hugely discounted as it is unlikely that we will be able to receive it.\n We compute the returns as a function of rewards rs. We also use next terminal states indicators and future value estimators in special cases.\nreturns γ rs = f rs where f (r:[]) = [r] f (r:xs) = let y = f xs -- Discounting a future return in r + γ * (head y) : y  In policy gradient algorithms we evaluate how good are our past decisions with respect to the future outcomes.\n Note that we compute the return recursively as in equation above: reward plus a discounted future return. Since Haskell is a lazy programming language, it is not critical that this value does not exist yet. Essentially, we promise to compute the list of future values y = f xs. Finally, we prepend current return to the list of future returns r + γ * (head y) : y. Fascinating!\nf (r:xs) = let y = f xs in r + γ * (head y) : y Optimizing parameters  Computing the loss. Running a gradient step.  optimize :: Config -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; (Agent, Trainer) -\u0026gt; IO (Agent, Trainer) optimize Config {..} logprobs_t returns_t (Agent {..}, Trainer {..}) = do let loss = Torch.sumAll $ -logprobs_t * returns_t (ϕ\u0026#39;, opt\u0026#39;) \u0026lt;- runStep ϕ opt loss (asTensor lr) pure (Agent ϕ\u0026#39;, Trainer opt\u0026#39;) Final Bits There are still a few other things to complete the project. Let's define our policy network type $\\Phi$. Here we have three fully-connected layers. In other words, two hidden layers.\ndata Phi = Phi { pl1 :: Linear , pl2 :: Linear , pl3 :: Linear } deriving (Generic, Show, Parameterized) Now we can implement the forward pass in a Policy Network:\npolicy :: Phi -\u0026gt; Tensor -\u0026gt; Tensor policy Phi {..} state = let x = ( linear pl1 ~\u0026gt; tanh ~\u0026gt; linear pl2 ~\u0026gt; tanh ~\u0026gt; linear pl3 ~\u0026gt; softmax (Dim 1)) state in x An agent is simply a policy network in Reinforce. We will update this data type to also accommodate the critic network in improved policy gradient later on:\ndata Agent = Agent { ϕ :: Phi } deriving (Generic, Show) REINFORCE Trainer type is a single optimizer.\ndata Trainer = Trainer { opt :: Adam } deriving Generic A new, untrained agent with random weights\nmkAgent :: Int -\u0026gt; Int -\u0026gt; IO Agent mkAgent obsDim actDim = do let hiddenDim = 16 ϕ \u0026lt;- samplePhi obsDim actDim hiddenDim pure $ Agent ϕ Parameters $\\phi \\in \\Phi$ initialization\nsamplePhi :: Int -\u0026gt; Int -\u0026gt; Int -\u0026gt; IO Phi samplePhi obsDim actDim hiddenDim = Phi \u0026lt;$\u0026gt; sample (LinearSpec obsDim hiddenDim) \u0026lt;*\u0026gt; sample (LinearSpec hiddenDim hiddenDim) \u0026lt;*\u0026gt; sample (LinearSpec hiddenDim actDim) Initializing the trainer\nmkTrainer :: Agent -\u0026gt; Trainer mkTrainer Agent {..} = let par = flattenParameters ϕ opt = mkAdam 0 0.9 0.999 par in Trainer opt Now, let's train the agent to solve the classic CartPole environment! See the complete REINFORCE project on Github.\nProximal Policy Optimization REINFORCE algorithm is nice because it is simple. On the other hand, in practice it has a problem: finding the best meta-parameters, such as learning rate. Choose a value to low and training needs more samples, too high and it does not converge. To alleviate this training instability multiple variations of this algorithm have been proposed. One popular variation is called Proximal Policy Optimization (PPO). Below we upgrade REINFORCE to PPO.\nActor-Critic Style In REINFORCE we only had a policy network $\\pi_{\\phi}$. A more advanced version would be not only to use a policy network (so-called actor), but also a value network (critic). This value network would estimate how good is the state we are currently in. Naturally, the output of the critic network is a scalar with this estimated state value.\n-- Value (Critic) Network type: Three fully-connected layers data Theta = Theta { l1 :: Linear , l2 :: Linear , l3 :: Linear } deriving (Generic, Show, Parameterized) -- | Forward pass in a Critic Network critic :: Theta -\u0026gt; Tensor -\u0026gt; Tensor critic Theta {..} state = let net = linear l1 ~\u0026gt; tanh ~\u0026gt; linear l2 ~\u0026gt; tanh ~\u0026gt; linear l3 in net state Here is how we will sample initial network weights:\nsampleTheta :: Int -\u0026gt; Int -\u0026gt; IO Theta sampleTheta obsDim hiddenDim = Theta \u0026lt;$\u0026gt; sample (LinearSpec obsDim hiddenDim) \u0026lt;*\u0026gt; sample (LinearSpec hiddenDim hiddenDim) \u0026lt;*\u0026gt; sample (LinearSpec hiddenDim 1) To make our life a bit simpler, we can also use the following wrapper when dealing with raw observations.\n-- | Get value: Convenience wrapper around `critic` function. value :: Theta -\u0026gt; [Float] -\u0026gt; Float value θ ob = let ob_ = unsqueeze (Dim 0) $ asTensor ob in asValue $ critic θ ob_ The state value given by the critic network will be used for advantage estimation, instead of simply calculating discounted returns.\nAdvantage Estimation As you remember, in policy gradients we optimize neural network parameters using $\\mathbf{A} \\cdot \\nabla \\log \\pi_{\\phi}(a_t|s_t)$. In REINFORCE, this term $\\mathbf{A} = R_t$ is discounted returns. This totally makes sense. However, this is not the only option5.\nLet me introduce advantage $A = r - v$ where $r$ is reward and $v$ is value, i.e. how good is our action. The advantage tells us how current action is better than an average action. Therefore, by optimizing our policy with respect to advantage, we would improve our actions compared to average. This would help to reduce variance of policy gradient.\nIn PPO, we typically use a fancy way to estimate the advantage called generalized advantage estimator (GAE).\nadvantages γ γ\u0026#39; rs dones vs = f $ L.zip4 rs (L.drop 1 dones) vs (L.drop 1 vs) -- vs are current values (denoted by v) and -- (L.drop 1 vs) are future values (denoted by v\u0026#39;) where -- Not necessary to reverse the list if using lazy evaluation f :: [(Float, Bool, Float, Float)] -\u0026gt; [Float] -- End of list to be reached: same as terminal (auxiliary value) f ((r, _, v, _):[]) = [r - v] -- Next state terminal f ((r, True, v, _):xs) = (r - v) : f xs -- Next state non-terminal f ((r, False, v, v\u0026#39;):xs) = let a = f xs delta = r + γ * v\u0026#39; - v in delta + γ * γ\u0026#39; * (head a) : a PPO Specifics In REINFORCE, the policy gradient loss function was simply\n$$L(\\phi) = -\\sum_{t} \\log \\pi_{\\phi}(a_t|s_t) \\cdot R_t.$$\nNote the negation sign in front: without it, the loss (to be minimized) becomes an objective (to be maximized) - earning highest returns6:\n$$L(\\phi) = \\sum_{t} \\log \\pi_{\\phi}(a_t|s_t) \\cdot R_t.$$\nNow, what distinguishes PPO, it its objective. It consists of three components: a clipped policy gradient objective (actor network), value loss (critic network), and entropy bonus. Since the value function is a loss term, it has a minus sign:\n$$L_t^{PPO} = L_t^{CLIP} - c_1 L^{VF} + c_2 S,$$\nwhere $c_1 = 0.5$ and $c_2=0.01$ are constants. If we are going to minimize loss instead,\n$$L_t^{PPO} = -L_t^{CLIP} + c_1 L^{VF} - c_2 S = $$ $$ = L_t^{PG} + c_1 L^{VF} - c_2 S.$$\nloss = pg + vfC `mulScalar` vLoss - entC `mulScalar` entropyLoss Now, the most interesting part of PPO. If we denote $r_t(\\phi)$ the probability ratio $r_t(\\phi) = \\frac{\\pi_{\\phi}(a_t|s_t)}{\\pi_{\\phi_{\\text{old}}}(a_t|s_t)}$. Then PPO is maximizing the following objective\n$$ L^{CLIP}(\\phi) = \\mathbb{\\hat E}_t \\left( \\min \\left( r_t(\\phi) \\hat A_t, \\text{clip}(r_t(\\phi), 1 - \\epsilon, 1 + \\epsilon) \\hat A_t \\right) \\right).$$\nFirst, let's take a look at the clipped objective $\\text{clip}(r_t(\\phi), 1 - \\epsilon, 1 + \\epsilon) \\hat A_t$. Here, we try to restrict how far our policy will move during the policy gradient update. If advantage $A$ is positive, the objective is clipped at value $1 + \\epsilon$. If advantage $A$ is negative, then the objective is clipped at $1 - \\epsilon$. Finally, we take a min between a clipped and unclipped objective. Therefore, the final objective is a pessimistic bound on the unclipped objective (see Schulman et al.).\nNow, transforming the objective into a loss: we add a minus sign and replace min with max:\n$$ L^{PG}(\\phi) = \\sum_t \\max \\left( -r_t(\\phi) \\hat A_t, -\\text{clip}(r_t(\\phi), 1 - \\epsilon, 1 + \\epsilon) \\hat A_t \\right).$$\npgLoss clipC logratio advNorm = let ratio = exp logratio ratio\u0026#39; = clamp (1 - clipC) (1 + clipC) ratio pgLoss1 = -advNorm * ratio pgLoss2 = -advNorm * ratio\u0026#39; in mean $ max\u0026#39; pgLoss1 pgLoss2 Note that for better numerical properties we typically use logratio, instead of ratio (a log of a ratio is the difference of logs):\nlogratio = newlogprobs - logprobs_t where newlogprobs are calculated by evaluating the new policy.\nNext term $L_t^{VF}$ is often a squared-error loss $(V_{\\theta}(s_t) - V_t^{\\text{targ}})^2$.\nvLoss = mean (newvalues - returns)^2 However, we use clipped value loss.\nclippedValueLoss clipC val newval ret = let lossUnclipped = (newval - ret)^2 clipped = val + (clamp (-clipC) clipC (newval - val)) lossClipped = (clipped - ret)^2 lossMax = max\u0026#39; lossUnclipped lossClipped in mean lossMax Finally, the entropy bonus $S$ is used in order to stimulate exploration, i.e. performing the same task by as many ways as possible.\nentropyLoss = mean entropy Here is updated evaluate function:\n-- | Get action, logProb, and entropy tensors evaluate :: Phi -- ^ Policy weights -\u0026gt; Tensor -- ^ Observations -\u0026gt; Maybe Tensor -- ^ Action -\u0026gt; IO (Tensor, Tensor, Tensor) evaluate ϕ obs a = do let probs = policy ϕ obs dist = Categorical.fromProbs probs action \u0026lt;- _getAct a dist let logProb = D.logProb dist action entropy = D.entropy dist pure (action, logProb, entropy) where _getAct :: Maybe Tensor -\u0026gt; Categorical.Categorical -\u0026gt; IO Tensor _getAct Nothing dist = D.sample dist [head $ shape obs] _getAct (Just a\u0026#39;) _ = pure a\u0026#39; Putting it all together, we get this optimize function:\noptimize :: Config -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; (Agent, Trainer) -\u0026gt; IO (Agent, Trainer) optimize Config {..} obs_t acs_t val_t logprobs_t advantages_t returns_t (Agent {..}, Trainer {..}) = do (_, newlogprobs, entropy) \u0026lt;- evaluate ϕ obs_t (Just acs_t) let newvalues = critic θ obs_t logratio = newlogprobs - logprobs_t -- Normalized advantages advNorm = (advantages_t - mean advantages_t) / (std advantages_t + 1e-8) pg = pgLoss clipC logratio advNorm vLoss = clippedValueLoss clipC val_t newvalues returns_t entropyLoss = mean entropy loss = pg + vfC `mulScalar` vLoss - entC `mulScalar` entropyLoss ((θ\u0026#39;, ϕ\u0026#39;), opt\u0026#39;) \u0026lt;- runStep (θ, ϕ) opt loss (asTensor lr) pure (Agent θ\u0026#39; ϕ\u0026#39;, Trainer opt\u0026#39;) See the complete code on Github.\nAlphaGo vs Lee Sedol We started this article with the defeated Go champion Lee Sedol. He played against software called AlphaGo (version Lee, after him). How actually did AlphaGo win? What is self-play and how does it relate to reinforcement learning? Below we will address those questions.\nOverview AlphaGo generates data by playing games against an opponent (since the game of Go is a two-player game). This opponent is another machine player randomly chosen from a pool of opponents. In the end of the game one player wins (reward $r = +1$) and another loses (reward $r = -1$).\nAfter several games have been played, the player is updated. Then, this new player is compared to the old one. If the new one wins sufficiently often, it is then accepted. Iteration by iteration, and the AlphaGo is improved by playing against itself. This is called a self-play. Below, we are going into mode details.\nMonte Carlo Tree Search We should introduce a new concept called Monte Carlo Tree Search. Monte Carlo, if you didn't know, is the area in the city-state of Monaco. It is also the European gambling capital. Some rich people like to waste money in the Monte Carlo casino. The author had a chance to visit it once and can confirm that is absolutely true.\nAnyway, I digress. Statisticians simply love calling everything Monte Carlo when there are random simulations involved. For example, have you noticed that REINFORCE is also named Monte Carlo Policy Gradient? This is related to stochastic trajectories generated during rollouts.\n Figure 1  Monte Carlo Tree Search. Source: Silver et al.   Monte Carlo Tree Search (MCTS) explores a tree of possible moves and is continuously refining its understanding of the game state by simulating random rollouts (also called playouts because of the game aspect). In Monte Carlo tree, each node is a board state (see Fig. 1). For each board state, we estimate the value of this state $Q$. That is how good it is or, in other words, whether we believe this board position is leading to a win or loss. The innovation behind AlphaGo was in combining MCTS with convolutional neural networks for state value estimation and for selecting the move to play.\nIn the first MCTS stage called Selection (Fig. 1a), an action is selected to maximize the value $Q$ plus some bonus $u(P)$ that encourages exploration. This bonus is designed such that in the beginning the algorithm prefers actions with high prior probability $P$ and low visit count; and eventually it prefers actions with high action value $Q$. This is achieved by weighting the bonus term by an exploration constant.\nAfter a certain depth of Monte Carlo tree is reached, the second stage Evaluation (Fig. 1c) is performed: current position (leaf node in tree) is evaluated using the value network. And the actions (game moves) are selected according to the policy network $\\pi$ until the end of the game (terminal state), which leads to the outcome $\\pm r$.\nFinally, the Backup is performed (Fig. 1d): the rollout statistics are updated by adding the outcome in a backward pass through the Monte Carlo tree. In the end, the value estimate $Q(s,a)$ becomes a weighted sum of the value network and just obtained rollout statistics. At the end of the search the algorithm selects an action with the maximum visit count. The authors state that this is less sensitive to outliers as compared to maximizing action value.\nOne more thing: when the number of certain node visits is frequent, the successor state is added to the tree. This is called Expansion (Fig. 1b).\nComing back to the neural networks you might be pleased to learn that the policy network $\\pi$ was trained using the REINFORCE algorithm you already know. Each iteration consisted of a minibatch of several games played between the current policy network $\\pi_{\\phi}$ and an opponent $\\pi_{\\phi}-$ using parameters from the previous iteration. Every 500 iterations, parameters $\\phi$ were saved to the opponent pool (so that there is always a variety of opponents to choose from).\nThe value network was trained to approximate the value function of the RL policy network $\\pi_{\\phi}$. This was a regression task. The network was trained on a dataset of 30 million positions drawn from the self-play. There are many interesting technical aspects I suggest to read about in the original publication. To summarize, Monte Carlo Tree Search was employed in AlphaGo because an exhaustive search is simply impossible as the game tree quickly grows too large. The idea of MCTS combined with neural networks is conceptually simple and provided sufficient computational resources, it wins.\nInstead of Conclusion Beating the strongest human player is an amazing feat by the DeepMind team. However, we should not forget that behind the scenes there was operating a whole data center to support AlphaGo computation. This is about six orders of magnitude more power consumption compared to the human brain (~20W)! Devising energy-efficient AI hardware is therefore our next milestone we are heading to.\nCitation  @article{penkovsky2023RL, title = \"Beyond Supervised Learning\", author = \"Penkovsky, Bogdan\", journal = \"penkovsky.com\", year = \"2023\", month = \"May\", url = \"https://penkovsky.com/neural-networks/beyond/\" }  If you like the article please consider sharing it.\nGoodbye? I have to admit that after all these days we have barely scratched the surface. Yet, I am happy about the journey we have made. We have seen how to create neural networks and to benefit from them; how to estimate model uncertainty; how to generate new things. And today we have learned how to continuously make decisions. There are so many more ideas to explore! Reinforcement learning is a rabbit hole in itself. By the way, did you know that ChatGPT is secretly using PPO to get better answers?\nLet me know if you have any remarks.\nLearn More  D. Silver. Policy Gradient - Lecture Understanding the role of the discount factor Hugging Face Deep RL Course Learning dexterous in-hand manipulation Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience Noise and The Reality Gap Policy Gradient with PyTorch Proximal Policy Optimization Algorithms Implementation matters in deep policy gradients Mastering the game of Go with deep neural networks and tree search Mastering the game of Go without human knowledge  MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });   This is what we were doing all the days before. Supervised learning. That is how it is called. One of my favourite teachers used to quote Molière, \u0026quot;Par ma foi, il y a plus de quarante ans que je dis de la prose, sans que j'en susse rien\u0026quot; (\u0026quot;These forty years now I've been speaking in prose without knowing it!\u0026quot;). ^ Honestly, I have no clue how a director of AI works at Tesla. But if you Andrej Karpathy and you are reading this, please share your past experiences. I would appreciate. ^ A trajectory is a repetitive sequence of observations and actions. ^ In Haskell it is possible to benefit from existing OpenAI Gym environments via Gym HTTP API. ^ See D. Silver's Lecture about Policy Gradients. ^ PyTorch seems to be better at minimizing rather than maximizing. ^  ","date":1683831600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685569350,"objectID":"a0d275fffbbcf08f730bfb7326c1ba55","permalink":"https://penkovsky.com/neural-networks/beyond/","publishdate":"2023-05-11T21:00:00+02:00","relpermalink":"/neural-networks/beyond/","section":"neural-networks","summary":"Ever wondered how machines defeated the best human Go player Lee Sedol in 2016? A historical moment for the game that was previously considered to be very tough. What is reinforcement learning that generates so much buzz recently? Superhuman performance playing arcade Atari games, real-time Tokamak plasma control, Google data center cooling, and autonomous chemical synthesis are all the recent achievements behind the approach. Let's dive to learn what empowers deep reinforcement learning.\n","tags":["Deep Learning","Haskell"],"title":"Day 10: Beyond Supervised Learning","type":"neural-networks"},{"authors":null,"categories":null,"content":"","date":1661969100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661969100,"objectID":"744d41389ce01decbc4043ab8e6d2274","permalink":"https://penkovsky.com/talk/towards-autonomous-synthesis2022/","publishdate":"2022-08-31T20:05:00+02:00","relpermalink":"/talk/towards-autonomous-synthesis2022/","section":"talk","summary":"Reinforcement learning (RL) is a general learning and decision making paradigm based on interaction with the environment. Despite being challenging in practice, implementing RL for real world tasks could improve the safety and efficiency of autonomous processes. In this work we apply RL for an autonomous molecule synthesis with continuous flow chemistry. In our preliminary experiments, we demonstrate that our agent trained by deep reinforcement learning is capable of responding to real-time challenges, such as changes in the environment, sensor noise, and perturbations in order to ensure the optimal chemical synthesis conditions. The ultimate goal of this work is to conceive an autonomous molecule synthesis unit.","tags":[],"title":"Towards Autonomous Synthesis With Deep Reinforcement Learning","type":"talk"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Imagine you are a designer and you want a new font: A little bit heavier, with rounder letters, more casual or a little bit more fancy. Could this font be created just by tuning a handful of parameters? Or imagine that you are a fashion designer and you would like to create a new collection as a mix of previous seasons? Or that you are a musician desperately looking for inspiration. How about a new song that mixes your mood and Beethoven's Symphony No 3? It turns out, all this is actually possible. To better illustrate the concept, here is some music interpolation:\n  Don't forget to check out\n Day 8: Model Uncertainty Estimation Day 7: Real World Deep Learning  The complete project is also available on Github.\nThe Secret Space  latent - present and capable of emerging or developing but not now visible, obvious, active, or symptomatic\n—Webster's Dictionary\n Autoencoders    A simple autoencoder with latent dimension $L$.   How do we implement an autoencoder? Let us take a multilayer network from the image above: $784 \\rightarrow 400 \\rightarrow L \\rightarrow 400 \\rightarrow 784$. Where the latent space dimension $L$ is some smaller number such as $20$ or maybe even $2$. The $L$-sized latent vector $z$ is often called a bottleneck. The left part from the bottleck is called an encoder $q_{\\phi}$ and the part on the right, a decoder $p_{\\theta}$. Where $\\phi$ and $\\theta$ are trainable parameters of encoder and decoder, respectively.\nThe encoder takes an input (like an image) and generates a compact representation, typically a vector. It is also not a mistake to call it a compressed representation. The decoder takes this compact representation and creates an output as close as possible to the original input. Hence the name, autoencoder. Of course, some information is lost due to the dimensionality reduction. Therefore, the goal of a autoencoder is to find the most relevant features to preserve as much information about the input object as possible.\ndata AE = AE { -- Encoder parameters l1 :: Linear, l2 :: Linear, -- Decoder parameters l3 :: Linear, l4 :: Linear } deriving (Generic, Show, Parameterized) Then, the whole autoencoder network is\nae :: AE -\u0026gt; Tensor -\u0026gt; (Tensor, Tensor) ae AE {..} = linear l1 -- Encoder ~\u0026gt; relu ~\u0026gt; linear l2 ~\u0026gt; relu -- Decoder ~\u0026gt; linear l3 ~\u0026gt; relu ~\u0026gt; linear l4 ~\u0026gt; sigmoid We can also specify the exact dimensions\naeConfig = AESpec (LinearSpec 784 400) (LinearSpec 400 latent_size) (LinearSpec latent_size 400) (LinearSpec 400 784) Of course, a smaller $L$ (latent_size), the more information is lost. Therefore, depending on the application we may want to change this value. As a rule of thumb, $L$ contains the smallest number of neurons to enforce the compression. In this article we set $L=2$ so that we can simply reveal our latent space in two dimensions.\nVariational autoencoder The principal difference of variational autoencoders (VAE) from normal autoencoders is in the bottleneck. Instead of a compressed input, it estimates a distribution. In practice, VAE estimates the mean $\\mu$ and the standard deviation $\\sigma$ -- normal distribution parameters. By sampling from that distribution, a new unseen before object can be generated. Like a new font or a new piece of cloth. Or a face. Or a melody. Isn't that nice?\n Variational autoencoder. Arrows signify fully-connected layers and vertical bars are data vectors.   data VAESpec = VAESpec { -- Encoder trainable parameters (phi) fc1 :: LinearSpec, fcMu :: LinearSpec, fcSigma :: LinearSpec, -- Decoder trainable parameters (theta) fc5 :: LinearSpec, fc6 :: LinearSpec } deriving (Show, Eq) myConfig = VAESpec (LinearSpec 784 400) (LinearSpec 400 latent_size) (LinearSpec 400 latent_size) (LinearSpec latent_size 400) (LinearSpec 400 784) data VAE = VAE { l1 :: Linear, lMu :: Linear, lSigma :: Linear, l4 :: Linear, l5 :: Linear } deriving (Generic, Show, Parameterized) It can be useful to have separate encode $q_{\\phi}(z|x)$ and decode $p_{\\theta}(x|z)$ functions.\nencode :: VAE -\u0026gt; Tensor -\u0026gt; (Tensor, Tensor) encode VAE {..} x0 = let enc_ = linear l1 ~\u0026gt; relu x1 = enc_ x0 mu = linear lMu x1 logSigma = linear lSigma x1 in (mu, logSigma) decode :: VAE -\u0026gt; Tensor -\u0026gt; Tensor decode VAE {..} = linear l4 ~\u0026gt; relu ~\u0026gt; linear l5 ~\u0026gt; sigmoid Then, the complete variational autoencoder will be\nvaeForward :: VAE -\u0026gt; Bool -\u0026gt; Tensor -\u0026gt; IO (Tensor, Tensor, Tensor) vaeForward net@(VAE {..}) _ x0 = do let (mu, logSigma) = encode net x0 sigma = exp (0.5 * logSigma) eps \u0026lt;- toLocalModel\u0026#39; \u0026lt;$\u0026gt; randnLikeIO sigma let z = (eps `mul` sigma) `add` mu reconstruction = decode net z return (reconstruction, mu, logSigma) Pay a special attention to the reparametrization:\n$$\\varepsilon \\sim \\mathcal{N}(0,\\,1)$$ $$z = \\varepsilon \\odot \\sigma + \\mu$$\nWhere $z$ is our latent vector, noise $\\varepsilon$ is sampled from the normal distribution (randnLikeIO1), and $\\odot$ is elementwise product. Thanks to this trick, we can backpropagate through a stochastic layer. See this excellent post for more details. There are two differences between variational and ordinary autoencoder training: (1) the reparametrization and (2) the loss function, which we cover below.\nVAE Loss Function The loss function consists of two parts:\n$$ \\rm{loss} = \\mathbb{L}(x, \\hat x) + \\rm{D}_\\rm{KL} \\left(q_\\phi(z) || p_\\theta(z) \\right).$$\n$\\mathbb{L}(x, \\hat x)$ is the reconstruction loss. It decreases when the decoded output $\\hat x$ is closer to the original input $x$. This is basically the loss of an ordinary autoencoder. For instance, it can be binary cross-entropy loss or L2 loss.\nAnd the second term $\\rm{D_{KL}}( \\cdot )$ is the Kullback-Leibner divergence. It tells how much the distribution $p_\\theta(z)$ is different from the distribution $q_\\phi(z)$. Or even more informative is to say that KL divergence tells how much information is lost if the distribution $p_\\theta$ is used to represent $q_\\phi$. From the original paper,\n$$ -\\rm{D_{KL}}\\left(q_\\phi(z) || p_\\theta(z) \\right) =\\frac 1 2 \\sum_{j=1}^J \\left( 1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right).$$\nTherefore, the complete VAE loss is\nvaeLoss :: Float -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor -\u0026gt; Tensor vaeLoss beta recon_x x mu logSigma = reconLoss + asTensor beta * kld where reconLoss = bceLoss recon_x x kld = -0.5 * sumAll (1 + logSigma - pow (2 :: Int) mu - exp logSigma) We also include the $\\beta \\ge 0$ term. When $\\beta = 0$ the networks is trained as an ordinary autoencoder. When $\\beta = 1$, we have a classical VAE. And when $\\beta \u0026gt; 1$, we force latent vector representations disentanglement. As we can see from the image below, in case of disentanglement, there are separate latent variables that encode position, rotation, and scale. Whereas entangled variables tend to encode all object properties at the same time. I recommend the excellent article by Higgins et al., which is featuring some insights from neuroscience.\n   Disentangled vs entangled latent representations. Image source: Higgins et al. 2016.   The binary cross-entropy loss is defined as\nbceLoss target x = -sumAll (x * Torch.log(1e-10 + target) + (1 - x) * Torch.log(1e-10 + 1 - target)) We add a small term $10^{-10}$ to avoid numerical errors due to $\\log(0)$.\nVisualizing the Latent Space Here is how our latent space looks for different values of $\\beta$.\n       Test data distributions for different $\\beta$ values.   And here is how we compute that\nmain = do (trainData, testData) \u0026lt;- initMnist \u0026#34;data\u0026#34; net0 \u0026lt;- toLocalModel\u0026#39; \u0026lt;$\u0026gt; sample myConfig beta_: _ \u0026lt;- getArgs putStrLn $ \u0026#34;beta = \u0026#34; ++ beta_ let beta = read beta_ trainMnistStream = V.MNIST { batchSize = 128, mnistData = trainData } testMnistStream = V.MNIST { batchSize = 128, mnistData = testData } epochs = 20 cpt = printf \u0026#34;VAE-Aug2022-beta_%s.ht\u0026#34; beta_ logname = printf \u0026#34;beta_%s.log\u0026#34; beta_ putStrLn \u0026#34;Starting training...\u0026#34; net\u0026#39; \u0026lt;- time $ train beta trainMnistStream epochs net0 putStrLn \u0026#34;Done\u0026#34; -- Saving the trained model save\u0026#39; net\u0026#39; cpt -- Restoring the model net \u0026lt;- load\u0026#39; cpt -- Test data distribution in the latent space putStrLn $ \u0026#34;Saving test dataset distribution to \u0026#34; ++ logname _ \u0026lt;- testLatentSpace logname testMnistStream net Where\ntestLatentSpace :: FilePath -\u0026gt; V.MNIST IO -\u0026gt; VAE -\u0026gt; IO () testLatentSpace fn testStream net = do runContT (streamFromMap (datasetOpts 2) testStream) $ recordPoints fn net. fst recordPoints :: FilePath -\u0026gt; VAE -\u0026gt; ListT IO (Tensor, Tensor) -\u0026gt; IO () recordPoints logname net = P.foldM step begin done. enumerateData where step :: () -\u0026gt; ((Tensor, Tensor), Int) -\u0026gt; IO () step () args = do let ((input, labels), i) = toLocalModel\u0026#39; args (encMu, _) = encode net input batchSize = head $ shape encMu let s = toStr $ Torch.cat (Dim 1) [reshape [-1, 1] labels, encMu] appendFile logname s return () done () = pure () begin = pure () toStr :: Tensor -\u0026gt; String toStr dec = let a = asValue dec :: [[Float]] b = map (unwords. map show) a in unlines b Now, let's take a walk around our latent space to get an idea how it looks like inside.\nmain = do net \u0026lt;- load\u0026#39; \u0026#34;VAE-Aug2022-beta_1.ht\u0026#34; let xs = [-3,-2.7..3::Float] -- 2D latent space as a Cartesian product zs = [ [x,y] | x\u0026lt;-xs, y\u0026lt;-xs ] decoded = Torch.cat (Dim 0) $ map (decode net. toLocalModel\u0026#39;. asTensor. (:[])) zs writeFile \u0026#34;latent_space_2D.txt\u0026#34; (toStr decoded)    Some examples from 2D latent space.   Pretty neat! We see gradual transitions between different digits. Note that these digits are actually generated by VAE.\nConvNet VAE While we have built a simple variational autoencoder based on MLP (multilayer perceptron), nothing prevents us from using other architectures. In fact, let's build a convolutional VAE!\ndata VAESpec = VAESpec { -- Encoder trainable parameters conv1 :: Conv2dSpec, conv2 :: Conv2dSpec, conv3 :: Conv2dSpec, fcMu :: LinearSpec, fcSigma :: LinearSpec, -- Decoder trainable parameters fc :: LinearSpec, deconv1 :: ConvTranspose2dSpec, deconv2 :: ConvTranspose2dSpec, deconv3 :: ConvTranspose2dSpec } deriving (Show, Eq) myConfig = VAESpec (Conv2dSpec 1 32 4 4) -- 1 -\u0026gt; 32 channels; 4 x 4 kernel (Conv2dSpec 32 64 4 4) -- 32 -\u0026gt; 64 channels; 4 x 4 kernel (Conv2dSpec 64 128 3 3) -- 64 -\u0026gt; 128 channels; 3 x 3 kernel (LinearSpec (2 * 2 * 128) latent_size) (LinearSpec (2 * 2 * 128) latent_size) (LinearSpec latent_size 1024) (ConvTranspose2dSpec 1024 256 4 4) (ConvTranspose2dSpec 256 128 6 6) (ConvTranspose2dSpec 128 1 6 6) data VAE = VAE { c1 :: Conv2d, c2 :: Conv2d, c3 :: Conv2d, lMu :: Linear, lSigma :: Linear, l :: Linear, t1 :: ConvTranspose2d, t2 :: ConvTranspose2d, t3 :: ConvTranspose2d } deriving (Generic, Show, Parameterized) instance Randomizable VAESpec VAE where sample VAESpec {..} = VAE \u0026lt;$\u0026gt; sample conv1 \u0026lt;*\u0026gt; sample conv2 \u0026lt;*\u0026gt; sample conv3 \u0026lt;*\u0026gt; sample fcMu \u0026lt;*\u0026gt; sample fcSigma \u0026lt;*\u0026gt; sample fc \u0026lt;*\u0026gt; sample deconv1 \u0026lt;*\u0026gt; sample deconv2 \u0026lt;*\u0026gt; sample deconv3 encode :: VAE -\u0026gt; Tensor -\u0026gt; (Tensor, Tensor) encode VAE {..} x0 = let enc_ = -- Reshape vectors [batch_size x 784] -- into grayscale images of [batch_size x 1 x 28 x 28] reshape [-1, 1, 28, 28] -- Stride 2, padding 0 ~\u0026gt; conv2dForward c1 (2, 2) (0, 0) ~\u0026gt; relu ~\u0026gt; conv2dForward c2 (2, 2) (0, 0) ~\u0026gt; relu ~\u0026gt; conv2dForward c3 (2, 2) (0, 0) ~\u0026gt; relu ~\u0026gt; flatten (Dim 1) (Dim (-1)) x1 = enc_ x0 mu = linear lMu x1 logSigma = linear lSigma x1 in (mu, logSigma) decode :: VAE -\u0026gt; Tensor -\u0026gt; Tensor decode VAE {..} = linear l ~\u0026gt; relu ~\u0026gt; reshape [-1, 1024, 1, 1] -- Stride 2, padding 0 ~\u0026gt; convTranspose2dForward t1 (2, 2) (0, 0) ~\u0026gt; relu ~\u0026gt; convTranspose2dForward t2 (2, 2) (0, 0) ~\u0026gt; relu ~\u0026gt; convTranspose2dForward t3 (2, 2) (0, 0) ~\u0026gt; sigmoid ~\u0026gt; reshape [-1, 784] -- Reshape back And that is all we need.\nHere is the latent space for $\\beta=1$ (normal VAE) using our CNN architecture:\n  Disentanglement To better illustrate how parameter $\\beta$ encourages disentanglement between latent representations, let us first increase the latent dimension to $L=10$. For $\\beta=1$ and $\\beta=4$ we perform scan along each individual $z$ coordinate.\n    Indeed, the latent space under $\\beta=4$ looks more disentangled compared to $\\beta=1$. We can see e.g. that $z_1$ is the parameter that defines how \u0026quot;light\u0026quot; or how \u0026quot;bold\u0026quot; is the digit, whereas $z_2$ controls how wide is the digit. Whereas such individual components for $\\beta=1$ are hard to identify. For instance when $\\beta=1$, $z_4$ controls not only how \u0026quot;bold\u0026quot; is the digit, but also its shape.\nFor more details, see this notebook.\nFind the complete project and associated data on Github. For suggestions about the content feel free to open a new issue.\nSummary Variational autoencoder is a great tool in modern deep learning. Manipulating the latent space allows us not only to \u0026quot;interpolate\u0026quot; between different images or other objects, but also to perform inpainting (adding details to incomplete images) or even zero shot learning. The last one is crucial for the so-called artificial general intelligence.\nThe loss function is important for VAE training. If your VAE does not work as expected, the odds are that the loss function is not implemented correctly. Also check if the random noise $\\varepsilon$ is drawn from the normal distribution.\nCitation  @article{penkovsky2022VAE, title = \"Roaming The Latent Space\", author = \"Penkovsky, Bogdan\", journal = \"penkovsky.com\", year = \"2022\", month = \"August\", url = \"https://penkovsky.com/neural-networks/day9/\" }  Learn More  Auto-Encoding Variational Bayes A Beginner's Guide to Variational Methods: Mean-Field Approximation Early Visual Concept Learning with Unsupervised Deep Learning Pytorch VAE Implementation Interpolating Music β-VAE: Learning Basic Visual Concepts With A Constrained Variational Framework The Reparameterization Trick Generating Diverse High-Fidelity Images with VQ-VAE-2 Generating Diverse Structure for Image Inpainting With Hierarchical VQ-VAE Variational Image Inpainting  A Technical Sidenote Compared to the previous day, the trainLoop is slightly modified: First, we rescale the images between 0 and 1. Second, we include our new loss function in the step function.\nstep :: Optimizer o =\u0026gt; (VAE, o) -\u0026gt; ((Tensor, Tensor), Int) -\u0026gt; IO (VAE, o) step (model, opt) args = do let ((x, _), iter) = toLocalModel\u0026#39; args -- Rescale pixel values [0, 255] -\u0026gt; [0, 1.0]. -- This is important as the sigmoid activation in decoder can -- reach values only between 0 and 1. x\u0026#39; = x / 255.0 (recon_x, mu, logSigma) \u0026lt;- vaeForward model False x\u0026#39; let loss = vaeLoss beta recon_x x\u0026#39; mu logSigma -- Print loss every 100 batches when (iter `mod` 100 == 0) $ do putStrLn $ printf \u0026#34;Batch: %d | Loss: %.2f\u0026#34; iter (asValue loss :: Float) runStep model opt loss lr The train function now uses Adam optimizer from Torch.Optim.CppOptim, which tends to be faster compared to mkAdam we used previously. This is not very different from mkAdam-based training, except that the learning rate is specified as Cpp.adamLr parameter and not as a trainLoop parameter (ignored when passed to runStep).\ntrain :: Float -\u0026gt; V.MNIST IO -\u0026gt; Int -\u0026gt; VAE -\u0026gt; IO VAE train beta trainMnist epochs net0 = do optimizer \u0026lt;- Cpp.initOptimizer adamOpt net0 (net\u0026#39;, _) \u0026lt;- foldLoop (net0, optimizer) epochs $ \\(net\u0026#39;, optState) _ -\u0026gt; runContT (streamFromMap dsetOpt trainMnist) $ trainLoop beta (net\u0026#39;, optState) 0.0 . fst return net\u0026#39; where dsetOpt = datasetOpts workers workers = 2 -- Adam optimizer parameters adamOpt = def { Cpp.adamLr = learningRate, Cpp.adamBetas = (0.9, 0.999), Cpp.adamEps = 1e-8, Cpp.adamWeightDecay = 0, Cpp.adamAmsgrad = False } :: Cpp.AdamOptions learningRate :: Double learningRate = 1e-3 I used a compiled version instead of a notebook since the network training worked much faster (the bottleneck was in training data mini-batches loading). Also I have trained networks with Torch.Optim.CppOptim. It is slightly faster compared to mkAdam.\nI was also wondering why I get large values out of the encoder. It turns out that this is because relu function is unbounded. You may want to replace relu with Torch.tanh and visualize the latent space again.\n We use sigma as an argument to randnLikeIO so that the resulting random tensor has the same shape as sigma. ^   ","date":1660204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1660863420,"objectID":"d9a9cb3fc33d88fe2a102a0a0a1e8867","permalink":"https://penkovsky.com/neural-networks/day9/","publishdate":"2022-08-11T10:00:00+02:00","relpermalink":"/neural-networks/day9/","section":"neural-networks","summary":"Imagine you are a designer and you want a new font: A little bit heavier, with rounder letters, more casual or a little bit more fancy. Could this font be created just by tuning a handful of parameters? Or imagine that you are a fashion designer and you would like to create a new collection as a mix of previous seasons? Or that you are a musician desperately looking for inspiration.","tags":["Deep Learning","Haskell"],"title":"Day 9: Roaming The Latent Space","type":"neural-networks"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Wouldn't it be nice if the model also told us which predictions are not reliable? Can this be done even on unseen data? The good news is yes, and even on new, completely unseen data. It is also simple to implement in practice. A canonical example is in a medical setting. By measuring model uncertainty, the doctor can learn how reliable is their AI-assisted patient's diagnosis. This allows the doctor to make a better informed decision whether to trust the model or not. And potentially save someone's life.\nToday we build upon Day 7 and we continue our journey with Hasktorch:\n We will introduce a Dropout layer. We will compute on a graphics processing unit (GPU). We will also show how to load and save models. We will train with Adam optimizer. And finally we will talk about model uncertainty estimation.  The complete project is also available on Github.\nDropout Layer Neural networks, as any other model with many parameters, tend to overfit. By overfitting I mean \u0026quot;fail to fit to additional data or predict future observations reliably\u0026quot;. Let us consider a classical example below.\n   Overfitting. Credit Ignacio Icke, CC BY-SA 4.0   The green line is a decision boundary created by an overfitted model. We see that the model tries to memorize every possible data point. However, it fails to generalize. To ameliorate the situation, we perform a so-called regularization. That is a technique that helps to prevent overfitting. In the image above, the black line is a decision boundary of a regularized model.\nOne of regularization techniques for artificial neural networks is called dropout or dilution. Its principle of operation is quite simple. During neural network training, we randomly disconnect a fraction of neurons with some probability. It turns out that dropout conditioning results in more reliable neural network models.\nA Neural Network with Dropout The data structures MLP (learnable parameters) and MLPSpec (number of neurons) remain unchanged. However, we will need to modify the mlp function (full network) to include a Dropout layer. If we inspect dropout :: Double -\u0026gt; Bool -\u0026gt; Tensor -\u0026gt; IO Tensor type, we see that it accepts three arguments: a Double probability of dropout, a Bool that turns this layer on or off, and a data Tensor. Typically, we turn the dropout on during the training and off during the inference stage.\nHowever, the biggest distinction between e.g. relu function and dropout is that relu :: Tensor -\u0026gt; Tensor is a pure function, i.e. it does not have any 'side-effects'. This means that every time when we call a pure function, the result will be the same. This is not the case with dropout that relies on an (external) random number generator, and therefore returns a new result each time. Therefore, its outcome is an IO Tensor.\nOne has to pay a particular attention to those IO functions, because they can change the state in the external world. This can be printing text on the screen, deleting a file, or launching missiles. Typically, we prefer to keep functions pure whenever possible, as function purity improves the reasoning about the program: It is a child's play to refactor (reorganize) a program consisting only of pure functions.\nI find the so-called do-notation to be the most natural way to combine both pure functions and those with side-effects. The pure equations can be grouped under let keyword(s), while the side-effects are summoned with a special \u0026lt;- glue. This is how we integrate dropout in mlp. Note that now the outcome of mlp also becomes an IO Tensor.\nmlp :: MLP -\u0026gt; Bool -\u0026gt; Tensor -\u0026gt; IO Tensor mlp MLP {..} isStochastic x0 = do -- This subnetwork encapsulates the composition -- of pure functions let sub1 = linear fc1 ~\u0026gt; relu ~\u0026gt; linear fc2 ~\u0026gt; relu -- The dropout is applied to the output -- of the subnetwork x1 \u0026lt;- dropout 0.1 -- Dropout probability isStochastic -- Activate Dropout when in stochastic mode (sub1 x0) -- Apply dropout to -- the output of `relu` in layer 2 -- Another linear layer let x2 = linear fc3 x1 -- Finally, logSoftmax, which is numerically more stable -- compared to simple log(softmax(x2)) return $ logSoftmax (Dim 1) x2 For model uncertainty estimation, it is empirically recommended to keep the dropout probability anywhere between 0.1 and 0.2.\nComputing on a GPU To transfer data onto a GPU, we use toDevice :: ... =\u0026gt; Device -\u0026gt; a -\u0026gt; a. Below are helper methods to traverse data structures containing tensors (e.g. MLP) to convert those between devices.\ntoLocalModel :: forall a. HasTypes a Tensor =\u0026gt; Device -\u0026gt; DType -\u0026gt; a -\u0026gt; a toLocalModel device\u0026#39; dtype\u0026#39; = over (types @Tensor @a) (toDevice device\u0026#39;) fromLocalModel :: forall a. HasTypes a Tensor =\u0026gt; a -\u0026gt; a fromLocalModel = over (types @Tensor @a) (toDevice (Device CPU 0)) Below is a shortcut to transfer data to cuda:0 device, assuming the Float type.\ntoLocalModel\u0026#39; = toLocalModel (Device CUDA 0) Float  The train loop is almost the same as in the previous post, except a few changes. First, we convert training data to GPU with toLocalModel' (assuming that the model itself was already converted to GPU). Second, predic \u0026lt;- mlp model isTrain input is an IO action. Third, we manage optimizer's internal state1.\ntrainLoop :: Optimizer o =\u0026gt; (MLP, o) -\u0026gt; LearningRate -\u0026gt; ListT IO (Tensor, Tensor) -\u0026gt; IO (MLP, o) trainLoop (model0, opt0) lr = P.foldM step begin done. enumerateData where isTrain = True step :: Optimizer o =\u0026gt; (MLP, o) -\u0026gt; ((Tensor, Tensor), Int) -\u0026gt; IO (MLP, o) step (model, opt) args = do let ((input, label), iter) = toLocalModel\u0026#39; args predic \u0026lt;- mlp model isTrain input let loss = nllLoss\u0026#39; label predic -- Print loss every 100 batches when (iter `mod` 100 == 0) $ do putStrLn $ printf \u0026#34;Batch: %d | Loss: %.2f\u0026#34; iter (asValue loss :: Float) runStep model opt loss lr done = pure begin = pure (model0, opt0) We also modify the train function to use Adam optimizer with mkAdam:\n 0 is the initial iteration number (then internally increased by the optimizer). We provide beta1 and beta2 values. flattenParameters net0 are needed to get the shapes of the trained parameters momenta. See also Day 2 for more details.  train :: V.MNIST IO -\u0026gt; Int -\u0026gt; MLP -\u0026gt; IO MLP train trainMnist epochs net0 = do (net\u0026#39;, _) \u0026lt;- foldLoop (net0, optimizer) epochs $ \\(net\u0026#39;, optState) _ -\u0026gt; runContT (streamFromMap dsetOpt trainMnist) $ trainLoop (net\u0026#39;, optState) lr. fst return net\u0026#39; where dsetOpt = datasetOpts workers workers = 2 lr = 1e-4 -- Learning rate optimizer = mkAdam 0 beta1 beta2 (flattenParameters net0) beta1 = 0.9 beta2 = 0.999 Here is a function to get model accuracy:\naccuracy :: MLP -\u0026gt; ListT IO (Tensor, Tensor) -\u0026gt; IO Float accuracy net = P.foldM step begin done. enumerateData where step :: (Int, Int) -\u0026gt; ((Tensor, Tensor), Int) -\u0026gt; IO (Int, Int) step (ac, total) args = do let ((input, labels), _) = toLocalModel\u0026#39; args -- Compute predictions predic \u0026lt;- let stochastic = False in argmax (Dim 1) RemoveDim \u0026lt;$\u0026gt; mlp net stochastic input let correct = asValue -- Sum those elements $ sumDim (Dim 0) RemoveDim Int64 -- Find correct predictions $ predic `eq` labels let batchSize = head $ shape predic return (ac + correct, total + batchSize) -- When done folding, compute the accuracy done (ac, total) = pure $ fromIntegral ac / fromIntegral total -- Initial errors and totals begin = pure (0, 0) testAccuracy :: V.MNIST IO -\u0026gt; MLP -\u0026gt; IO Float testAccuracy testStream net = do runContT (streamFromMap (datasetOpts 2) testStream) $ accuracy net. fst Below we provide the MLP specification: number of neurons in each layer.\nspec = MLPSpec 784 300 50 10 Saving and Loading the Model Before we can save the model, we have to make the weight tensors dependent first:\nsave\u0026#39; :: MLP -\u0026gt; FilePath -\u0026gt; IO () save\u0026#39; net = save (map toDependent. flattenParameters $ net) The inverse is true for model loading. We also replace parameters in a newly generated model with the one we have just loaded:\nload\u0026#39; :: FilePath -\u0026gt; IO MLP load\u0026#39; fpath = do params \u0026lt;- mapM makeIndependent \u0026lt;=\u0026lt; load $ fpath net0 \u0026lt;- sample spec return $ replaceParameters net0 params Load the MNIST data:\n(trainData, testData) \u0026lt;- initMnist \u0026#34;data\u0026#34; Train a new model:\n-- A train \u0026#34;loader\u0026#34; trainMnistStream = V.MNIST { batchSize = 256, mnistData = trainData } net0 \u0026lt;- toLocalModel\u0026#39; \u0026lt;$\u0026gt; sample spec epochs = 5 net\u0026#39; \u0026lt;- train trainMnistStream epochs net0 Saving the model:\nsave\u0026#39; net\u0026#39; \u0026#34;weights.bin\u0026#34; To load a pretrained model:\nnet \u0026lt;- load\u0026#39; \u0026#34;weights.bin\u0026#34; We can verify the model's accuracy:\n-- A test \u0026#34;loader\u0026#34; testMnistStream = V.MNIST { batchSize = 1000, mnistData = testData } ac \u0026lt;- testAccuracy testMnistStream net putStrLn $ \u0026#34;Accuracy \u0026#34; ++ show ac Accuracy 0.9245 The accuracy is not tremendous, but it can be improved by introducing batch norm, convolutional layers, and training longer. We are about to discuss model uncertainty estimation and this accuracy is good enough.\nPredictive Entropy Model uncertainties are obtained as:\n$$ \\begin{equation} \\mathbb{H}(y|\\mathbf{x}) = -\\sum_c p(y = c|\\mathbf{x}) \\log p(y = c|\\mathbf{x}), \\end{equation} $$\nwhere $y$ is label, $\\mathbf{x}$ – input image, $c$ – class, $p$ – probability.\nWe call $\\mathbb{H}$ predictive entropy. And it is the very dropout technique that helps us to estimate those uncertainties. All we need to do is to collect several predictions in the stochastic mode (i.e. dropout enabled) and apply the formula from above.\npredictiveEntropy :: Tensor -\u0026gt; Float predictiveEntropy predictions = let epsilon = 1e-45 a = meanDim (Dim 0) RemoveDim Float predictions b = Torch.log $ a + epsilon in asValue $ negate $ sumAll $ a * b Visualizing Softmax Predictions To get a better feeling what model outputs look like, it would be nice to visualize the softmax output as a histogram or a bar chart. For instance\nbar [\u0026#34;apples\u0026#34;, \u0026#34;oranges\u0026#34;, \u0026#34;kiwis\u0026#34;] [50, 100, 25] apples ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 50.00 oranges ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 100.00 kiwis ▉▉▉▉▉▉▉▉▉▉▉▉▋ 25.00 Now, we would like to display an image, the predictive entropy, and the softmax output, followed by prediction and ground truth. To transform logSoftmax into softmax, we use the following identity:\n$$ \\begin{equation} e^{\\ln(\\rm{softmax}(x))} = \\rm{softmax}(x), \\end{equation} $$\nthat is softmax = exp. logSoftmax.\ndisplayImage :: MLP -\u0026gt; (Tensor, Tensor) -\u0026gt; IO () displayImage model (testImg, testLabel) = do let repeatN = 20 stochastic = True preds \u0026lt;- forM [1..repeatN] $ \\_ -\u0026gt; exp -- logSoftmax -\u0026gt; softmax \u0026lt;$\u0026gt; mlp model stochastic testImg pred0 \u0026lt;- mlp model (not stochastic) testImg let entropy = predictiveEntropy $ Torch.cat (Dim 0) preds -- Select only the images with high entropy when (entropy \u0026gt; 0.9) $ do V.dispImage testImg putStr \u0026#34;Entropy \u0026#34; print entropy -- exp. logSoftmax = softmax bar (map show [0..9]) (asValue $ flattenAll $ exp pred0 :: [Float]) putStrLn $ \u0026#34;Model : \u0026#34; ++ (show. argmax (Dim 1) RemoveDim. exp $ pred0) putStrLn $ \u0026#34;Ground Truth : \u0026#34; ++ show testLabel Note that below we show only some of those images the model is uncertain about (entropy \u0026gt; 0.9)\ntestMnistStream = V.MNIST {batchSize = 1, mnistData = testData} forM_ [0 .. 200] $ displayImage (fromLocalModel net) \u0026lt;=\u0026lt; getItem testMnistStream +% % * #- +%%= % %% % % %+ # % % * % % :% #*:=%# -%=. Entropy 1.044228 0 ▉▏ 0.01 1 ▏ 0.00 2 ▋ 0.01 3 ▏ 0.00 4 ▉ 0.01 5 ▍ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.70 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.21 9 ▉▉▉▋ 0.05 Model : Tensor Int64 [1] [ 6] Ground Truth : Tensor Int64 [1] [ 6] .#%#. %%+: % %.. ##-#%. -% :% + - .% @%+*%%+ Entropy 1.2909155 0 ▏ 0.00 1 ▏ 0.00 2 ▍ 0.00 3 ▉▉▉▉▉▉▉▉ 0.07 4 ▏ 0.00 5 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.44 6 ▏ 0.00 7 ▍ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.47 9 ▉▏ 0.01 Model : Tensor Int64 [1] [ 8] Ground Truth : Tensor Int64 [1] [ 5] =- = #- =# %- # +% % %. .% ## .* %%%%%#%#. . % Entropy 1.3325933 0 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.19 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.46 5 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▊ 0.18 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▊ 0.16 7 ▏ 0.00 8 ▏ 0.00 9 ▏ 0.00 Model : Tensor Int64 [1] [ 4] Ground Truth : Tensor Int64 [1] [ 4] *: :%%* #- -+ - # +: # =. #. =%: *.*%- #%%: Entropy 1.2533671 0 ▉ 0.01 1 ▉▉▍ 0.03 2 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.38 3 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.54 4 ▏ 0.00 5 ▋ 0.01 6 ▏ 0.00 7 ▏ 0.00 8 ▉▉▋ 0.03 9 ▏ 0.00 Model : Tensor Int64 [1] [ 3] Ground Truth : Tensor Int64 [1] [ 2] +##- * : = % = % % -= @ = % % % % Entropy 0.9308149 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▉ 0.01 4 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.29 5 ▍ 0.00 6 ▏ 0.00 7 ▎ 0.00 8 ▉▎ 0.02 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.67 Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 9] # % # % * % = %%@% * % % % % = Entropy 1.39582 0 ▏ 0.00 1 ▉▍ 0.01 2 ▏ 0.00 3 ▉▉▉▉▉▊ 0.06 4 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.48 5 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▋ 0.17 6 ▉▉▉▉ 0.04 7 ▏ 0.00 8 ▉▋ 0.02 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.22 Model : Tensor Int64 [1] [ 4] Ground Truth : Tensor Int64 [1] [ 4] .#%@ %%%%= +%. %# %%%%: %%% -%% -%% .%% %%- %* Entropy 1.0009595 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▉▊ 0.02 4 ▏ 0.00 5 ▎ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.35 8 ▉ 0.01 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.62 Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 9] %##% :%+%%. -% %: -% %+ + %+ %+ %+ %# %% .+ Entropy 1.0057298 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▉▉▍ 0.03 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.33 8 ▏ 0.00 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.63 Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 7] %%%%% .% %. =%%%+ % %# - %%. *%- %:% %-%= %%- Entropy 1.0500848 0 ▉▉▉▉▍ 0.07 1 ▎ 0.00 2 ▎ 0.00 3 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.79 4 ▉▉▊ 0.04 5 ▉▉▉▎ 0.05 6 ▏ 0.00 7 ▍ 0.01 8 ▎ 0.00 9 ▉▊ 0.03 Model : Tensor Int64 [1] [ 3] Ground Truth : Tensor Int64 [1] [ 3] :* % %% :% %* +* % % % = Entropy 1.590256 0 ▏ 0.00 1 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.36 2 ▏ 0.00 3 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.10 4 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.32 5 ▉▉▉▎ 0.02 6 ▏ 0.00 7 ▎ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.12 9 ▉▉▉▉▉▉▉▉▉▉▍ 0.07 Model : Tensor Int64 [1] [ 1] Ground Truth : Tensor Int64 [1] [ 1] = = %%%%%. :%% %* .%%%%%%%%+ %%%*: %% %% %% %% Entropy 0.9592192 0 ▏ 0.00 1 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▊ 0.28 2 ▋ 0.01 3 ▍ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▍ 0.01 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.67 8 ▏ 0.00 9 ▉▉▏ 0.03 Model : Tensor Int64 [1] [ 7] Ground Truth : Tensor Int64 [1] [ 7] =%#* :%%- .# %% :% .% #= % %%# -%%%% %%%.% #% *+ : Entropy 1.0005924 0 ▍ 0.00 1 ▏ 0.00 2 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.48 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.47 8 ▉▉▉▋ 0.03 9 ▉▎ 0.01 Model : Tensor Int64 [1] [ 2] Ground Truth : Tensor Int64 [1] [ 2] - :%%%- :% % +: :%- -% *% *: %* == *% * :% #::..:*%% :%*%%-: Entropy 1.3647958 0 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.50 1 ▏ 0.00 2 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.23 3 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.23 4 ▏ 0.00 5 ▉▉▉▏ 0.03 6 ▏ 0.00 7 ▏ 0.00 8 ▏ 0.00 9 ▉▍ 0.01 Model : Tensor Int64 [1] [ 0] Ground Truth : Tensor Int64 [1] [ 0] %- :% # -%#%* :: @%. * % #. %% % % % Entropy 1.1518966 0 ▉▉▉▎ 0.06 1 ▍ 0.01 2 ▊ 0.01 3 ▏ 0.00 4 ▉▉▊ 0.05 5 ▏ 0.00 6 ▏ 0.00 7 ▏ 0.00 8 ▍ 0.01 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.86 Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 2] =%%%%+ .#. =#% %* %# #. .% .# *%: .%%%- = # # -%% =% =%%# Entropy 1.1256037 0 ▉▊ 0.02 1 ▏ 0.00 2 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▏ 0.29 3 ▎ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.59 9 ▉▉▉▉▉▉▉▉▎ 0.10 Model : Tensor Int64 [1] [ 8] Ground Truth : Tensor Int64 [1] [ 9] --%: . % %: ** .% *%%. %%*% %* % % % % %: %%%: Entropy 1.0862491 0 ▏ 0.00 1 ▉▉▋ 0.03 2 ▉▉▉▉▉ 0.05 3 ▏ 0.00 4 ▏ 0.00 5 ▋ 0.01 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.42 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.50 9 ▏ 0.00 Model : Tensor Int64 [1] [ 8] Ground Truth : Tensor Int64 [1] [ 8] %% %% *%# :%%- .%% %%+ +%% *%+ =%= =: Entropy 1.0085171 0 ▏ 0.00 1 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.81 2 ▎ 0.00 3 ▍ 0.01 4 ▎ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▏ 0.16 8 ▎ 0.01 9 ▍ 0.01 Model : Tensor Int64 [1] [ 1] Ground Truth : Tensor Int64 [1] [ 1] -@@: -# +: #- % %: ..- +%=*% .%% %* %% %% %. Entropy 1.5438546 0 ▏ 0.00 1 ▏ 0.00 2 ▉▉▉▉ 0.03 3 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.14 4 ▉▉▉▉▉▊ 0.05 5 ▊ 0.01 6 ▏ 0.00 7 ▉▉▉▉▊ 0.04 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▎ 0.31 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.42 Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 9] Reflecting on softmax outputs above we can state that\n Softmax output alone is not enough to estimate the model uncertainty. We can observe wrong predictions even when the margin between the top and second-best guess is large. Sometimes prediction and ground truth coincide. So why the entropy is high? We actually need to inspect such cases in more details.  The first point is well illustrated by this example:\n%- :% # -%#%* :: @%. * % #. %% % % % Entropy 1.1518966 0 ▉▉▉▎ 0.06 1 ▍ 0.01 2 ▊ 0.01 3 ▏ 0.00 4 ▉▉▊ 0.05 5 ▏ 0.00 6 ▏ 0.00 7 ▏ 0.00 8 ▍ 0.01 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.86 Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 2] To illustrate the last point, let us take a closer look at cases with high entropy. By running several realizations of the stochatic model, we can verify if the model has any \u0026quot;doubt\u0026quot; by selecting different answers.\ndisplayImage\u0026#39; :: MLP -\u0026gt; (Tensor, Tensor) -\u0026gt; IO () displayImage\u0026#39; model (testImg, testLabel) = do let repeatN = 10 pred\u0026#39; \u0026lt;- forM [1..repeatN] $ \\_ -\u0026gt; exp -- logSoftmax -\u0026gt; softMax \u0026lt;$\u0026gt; mlp model True testImg pred0 \u0026lt;- mlp model False testImg let entropy = predictiveEntropy $ Torch.cat (Dim 0) pred\u0026#39; V.dispImage testImg putStr \u0026#34;Entropy \u0026#34; print entropy forM_ pred\u0026#39; ( \\pred -\u0026gt; putStrLn \u0026#34;\u0026#34; \u0026gt;\u0026gt; bar (map show [0..9]) (asValue $ flattenAll pred :: [Float]) ) putStrLn $ \u0026#34;Model : \u0026#34; ++ (show. argmax (Dim 1) RemoveDim. exp $ pred0) putStrLn $ \u0026#34;Ground Truth : \u0026#34; ++ show testLabel The first example from above (dataset index 11) gives this:\n(displayImage\u0026#39; (fromLocalModel net) \u0026lt;=\u0026lt; getItem testMnistStream) 11 +% % * #- +%%= % %% % % %+ # % % * % % :% #*:=%# -%=. Entropy 1.1085687 0 ▎ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.90 7 ▏ 0.00 8 ▉▉▉▉▉▍ 0.10 9 ▏ 0.00 0 ▋ 0.01 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▋ 0.01 5 ▎ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.74 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.20 9 ▉▉▋ 0.04 0 ▋ 0.01 1 ▏ 0.00 2 ▏ 0.00 3 ▎ 0.01 4 ▉▉▉▏ 0.05 5 ▏ 0.00 6 ▉▉▎ 0.04 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.86 9 ▉▎ 0.02 0 ▋ 0.01 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▋ 0.01 5 ▎ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.74 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.20 9 ▉▉▋ 0.04 0 ▉▉▉▉▍ 0.04 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▉▉▉▉▉▉▉▉▉▉▏ 0.09 5 ▉▏ 0.01 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▋ 0.30 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.12 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.43 0 ▋ 0.01 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▋ 0.01 5 ▎ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.74 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.20 9 ▉▉▋ 0.04 0 ▋ 0.01 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▋ 0.01 5 ▎ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.74 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.20 9 ▉▉▋ 0.04 0 ▋ 0.01 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▋ 0.01 5 ▎ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.74 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.20 9 ▉▉▋ 0.04 0 ▉▏ 0.02 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▋ 0.01 5 ▏ 0.00 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.80 7 ▏ 0.00 8 ▉▉▉▉▉▉▋ 0.10 9 ▉▉▉▉▎ 0.07 0 ▉▉▉▉▍ 0.04 1 ▏ 0.00 2 ▎ 0.00 3 ▏ 0.00 4 ▉▉▉▉▉▉▉▉▉▉▏ 0.09 5 ▉▏ 0.01 6 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▋ 0.30 7 ▏ 0.00 8 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▍ 0.12 9 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 0.43 Model : Tensor Int64 [1] [ 6] Ground Truth : Tensor Int64 [1] [ 6] Wow! The model sometimes \u0026quot;sees\u0026quot; digit 6, sometimes digit 8, and sometimes digit 9! For the contrast, here is how predictions with low entropy typically look like.\n(displayImage\u0026#39; (fromLocalModel net) \u0026lt;=\u0026lt; getItem testMnistStream) 0 #%%***** ::: % %: :% #: :% %. #= :%. =# Entropy 4.8037423e-4 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 0 ▏ 0.00 1 ▏ 0.00 2 ▏ 0.00 3 ▏ 0.00 4 ▏ 0.00 5 ▏ 0.00 6 ▏ 0.00 7 ▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉▉ 1.00 8 ▏ 0.00 9 ▏ 0.00 Model : Tensor Int64 [1] [ 7] Ground Truth : Tensor Int64 [1] [ 7] The model always \u0026quot;sees\u0026quot; digit 7. That is why the predictive entropy is low. Note that the results are model-dependent. Therefore we also share the weights for reproducibility. However, every realization of the stochastic model might still be different, especially in those cases where the entropy is high.\nFind the complete project on Github. For suggestions about the content feel free to open a new issue.\nSummary I hope you are now convinced that model's uncertainty estimation is an invaluable tool. This simple technique is essential when applying deep learning for real-life decision making. This post also develops on how to use Hasktorch library in practice. Notably, it is very straightforward to run computations on a GPU. Overall, Hasktorch can be used for real-world deep learning. The code is well-structured and relies on a mature Torch library. On the other hand, it would be desirable to capture high-level patterns so that the user does not need to think about low-level concepts such as dependent and independent tensors, for example. The end user should be able to simply apply save net \u0026quot;weights.bin\u0026quot; and mynet \u0026lt;- load \u0026quot;weights.bin\u0026quot; without any indirections. The same reasoning applies to the trainLoop, i.e. the user does not need to reinvent it every time. Eventually, a higher-level package on top of Hasktorch should capture the best practices, similar to PyTorch Lightning or fast.ai.\nNow your turn: explore image recognition with AlexNet convolutional network and have fun!\nEdit 27/04/2022: The original version from 23/04 did not correctly handle optimizer's internal state. Therefore, train and trainLoop were fixed. You will find the updated notebook on Github.\nLearn More  Improving neural networks by preventing co-adaptation of feature detectors Dropout: A Simple Way to Prevent Neural Networks from Overfitting Tutorial: Dropout as Regularization and Bayesian Approximation Two Simple Ways To Measure Your Model’s Uncertainty Uncertainty in Deep Learning, Yarin Gal AlexNet example in Hasktorch   Previously, there was no need to handle GD optimizer's internal state. This is not true in a more general case. For instance, Adam keeps track of momenta and iterations for bias adjustment. ^   ","date":1650727200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650727200,"objectID":"1d5286dc84a2729ebf846dc4c12b55a0","permalink":"https://penkovsky.com/neural-networks/day8/","publishdate":"2022-04-23T17:20:00+02:00","relpermalink":"/neural-networks/day8/","section":"neural-networks","summary":"Wouldn't it be nice if the model also told us which predictions are not reliable? Can this be done even on unseen data? The good news is yes, and even on new, completely unseen data. It is also simple to implement in practice. A canonical example is in a medical setting. By measuring model uncertainty, the doctor can learn how reliable is their AI-assisted patient's diagnosis. This allows the doctor to make a better informed decision whether to trust the model or not.","tags":["Deep Learning","Haskell"],"title":"Day 8: Model Uncertainty Estimation","type":"neural-networks"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" So far we have explored neural networks almost in the vacuum. Although we have provided some illustrations for better clarity, relying an existing framework would allow us to benefit from the knowledge of previous contributors. One such framework is called Hasktorch. Among the practical reasons to use Hasktorch is relying on a mature Torch Tensor library. Another good reason is strong GPU acceleration, which is necessary for almost any serious deep learning project. Finally, standard interfaces rather than reinventing the wheel will help to reduce the boilerplate.\n Fun fact: one of Hasktorch contributors is Adam Paszke, the original author of Pytorch.\n Today's post is also based on\n Day 2: What Do Hidden Layers Do? Day 4: The Importance Of Batch Normalization Day 5: Convolutional Neural Networks Tutorial  The source code from this post is available on Github.\nThe Basics The easiest way to start with Hasktorch is via Docker:\ndocker run --gpus all -it --rm -p 8888:8888 \\  -v $(pwd):/home/ubuntu/data \\  htorch/hasktorch-jupyter:latest-cu11 Now, you may open localhost:8888 in your browser to access Jupyterlab notebooks. Note that you need to select Haskell kernel when creating a new notebook.\nIf you have never used Torch library before, you may also want to review this tutorial.\nMNIST Example Let's take the familiar MNIST example and see how it can be implemented in Hasktorch.\nImports {-# LANGUAGE DeriveAnyClass #-} {-# LANGUAGE DeriveGeneric #-} {-# LANGUAGE MultiParamTypeClasses #-} {-# LANGUAGE RecordWildCards #-} {-# LANGUAGE ScopedTypeVariables #-} import Control.Exception.Safe ( SomeException (..), try, ) import Control.Monad ( forM_, when, (\u0026lt;=\u0026lt;) ) import Control.Monad.Cont ( ContT (..) ) import GHC.Generics import Pipes hiding ( (~\u0026gt;) ) import qualified Pipes.Prelude as P import Torch import Torch.Serialize import Torch.Typed.Vision ( initMnist ) import qualified Torch.Vision as V import Prelude hiding ( exp ) The most notable import is the Torch module itself. There are also related helpers such Torch.Vision to handle image data. The function initMnist has type\ninitMnist :: String -\u0026gt; IO (MnistData, MnistData) The function is loading MNIST train and test datasets, similar to loadMNIST from previous posts.\nIt might be also useful to pay attention to Pipes module. It is an alternative to previously used Streamly, which also allows building streaming components.\nWe also import functions from Control.Monad, which are useful for IO operations.\nFinally, we hide exp function in favor of Torch exp, which operates on tensors (arrays)1 rather than floating point scalars:\nTorch.exp :: Tensor -\u0026gt; Tensor Defining Neural Network Architecture First we define a neural network data structure that contains trained parameters (neural network weights). In the simplest case, it can be a multilayer perceptron (MLP).\ndata MLP = MLP { fc1 :: Linear, fc2 :: Linear, fc3 :: Linear } deriving (Generic, Show, Parameterized) This MLP contains three linear layers. Next, we may define a data structure that specifies the number of neurons in each layer:\ndata MLPSpec = MLPSpec { i :: Int, h1 :: Int, h2 :: Int, o :: Int } deriving (Show, Eq) Now, we can define a neural network as a function, similar as we did on Day 5 with a \u0026quot;reversed\u0026quot; composition operator (~\u0026gt;).\n(~\u0026gt;) :: (a -\u0026gt; b) -\u0026gt; (b -\u0026gt; c) -\u0026gt; a -\u0026gt; c f ~\u0026gt; g = g. f mlp :: MLP -\u0026gt; Tensor -\u0026gt; Tensor mlp MLP {..} = -- Layer 1 linear fc1 ~\u0026gt; relu -- Layer 2 ~\u0026gt; linear fc2 ~\u0026gt; relu -- Layer 3 ~\u0026gt; linear fc3 ~\u0026gt; logSoftmax (Dim 1) We finish by a (log) softmax layer reducing the tensor's dimension 1 (Dim 1). Derivatives of linear, relu, and logSoftmax are already handled by Torch library.\nInitial Weights How do we generate initial random weights? As you may remember from Day 5, we could create a function such as this one:\nrandNetwork = do let [i, h1, h2, o] = [784, 64, 32, 10] fc1 \u0026lt;- randLinear (Sz2 i h1) fc2 \u0026lt;- randLinear (Sz2 h1 h2) fc3 \u0026lt;- randLinear (Sz2 h2 o) return $ MLP { fc1 = fc1 , fc2 = fc2 , fc3 = fc3 } In our example we do almost the same, except we benefit from applicative functors and Randomizable.\ninstance Randomizable MLPSpec MLP where sample MLPSpec {..} = MLP \u0026lt;$\u0026gt; sample (LinearSpec i h1) \u0026lt;*\u0026gt; sample (LinearSpec h1 h2) \u0026lt;*\u0026gt; sample (LinearSpec h2 o) We say above that MLP is an instance of the Randomizable typeclass, parametrized by MLPSpec. All we needed to define this instance was to implement a sample function. To generate initial MLP weights, later we can simply write\nlet spec = MLPSpec 784 64 32 10 net \u0026lt;- sample spec Train Loop The core of the neural network training is trainLoop, which enables a single training \u0026quot;epoch\u0026quot;. Let us first inspect its type signature.\ntrainLoop :: Optimizer o =\u0026gt; MLP -\u0026gt; o -\u0026gt; ListT IO (Tensor, Tensor) -\u0026gt; IO MLP This signifies that the function accepts an initial neural network configuration, an optimizer, and a dataset. The optimizer can be a gradient descent (GD), Adam, or other optimizer. The result of the function is a new MLP configuration, as a result of IO call. IO is necessary for instance if we want to print the loss after each iteration. Now, let's take a look at the implementation:\ntrainLoop model optimizer = P.foldM step begin done. enumerateData First, we enumerate the dataset with enumerateData. Then, we iterate over (fold) the batches. The step function is an analogy to a step in the gradient descent algorithm:\nwhere step :: MLP -\u0026gt; ((Tensor, Tensor), Int) -\u0026gt; IO MLP step model ((input, label), iter) = do let loss = nllLoss\u0026#39; label $ mlp model input -- Print loss every 50 batches when (iter `mod` 50 == 0) $ do putStrLn $ \u0026#34;Iteration: \u0026#34; ++ show iter ++ \u0026#34; | Loss: \u0026#34; ++ show loss (newParam, _) \u0026lt;- runStep model optimizer loss 1e-3 return newParam We calculate a negative log likelihood loss nllLoss' between the ground truth label and the output of our MLP. Note that model is the parameter, i.e. weights of the MLP network. Then, we take advantage of the iteration number iter to print the loss every 50 iterations. Finally, we perform a gradient descent step using our optimizer via runStep :: ... =\u0026gt; model -\u0026gt; optimizer -\u0026gt; Loss -\u0026gt; LearningRate -\u0026gt; IO (model, optimizer) and keep only new model newParam. The learning rate here is 1e-3, but can be eventually changed.\nThe done function is (trivial in this case) finalization of foldM iterations over the MLP model and begin are the initial weights (we use pure to satisfy the type m x requirement).\ndone = pure begin = pure model Putting It All Together The remaining part is simple. We load the data into batches, specify the number of neurons in our MLP, choose an optimizer, and initialize the random weights.\nmain = do (trainData, testData) \u0026lt;- initMnist \u0026#34;data\u0026#34; let trainMnist = V.MNIST {batchSize = 256, mnistData = trainData} testMnist = V.MNIST {batchSize = 1, mnistData = testData} spec = MLPSpec 784 64 32 10 optimizer = GD net \u0026lt;- sample spec Then, we train the network for 5 epochs:\nnet\u0026#39; \u0026lt;- foldLoop net 5 $ \\model _ -\u0026gt; runContT (streamFromMap (datasetOpts 2) trainMnist) $ trainLoop model optimizer. fst Finally, we may examine the model on test images\nforM_ [0 .. 10] $ displayImages net\u0026#39; \u0026lt;=\u0026lt; getItem testMnist For this purpose may use a function such as\ndisplayImages :: MLP -\u0026gt; (Tensor, Tensor) -\u0026gt; IO () displayImages model (testImg, testLabel) = do V.dispImage testImg putStrLn $ \u0026#34;Model : \u0026#34; ++ (show. argmax (Dim 1) RemoveDim. exp $ mlp model testImg) putStrLn $ \u0026#34;Ground Truth : \u0026#34; ++ show testLabel Running Iteration: 0 | Loss: Tensor Float [] 12.3775 Iteration: 50 | Loss: Tensor Float [] 1.0952 Iteration: 100 | Loss: Tensor Float [] 0.5626 Iteration: 150 | Loss: Tensor Float [] 0.6660 Iteration: 200 | Loss: Tensor Float [] 0.4771 Iteration: 0 | Loss: Tensor Float [] 0.5012 Iteration: 50 | Loss: Tensor Float [] 0.4058 Iteration: 100 | Loss: Tensor Float [] 0.3095 Iteration: 150 | Loss: Tensor Float [] 0.4237 Iteration: 200 | Loss: Tensor Float [] 0.3433 Iteration: 0 | Loss: Tensor Float [] 0.3671 Iteration: 50 | Loss: Tensor Float [] 0.3206 Iteration: 100 | Loss: Tensor Float [] 0.2467 Iteration: 150 | Loss: Tensor Float [] 0.3420 Iteration: 200 | Loss: Tensor Float [] 0.2737 Iteration: 0 | Loss: Tensor Float [] 0.3054 Iteration: 50 | Loss: Tensor Float [] 0.2779 Iteration: 100 | Loss: Tensor Float [] 0.2161 Iteration: 150 | Loss: Tensor Float [] 0.2933 Iteration: 200 | Loss: Tensor Float [] 0.2289 Iteration: 0 | Loss: Tensor Float [] 0.2693 Iteration: 50 | Loss: Tensor Float [] 0.2530 Iteration: 100 | Loss: Tensor Float [] 0.1979 Iteration: 150 | Loss: Tensor Float [] 0.2616 Iteration: 200 | Loss: Tensor Float [] 0.1986 #%%***** ::: % %: :% #: :% %. #= :%. =# Model : Tensor Int64 [1] [ 7] Ground Truth : Tensor Int64 [1] [ 7] %%%# %# % . #% :%: %+ *% %= %% %%%%++%%%= ==%%=. Model : Tensor Int64 [1] [ 2] Ground Truth : Tensor Int64 [1] [ 2] .- = % .# =: @ # ++ %: % Model : Tensor Int64 [1] [ 1] Ground Truth : Tensor Int64 [1] [ 1] %. *%- %%%%# :%%+:%- %% -%. % .@+ % %%. % #%* %%%%%% :%%%- Model : Tensor Int64 [1] [ 0] Ground Truth : Tensor Int64 [1] [ 0] = + % % +. % % %: + % %--=*% :: +% =% =% * Model : Tensor Int64 [1] [ 4] Ground Truth : Tensor Int64 [1] [ 4] %@ @: =@ @% @ :@ %# @ @ + Model : Tensor Int64 [1] [ 1] Ground Truth : Tensor Int64 [1] [ 1] % % % % +# -+ +%*::*% :%==%+ % ++ % %-+ * Model : Tensor Int64 [1] [ 4] Ground Truth : Tensor Int64 [1] [ 4] + %%+ .%*%% -: *% -#-%%. %% =# % .% #. % Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 9] ..=. .%%%%%% ::%+: % % %= %%%%%%+ :%%%% %%%% %# Model : Tensor Int64 [1] [ 6] Ground Truth : Tensor Int64 [1] [ 5] +%%%# +%* .%% :%. .#%+ %@%%%%* +%- -%# %% %% %= @ Model : Tensor Int64 [1] [ 9] Ground Truth : Tensor Int64 [1] [ 9] ==: %%**%% .% %: *- +# % :# # :# -# +# -# .% # +%: #%%%%= Model : Tensor Int64 [1] [ 0] Ground Truth : Tensor Int64 [1] [ 0] See the complete project on Github. For suggestions about the content feel free to open a new issue.\nSummary Today we have learned the basics of Hasktorch library. The most important is that the principles from our previous days still apply. Therefore, the transition to the new library was quite straightforward. With a few minor changes, this example could be run on a graphics processing unit accelerator.\nFurther Reading Hasktorch:\n Hasktorch tutorial Hasktorch examples Hasktorch documentation Applicative functors  Docker containers:\n Getting started with Docker   Tensors are represented by n-dimensional arrays. ^   ","date":1650315300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1650315300,"objectID":"aa80c44fdd8df7ee82a9440b3ab44e42","permalink":"https://penkovsky.com/neural-networks/day7/","publishdate":"2022-04-18T22:55:00+02:00","relpermalink":"/neural-networks/day7/","section":"neural-networks","summary":"So far we have explored neural networks almost in the vacuum. Although we have provided some illustrations for better clarity, relying an existing framework would allow us to benefit from the knowledge of previous contributors. One such framework is called Hasktorch. Among the practical reasons to use Hasktorch is relying on a mature Torch Tensor library. Another good reason is strong GPU acceleration, which is necessary for almost any serious deep learning project.","tags":["Deep Learning","Haskell"],"title":"Day 7: Real World Deep Learning","type":"neural-networks"},{"authors":["Bogdan Penkovsky"],"categories":["Homelab"],"content":" It has been a while since I was thinking about upgrading my drone to a first person view version aka FPV. The idea was placing a camera for video streaming in real time. My first attempt was with a small raspi board and streaming image data from a raspi camera via WiFi. Because of the latency, range, and complicated setup this solution was not very practical.\nTherefore, I decided to make a \u0026quot;real\u0026quot; FPV drone.\nThe jargon and some technical details were explained in the previous post. Before you continue, you may want to read it first:\n A Quadcopter From Scratch  FPV Components    Part Qty Unit W. (g) Price (EUR)     FPV micro camera 1 8.6* 50.67   Video transmitter (analog) 1 12* 47.92   MMCX antenna 1 3.1 10.23   FPV monitor or goggles 1 - 106.31    * Weight without cables. \nNote that you can easily find cheaper versions of those. For instance, a camera and VTX combo can cost under 40 EUR in total.\n   Basic FPV: Connecting camera directly to VTX   Technically these components are enough to build an autonomous FPV system. As seen above, I have connected the camera to the VTX (video transmitter), which was powered directly from a 3S battery (12.6 V). The FPV test flight was successful. Then, I found some space for further improvement. So I experimented with additional components. If you are about to build a quad from the ground up, you may want to consider those. These parts make it much easier to build a new copter.\nOptional Components    Part Qty Unit W. (g) Price (EUR)     F4-based flight controller + ESC stack 1 6 + 10* 65.70   Carbon fiber frame 1 107 28.07   Micro receiver 1 1.7 15.76   Pink propellers (5040) 4 ? 11.27    * Weight without connector wires. \nThe flight controller1 stack gave several advantages. First, a more powerful processor. Second, there was no need in additional UBEC, i.e. the flight controller was connected to the battery. Third, the controller provided an OSD (on-screen display) that could transmit flight information such as battery level or link quality (RSSI, LQ) in real time. I also find it overall convenient soldering VTX and camera wires directly to the flight controller2. And don't forget included silicon vibration dampeners! It is called a stack because of the flight controller and 4-in-1 ESC vertical integration. This implies a standard way to place and connect the two. Besides, the ESC already came with a XT60 connector to solder.\nA carbon fiber frame instead of a 3D-printed one is a bit more ergonomic. For instance, it already has standard emplacement for four M2 screws to attach the VTX and a support for the VTX camera. Overall it results in a cleaner, slim build. Commercial frames are strong enough to support a light X-shape, whereas the 3D-printed frame I used previously had a slightly bulky H-shape. Surprisingly, the weight of the new frame was about the same (107g vs 110g). Last but not least the commercial frame was already supplied with a set of screws and standoffs.\nThe FS-iA10B receiver was quite bulky, so I have replaced it with a smaller one. If you are choosing a transmitter-receiver system, then you may want to consider a long-range TBS CrossFire or ExpressLRS. The last one is cheaper because it is open-source, by the way. In both cases you avoid the problem of loosing the control because of the range. Believe me, I know how it feels bathing a copter in a lake!\n   F4 controller in a carbon fiber frame   There was no particular utility in pink propellers, but the older ones were worn out and had to be replaced anyway. The four motors, batteries, the radio transmitter, \u0026quot;velcro\u0026quot; fasteners, and motor-antivibration pads were taken from the previous build. There was no separate UBEC part anymore.\nThis is how it feels to fly analog FPV. Signal interferences are visible, but the latency is only 6ms!\n  And here is a video:\n  What remains now is to mount a GoPro camera on top for high-resolution images.\nLearn More  FPV FreeRider simulator Lessons how to fly an FPV drone   Powered by Betaflight software. ^ A pro tip: They recommend twisting the wires to reduce the interference. ^   ","date":1645049400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645049400,"objectID":"db60203f344dbd9fc0fa7e5f160fecb9","permalink":"https://penkovsky.com/homelab/fpv-quad/","publishdate":"2022-02-17T00:10:00+02:00","relpermalink":"/homelab/fpv-quad/","section":"homelab","summary":"It has been a while since I was thinking about upgrading my drone to a first person view version aka FPV. The idea was placing a camera for video streaming in real time. My first attempt was with a small raspi board and streaming image data from a raspi camera via WiFi. Because of the latency, range, and complicated setup this solution was not very practical.\nTherefore, I decided to make a \u0026quot;real\u0026quot; FPV drone.","tags":["DIY","Drone"],"title":"A Quadcopter From Scratch: FPV Upgrade","type":"homelab"},{"authors":["Vivek Parmar","Bogdan Penkovsky","Damien Querlioz","Manan Suri"],"categories":null,"content":"","date":1641337200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641337200,"objectID":"32da2fc9c8ba64c046d49b3a706de42d","permalink":"https://penkovsky.com/publication/stochastic-binary-cnn/","publishdate":"2022-01-05T00:00:00+01:00","relpermalink":"/publication/stochastic-binary-cnn/","section":"publication","summary":"With recent advances in the field of artificial intelligence (AI) such as binarized neural networks (BNN), a wide variety of vision applications with energy-optimized implementations have become possible at the edge. Such networks, have the first layer implemented with high-precision which poses a challenge in deploying a uniform hardware mapping for the network implementation. Stochastic computing can allow conversion of such high-precision computations to a sequence of binarized operations while maintaining equivalent accuracy. In this work, we propose a fully binarized hardware-friendly computation engine based on stochastic computing as a proof of concept for vision applications involving multi-channel inputs. Stochastic sampling is performed by sampling from a non-uniform (normal) distribution based on analog hardware sources. We first validate the benefits of the proposed pipeline on the CIFAR-10 dataset. To further demonstrate its application for real-world scenarios, we present a case-study of microscopy image diagnostics for pathogen detection. We then evaluate benefits of implementing such a pipeline using OxRAM-based circuits for stochastic sampling as well as in-memory computing based binarized multiplication. The proposed implementation is about 1,000 times more energy-efficient compared to conventional floating-precision based digital implementations, with memory savings of a factor of 45","tags":["Deep Learning"],"title":"Hardware-Efficient Stochastic Binary CNN Architectures for Near-Sensor Computing","type":"publication"},{"authors":null,"categories":null,"content":"","date":1634187600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634187600,"objectID":"f7bdb702c5e3c9627bccc53f0ae6d69c","permalink":"https://penkovsky.com/project/autonomous-factory/","publishdate":"2021-10-14T07:00:00+02:00","relpermalink":"/project/autonomous-factory/","section":"project","summary":"Autonomous molecule production factory powered by artificial intelligence.","tags":["Deep Learning"],"title":"Autonomous Factory","type":"project"},{"authors":["Matteo Cucchi","Christopher Gruener","Lautaro Petrauskas","Peter Steiner","Hsin Tseng","Axel Fischer","Bogdan Penkovsky","Christian Matthus","Peter Birkholz","Hans Kleemann","Karl Leo"],"categories":null,"content":"","date":1629756000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629756000,"objectID":"95669a288808e19b94f107d7a7f54d4e","permalink":"https://penkovsky.com/publication/biocompatible-reservoir-computing/","publishdate":"2021-08-24T00:00:00+02:00","relpermalink":"/publication/biocompatible-reservoir-computing/","section":"publication","summary":"Early detection of malign patterns in patients’ biological signals can save millions of lives. Despite the steady improvement of artificial intelligence–based techniques, the practical clinical application of these methods is mostly constrained to an offline evaluation of the patients’ data. Previous studies have identified organic electrochemical devices as ideal candidates for biosignal monitoring. However, their use for pattern recognition in real time was never demonstrated. Here, we produce and characterize brain-inspired networks composed of organic electrochemical transistors and use them for time-series predictions and classification tasks using the reservoir computing approach. To show their potential use for biofluid monitoring and biosignal analysis, we classify four classes of arrhythmic heartbeats with an accuracy of 88%. The results of this study introduce a previously unexplored paradigm for biocompatible computational platforms and may enable development of ultralow–power consumption hardware-based artificial neural networks capable of interacting with body fluids and biological tissues.","tags":["Reservoir Computing"],"title":"Reservoir computing with biocompatible organic electrochemical networks for brain-inspired biosignal classification","type":"publication"},{"authors":["Bogdan Penkovsky"],"categories":["3D","Homelab"],"content":" Recently NASA has made the first powered flight on Mars. They have deployed a drone called Ingenuity costing about $85 million. The helicopter was able to fly on about ten meters altitude (as of May, 7) over the surface of the Red Planet.\nCoincidently, I have also built a flying drone. This looked like a good challenge and an opportunity to learn about unmanned aerial vehicles (UAVs).\nHere is what I learned.\n   Ingenuity helicopter hovering on Mars (2021). Image credit: NASA/JPL-Caltech.   Contents\n Parts Assembling a Drone Some of the Challenges I Have Faced  Parts What you will essentially need is propellers, motors, and some way to power and control them.\n   Our quadcopter parts      Part Qty Unit W. (g) Weight (g) Price (EUR)     Arm (printed) 4 13 52    Arm support (printed) 2 3.5 7    Base (v1.1) (printed) 1 30 30    Top (v1.1) (printed) 1 21 21    1100 mAh Battery 3S 1 117 117 14.72   2300 KV Motors 2204 4 25 100 14.02   ESC (4 in 1) 1 19 19 24.84   F3 flight controller* 1 17 17 18.87   Radio receiver (FS-iA10B) 1 17.6 17.6 19   UBEC 3A 5V 1  ? 3.13   Wires* 1 20 20    Bolts and nuts* 1 18 18 \u0026lt;10   5040 Propellers 4  ? 7.38   Nylon spacers 40 mm 6  ? 5.72   \u0026quot;Velcro\u0026quot; fasteners 2-3  ? 1.63   XT60 connector 1  ? 3.27   Motor anti-vibration pads 4   2.01          Total**   418.6 124.59    * Approximate weight. \n** Total price without LiPo charger (iMAX B6), radio transmitter, and camera. You can reuse those from other projects. The prices are given for indicative purpose only. The parts are not guaranteed to be optimal or cheapest. \nThe 3D printed frame was downloaded from Thingiverse: design called Peon230. Initially I printed everything in PLA. However, I have found out that, when crashing, base and top parts tend to break. Now, I use nylon for those two parts: they are lighter in nylon and do not break so easily. By the way, I did not use glue for platform adhesion when printing with PLA1. Printing directly on the glass platform gave parts a shiny finish.\n   Fabricating quadcopter frame. Overall it took 11 hours to print all the parts.   Some Jargon ESC = electronic speed controller. Those translate control pulses coming from the flight controller into voltage actually driving motors. The ESC I used in this build contains actually four ESCs, permitting to control all four motors. That reduces the copter's weight, which is great.\nUBEC = universal battery elimination circuit (DC-DC converter). Helped me to use a single battery both for the motors (12 V) and for the flight controller (5 V). UBECs are switching converters and they are recommended over linear DC converters, which dissipate a lot of heat when stepping down the voltage.\n(Battery) 3S = 3 cells. Each cell contributes 3.7 V, therefore 11.1 V total. I used a battery with capacity of 1100 mAh. This battery gave me up to 15 minutes to fly.\nMotor characteristics. 2300 KV: 2300 revolutions per minute (RPMs) per volt (with no load attached to that motor). Therefore, at 12 volts, these motors are expected to achieve 12 * 2300 = 27600 RPMs. 2204: 22 is the rotor diameter and 04 is the stator height. Larger motors give you more torque, which is related to the uplift force you want to generate. This is especially important when considering the vehicle's weight. Racing drones have high thrust to weight ratio enhancing their maneuverability and ability to rapidly accelerate Camera drones, on the other hand, have lower thrust to weight ratio making them more stable and easier to pilot.\nBattery Safety The nominal voltage of a lithium-polymer (LiPo) battery cell (3.7 V) is actually closer to its storage voltage (3.8 V). When a battery cell is fully charged, it reaches 4.2 V. The battery should never be overcharged because of an explosion/fire hazard. Also discharging under 3 V is not recommended as the battery may break. It is a good idea using a balance charger that controls each cell individually. Be careful with your batteries. There exist special safety bags designed for LiPo charging. Never charge your batteries unattended.\n  -- Assembling a Drone Soldering Motor Wires First of all we need to make sure that our electronic part (flight controller, ESCs, motors, etc.) works properly. We solder three wires of each motor directly to ESCs. Please pay attention to the motor wiring as neighboring motors rotate in opposite directions. Then, we solder an XT60 battery connector.\n  In parallel to motors we solder a UBEC, not shown on the video2. We connect UBEC to power the flight controller from the same battery.\nHere is an example diagram how the flight controller can be connected to the receiver via the iBus protocol. It is also shown how the four motor outputs are connected to electronic speed controllers. Usually ground (black) wires are connected to the ESCs for the ground reference3. If you decide to power the flight controller from ESCs (provided that your ESCs have BEC circuits), it is not recommended to provide more then one VCC (typically red) wire. In my configuration I do not power the flight controller from ESCs. Instead, I use a separate UBEC connected to the battery in parallel.\n   Wiring the receiver and, flight controller, and motor ESCs   The diagram above also illustrates the direction of the motor rotation. Pay attention as it is equally important to install the appropriate propellers before the flight. Crucially, all four propellers will have to push the air down. It is also a good idea to test every motor individually (without propellers of course) to verify if signals are arriving to the correct motors (see Tuning the Flight Controller section below).\nComplete Build In this video we assemble the frame from earlier printed parts. We put motors, ESCs, flight controller, and the receiver on the frame. The flight controller is bolted on top of the 4-in-1 ESC. Then, we connect the radio receiver to the flight controller. We also connect flight controller output wires to ESCs. We could solder everything instead, but these connections are already good for the test. We test connection with a transmitter and the motors. We finalize the build by putting the remaining part of the frame on top.\n  Please also note that it is crucial that your final build is tight. No wire, nothing should be in the way of your propellers.\nTuning the Flight Controller While your UAV may seem functional, it is probably not yet ready to fly. In fact, the flight controller is probably the most important part, it acts as a \u0026quot;quadcopter's brain\u0026quot;. The controller interprets the received radio commands and the data coming from its sensors, most notably from the inertia measurement unit (IMU), to stabilize and steer the vehicle. In fact, without a flight controller it would be virtually impossible for a human to pilot a quadcopter.\nTo make sure the copter interprets the flight situation adequately, the controller has to be properly calibrated. This includes calibrating the IMU and compass. It is also important to verify that the remote control commands are properly interpreted. And that the appropriate motors are activated. To perform these individual checks and calibrations, I used Clean Flight software. However, there exist multiple alternative options, such as Beta Flight, which is a popular fork of Clean Flight.\nSome of the Challenges I Have Faced  Vibrations on captured videos from the quadcopter turned out to be mostly because of slightly damaged propellers (after emergency landings) The best way to position antennas turned out to be along the body. This way they don't get into the way if crashing. Regulations. In Europe, you need to obtain a special permission to fly a drone. Also there are very strict limitations where you can fly as a hobbyist.  I am still learning to fly this thing.\n    Next Episode Learn how to upgrade to FPV.\nLearn More  Ingenuity helicopter Clean Flight FPV FreeRider simulator Lessons how to fly an FPV drone   It is always advised to apply 3d printing glue for nylon prints because of high warping forces of this material. ^ I used UBEC in the second build. In the first build, I had a separate battery and a linear step-up DC converter to power the flight controller. ^ Since I use a 4-in-1 ESC, it is a single ground wire. ^   ","date":1620552780,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620552780,"objectID":"90cfa404e969ac3a6bb3071bec896c58","permalink":"https://penkovsky.com/homelab/quadcopter/","publishdate":"2021-05-09T11:33:00+02:00","relpermalink":"/homelab/quadcopter/","section":"homelab","summary":"Recently NASA has made the first powered flight on Mars. They have deployed a drone called Ingenuity costing about $85 million. The helicopter was able to fly on about ten meters altitude (as of May, 7) over the surface of the Red Planet.\nCoincidently, I have also built a flying drone. This looked like a good challenge and an opportunity to learn about unmanned aerial vehicles (UAVs).\nHere is what I learned.","tags":["DIY","3D Printing","Drone"],"title":"A Quadcopter From Scratch","type":"homelab"},{"authors":null,"categories":null,"content":"","date":1619012400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619012400,"objectID":"08d7e4144b58b8257c0450b314b83f3c","permalink":"https://penkovsky.com/talk/optical-computing-applications2021/","publishdate":"2021-04-21T15:40:00+02:00","relpermalink":"/talk/optical-computing-applications2021/","section":"talk","summary":"Optical (photonic) computing uses photons for computation. The advantages of this approach are inherent parallelism and high processing speeds. Recent advances in silicon photonics have led to miniaturization of optical computing systems on an integrated chip. Moreover, high energy efficiency of such systems promise to help overcoming the von Neumann bottleneck and open a window for new photonic applications. In this talk we will discuss how optical hardware can impact artificial intelligence, quantum computing, and cryptocurrency.","tags":[],"title":"Optical Computing Applications","type":"talk"},{"authors":["Bogdan Penkovsky"],"categories":["3D","Homelab"],"content":"Here is a step by step tutorial on how to design objects for 3D printing using OpenSCAD. We illustrate the design process by creating a micro quadcopter frame. This small drone bears the code name of Beatle-1. After following the tutorial you will be able to conceive your own designs for 3D printing.\nContents\n 3D Printing Intro Designing a Drone Frame Building Demo  3D Printing Intro 3D printing (known in the industry as additive manufacturing) is a process of fabrication of a 3D object from a digital model. 3D printer prices have significantly dropped during past decade, so 3D printing is much more common nowadays. The reason why we don't see such printers in every household is not economical, but rather psychological. People often prefer ready-made solutions to their problems.\nDespite that, maker communities such as Thingiverse provide millions of objects for free. Having access to a 3D printer means that you can download and fabricate things that you would need to buy otherwise. Last Christmas, for example, I have printed pegs to hang Christmas stockings :)\n3D printing steps  Computer-aided design (CAD) Export to STL model (de-facto standard format) File preparation (slicing) - transforming into the series of commands \u0026quot;understood\u0026quot; by a 3D printer Building (\u0026quot;3D printing\u0026quot;) Post-processing (removing fabricated parts from the printing bed, removing support, etc.)  Materials Plastics, such as PLA, are the most common materials you can use to 3D print objects. While PLA is one of the popular materials to print with, it is also relatively fragile. For increased robustness, it is possible to use e.g. nylon, which like PLA is also compatible with FFF (FDM), the most common 3D printing process. In the industry, there exist different other methods to fabricate objects in metal, carbon fiber, PEEK, and many other materials.\nFAQ  If you’ve ever written a simple blog post or email in HTML, you can handle OpenSCAD.\nCubehero\n OpenSCAD vs traditional graphic CAD interfaces\nQ: Why should I use OpenSCAD?\nA: There might be different reasons:\n OpenSCAD is a great project (and it is open source). You prefer programming to avoid wasting time for repeatable actions. Then, you can learn OpenSCAD in about an hour. Or maybe you have never done any programming, but would not mind to acquire this valuable skill. Your OpenSCAD models are easily parametrizable, virtually out-of-box.  Designing a Drone Frame Some requirements for the frame I had in mind:\n It should accommodate four motors of a given shape. The motor positions should be stable when flying (they should not wiggle). The propellers should be far enough from each other. Easy to print (fast, no support, one or two pieces). Lightweight frame.  CAD design is an iterative process. My first attempt at designing a drone frame was naive and looked like this.\n   First version. Don't recommend.   A quadcopter's frame is simply a holder for four motors and a body. Right? Wrong. The biggest issue with this design was that the arms were fragile. In fact, one of them cracked while I was removing the newly fabricated frame from printer's platform. Clearly, a better solution was required.\nOne improvement might be to orient the arms vertically, not horizontally as they are. That would make them more robust in the vertical direction and prevent undesired wiggling. I have also recalled that I have seen a micro quadcopter on Thingiverse some time ago. Indeed, this copter's design suggests a more robust frame. What I like about it is that each motor is supported not by one, but two segments making it more stable in the horizontal plane. So let's see what we can build with OpenSCAD.\nBasic Operations: Motor Compartment Design I like to start my design from individual components (bottom-up), however, the opposite method (top-bottom) is also perfectly valid. For starters, I have created emplacements for F-1607 DC motors, which I have ordered from Olimex. This part can be imagined as a shell around the motor, or a Boolean difference between the external box (gray cylinder) and the motor itself (red cylinder).\n   A motor holder is simply a difference between two cylinders: the outer (grey) and the inner (highlighted in red).   The motor dimensions given by Olimex are 7 mm diameter and 16.5 mm height. I have chosen shell size parameter to be 0.8 mm. I have lifted the inner cylinder by 0.8 mm (translate) so that the result is a box, not a tube. The red transparent cylinder only illustrates where the motor goes, so it will be not visible in the final rendering.\nshell=0.8; difference() { cylinder(d=7 + 2 * shell, h=16.5); translate([0, 0, shell]) cylinder(d=7, h=16.5); } How about the motor wires? We can carve out a side and a bottom holes using Boolean difference and union transformations.\n   A motor compartment: perspective view and top view. The holes will allow motor wires to escape comfortably.   module SlotF1607(h=16.5, d=7, shell=0.8, // Shell thickness  slack=0.1 // Some extra space to accommodate a motor  ) { difference() { // Motor compartment  cylinder(d=d + 2 * shell, h=16.5); union () { // Motor emplacement  translate([0, 0, shell]) cylinder(d=d + slack, h=16.5); // Bottom hole for motor contacts/wires  cylinder(d=6, h=h, center=true); // Wires hole  translate([d/4, 0, 0]) cube([d, 2.5, h * 3], center=true); } } } Last but not least you can see that I have also given some slack for a motor (0.1 mm) to more easily fit into its compartment.\nPlacing Motor Compartments Usually quadcopter size is characterised by its diagonal, i.e. the largest distance between the centers of the propellers (thus the centers of the motors). Our microdrone is small and its diagonal will be only 92 mm (it can fit into a hand). The nearest distance between any two motors is then computed as $\\sqrt{2} \\cdot (\\text{diagonal} / 2)$. This distance is an edge of a square with 92 mm diagonal. Now, we can easily place the four motors on the vertices of the imaginary square.\n  diagonal = 92; motors_dist = sqrt(2) * (0.5 * diagonal); // Half distance between motors half_dist = 0.5 * motors_dist; for (i = [1, -1], j = [1, -1]) { // Place the motors  translate([half_dist * i, half_dist * j, 0]) SlotF1607(); } Note that for (i = [1, -1], j = [1, -1]) is simply a shortcut of two nested loops, i.e. it is the same as\nfor (i = [1, -1]) for (j = [1, -1]) // ... Anything wrong? Yes, we forgot to properly rotate the wire outlets.\nfor (i = [1, -1], j = [1, -1]) { // Place the motors  translate([half_dist * i, half_dist * j, 0]) rotate([0, 0, -45 * i * j - 90 * j]) // \u0026lt;--  SlotF1607(); }   Now much better.\nMotors Support We would like to connect the motors using something like an arc. How to do that? First, create and place a small segment:\ntranslate([41, 0, 0]) square([1.2, 3.6]); Then, by rotating this segment in space (rotate_extrude), we obtain a 90-degree arc:\nmodule arc() { rotate_extrude(angle=90, $fn=70) translate([41, 0, 0]) square([1.2, 3.6]); }    An arc created with rotate_extrude()   Let us link those arcs to the motors.\nfor (i = [1, -1], j = [1, -1]) { // Place the motors  translate([half_dist * i, half_dist * j, 0]) rotate([0, 0, -45 * i * j - 90 * j]) SlotF1607(h=6.5); // Connect the motors  translate([half_dist * j * (i + 1), half_dist * j * (i - 1), 0]) rotate([0, 0, 45 * i + 90 * j]) arc(); }   I figured out that probably 6.5 mm slot heigh would be enough. That is why I have specified the height parameter SlotF1607(h=6.5). Works like a charm!\nThe Platform By now I have realized that to create the body I could reuse the same method as for the motor emplacements to create an open box, a box with no top cover. However, the only distinction would be the shape (a Boolean difference between rectangular boxes rather than cylinders). Now, I would like to somehow generalize a method of creating an open box. Ideally, I would like to create boxes of any base shape (circle, square, etc.), thus parametrizing the method by a base shape.\nTo help me with this idea, OpenSCAD provides children() method to access child modules. The new operation could be described as\nmodule openbox(delta=0.8) { difference() { resize_somehow(...) children(); translate([0,0,delta]) children(); } } So for example the motor compartment could be created simply as\nopenbox(h=16.5, delta=shell) circle(d=7); Where openbox would receive a 2D shape as a child (a circle, a square, or a polygon) and transform it into a 3D shape (a box) of a given height using the recipe from above. So how to transform a 2D circle into a 3D cylinder? Using a common method called linear_extrude:\nlinear_extrude(16.5) circle(d=7); This method is similar to rotate_extrude that we used for the arc. However, it does not rotate the base 2D shape.\nNow, how to create a larger version of the cylinder (becoming the shell)? One way to do that would be to use scale() or resize(). However, a more efficient method would be to operate on the original 2D object, before extruding it (that is actually the reason why our openbox was assigned to operate on 2D objects). The offset() transformation does exactly what we need to: enlarge the 2D object by a \u0026quot;delta\u0026quot; difference (see documentation). Therefore, the complete openbox module becomes\nmodule openbox(h=10, delta=0.8) { difference() { linear_extrude(h) // Create a larger base by offsetting the child module by delta  offset(delta=delta) children(); translate([0,0,delta]) linear_extrude(h) children(); } } Create a motor compartment as planned\nopenbox(h=16.5, delta=0.8) circle(d=7); Here is how to refactor the SlotF1607 module, making use of openbox\nmodule SlotF1607(h=16.5, // Module height  d=7, // Motor diameter  shell=0.8, // Shell thickness  slack=0.1 // Some extra space to accommodate a motor  ) { difference() { // Motor compartment shell  openbox(h=h, delta=shell) circle(d=d + slack); union () { // Bottom hole for motor contacts/wires  cylinder(d=6, h=h, center=true); // Wires hole  translate([d/4, 0, 0]) cube([d, 2.5, h * 3], center=true); } } } Similarly, create the body\nopenbox(h=3.6, delta=0.8) square([46, 23], center=true); If it is hard to follow this section, note what we do in the line above is the same operation as with the cylinders in the very beginning:\nshell=0.8; difference() { cube([46 + 2 * shell, 23 + 2 * shell, 3.6]); translate([0, 0, shell]) cube([46, 23, 3.6]); }   We are essentially done. However, this design somewhat does not inspire me. Replacing the base square with a rounded one looks like a good idea. After all, we designed openbox to support any 2D base shape. Here is a common idiom: create a rounded square as a Minkowski sum.\nmodule rounded_square(dim=[20,20], r=5) { minkowski() { square([dim[0] - 2 * r, dim[1] - 2 * r], center=true); circle(r=r); } }   So here we go:\nfor (i = [1, -1], j = [1, -1]) { // Place the motors  translate([half_dist * i, half_dist * j, 0]) rotate([0, 0, -45 * i * j - 90 * j]) SlotF1607(h=6.5); // Connect the motors  translate([half_dist * j * (i + 1), half_dist * j * (i - 1), 0]) rotate([0, 0, 45 * i + 90 * j]) arc(); } openbox(h=3.6, delta=0.8) rounded_square([46, 23]);   Much better!\nNow, I would like to shave off a bit of weight if possible. So I will introduce some holes as e.g. in the honeycomb pattern. I will generate an array of hexagonal cylinders and subtract it from the platform. A hexagonal cylinder is simply a cylinder with six fragments, i.e. cylinder($fn=6, …);. Here is an array of those generated with two nested loops for (i = [-M:M], j = [-N:N]):\nmodule honey_comb(h=10, M=10, N=4, d1=4, d2=4.6) { for (i = [-M:M], j = [-N:N]) { // Shift the row by d1/4  if (abs(i) % 2 == 0) translate([i * d1, j * d2 + d1/4, -h]) cylinder(d = d1, h = h * 2, $fn=6); // Shift the row by -d1/4  else translate([i * d1, j * d2 - d1/4, -h]) cylinder(d = d1, h = h * 2, $fn=6); } } Where abs(i) % 2 == 0 simply checks if index i is an even value so that odd and even rows are shifted by a different amount.\n  Now I use an intersection with another shape to limit the span of the honeycomb pattern.\nintersection() { honey_comb(h=11, M=6, N=4, d1=4.1, d2=4.9); // Limited by this volume  translate([0,0,-0.5]) linear_extrude(10) rounded_square([dim[0]-2.5, dim[1]-2.5], r=6); }   Finally, I subtract the above pattern and some side holes.\n// The platform module platform(dim=[46, 23], shell=0.8) { difference() { openbox(h=3.6, delta=0.8) rounded_square(dim); union () { // Honey-comb patterned floor  intersection() { honey_comb(h=11, M=6, N=4, d1=4.1, d2=4.9); // Limited by this volume  translate([0,0,-0.5]) linear_extrude(10) rounded_square([dim[0]-2.5, dim[1]-2.5], r=6); } // Side holes  for (i = [-1,1]) translate([12.5 * i, 0, shell + 4]) cube([11, 40, 8], center=true); } } } // The main module module beatle1() { for (i = [1, -1], j = [1, -1]) { // Place the motors  translate([half_dist * i, half_dist * j, 0]) rotate([0, 0, -45 * i * j - 90 * j]) SlotF1607(h=6.5); // Connect the motors  translate([half_dist * j * (i + 1), half_dist * j * (i - 1), 0]) rotate([0, 0, 45 * i + 90 * j]) arc(); } platform(); } beatle1();   That was quite easy. Wasn’t it?\nI made the complete design available on Thingiverse.\nTo learn OpenSCAD, check its official cheatsheet. For example, there you will find the magic parameter $fn (number of fragments) which I have used to make the shapes smooth (or conversely to create hexagonal cylinders).\nBuilding First, export the design as STL. This is quite straightforward: after rendering the design, press  and save an .stl file.\nParameters:\n Fabrication method: FFF Material: PLA 0.2 mm layer height Three perimeters 30 % infill Designed to be fabricated with no support  Next, open your favorite slicer software that often comes with a 3D printer. You will need to specify 0.2 mm layer height and other settings from the list above. After slicing (transforming into a series of commands that 3D printer can perform) the design can be inspected layer by layer.\n   Sliced Beatle-1. The pink lines (so-called skirt) will be printed first to stabilize the flow of plastic.   Demo I used a set of four F-1607 DC motors with propellers, a flight controller from a drone toy, and a 150 mAh LiPo battery.\n  Some of the Challenges I Have Faced  The design from the very first version was impractical and fragile. Motor wires could be easily detached from the controller. To prevent this, I fixated the wires to the frame. I tried several methods to create the openbox module. The one presented here was the least ambiguous.  As always, if you have any questions, remarks, or spotted any typos please send me a message.\nLearn More  Fused filament fabrication OpenSCAD cheatsheet ","date":1616344800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616344800,"objectID":"564fd53042b92692ac5059e385e88249","permalink":"https://penkovsky.com/homelab/beatle/","publishdate":"2021-03-21T18:40:00+02:00","relpermalink":"/homelab/beatle/","section":"homelab","summary":"Here is a step by step tutorial on how to design objects for 3D printing using OpenSCAD. We illustrate the design process by creating a micro quadcopter frame. This small drone bears the code name of Beatle-1. After following the tutorial you will be able to conceive your own designs for 3D printing.\n","tags":["DIY","3D Printing","Drone","Tutorial"],"title":"3D Printing Tutorial: Designing a Micro Drone Frame","type":"homelab"},{"authors":["Bogdan Penkovsky"],"categories":["Raspberry Pi","Homelab"],"content":" We will build an autonomous robot. Captured by robot's camera, video stream will be analyzed by a neural network. The network will be running on the onboard Raspberry Pi that will steer the robot. Before you start with the project, I want you to answer two questions. First, how everything will be attached mechanically? Second, what will be the energy source? While your autonomous robot can work from a cardboard box, having a mechanically sound chassis will greatly improve the result on the AI training stage. And the question of the energy source is essential for any autonomous robot. Ready to answer? Then let's go!\nParts And Tools For this project I used:\n Raspberry Pi 3B, 1 GB RAM (priceless 😉) Power bank (for mobile phone) USB to micro-USB cable SD memory card class 10, at least 16 GB, $11.99 Chassis + 2 DC motors $13.99 Raspberry Pi camera $17.21 L298N dual H-bridge controller $6.89 9 V battery (local store) Jumper wires Something to attach the components to chassis (I used rubber bands and adhesive putty) Enclosure for the electronics  Tools:\n Soldering iron $20.07 A laptop with WiFi and SSH client (for robot training) Multimeter (I didn't have one at hand, but it would have been useful) Small screw driver Wire strippers (optional) Breadboard (optional)  Note that the prices may vary and they are here for indicative budget only. You can also order similar parts on AliExpress or other sites. The bottom line is that you can easily build a self-driving robot under $100.\nI decided to power the Raspberry Pi board and motors separately so that the board will be reliably powered, even when the motors battery exhausts itself. The power bank for that purpose (originally, for a mobile phone) and a USB cable I already had at home, so they didn't count towards the budget.\nI recommend at least 16 GB SD memory card so that you can install the Raspberry Pi OS and have some storage space for training data acquisition (images from the camera). Regarding camera itself, usually it is recommended to use a wide-angle camera. However, feel free to use any camera you have (as I did).\nThe H-bridge circuit is useful to allow DC motors to run forwards or backwards. Unlike some tutorials, mine H-bridge required between 7 V and 35 V driving voltage. To see if the circuit is working, check if the red LED is on. With 6 V driving voltage, it was still off. Therefore, I used a 9 V battery to power the motors. Finally, using a laptop with Linux will make your life easier. But feel free to use whatever you have.\nPart I: Let The Hack Begin Assemble The Robot 1. Assemble the chassis. Attach the wheels, the ball caster, and the motors. Don't mind the battery holder, I didn't use it in the end.\n  2. Solder wires to the motors.   3. Connect motors to the H-bridge controller. Here you will need a small screw driver.\n  4. Connect the motors controller to the Raspberry Pi. Here I use a breadboard in-between, but feel free to connect the jumper wires directly. You can choose any free (non-specialized) GPIOs pins. Please refer to the board's pinout.\n   Raspberry Pi pinout. Source: pinout.xyz.   For instance, I have chosen pins 15, 11, 13, and 31. Note those are Raspberry Pi pins, not GPIO numbers. For example, the above pins correspond to GPIO 22, GPIO 17, GPIO 27, and GPIO 6. Remember those magic numbers.\n Please do not confuse pins and GPIO numbers. The gpiozero Python module refers to GPIO numbers, whereas the software that we will use later requires Raspberry Pi pins.\n To determine the order in which you connect the jumper wires to the controller you may want to check the controller's data sheet. Or determine the pins experimentally, testing the digital inputs in isolation. There are four wires: two controlling the left wheel (IN1 and IN2) and two for the right wheel control (IN3 and IN4). For example, when both IN1 and IN3 digital signals are high, the robot will move forward.\nNote that you will have to connect the ground (GND) of the controller board and the ground of Raspberry. Otherwise, your controller may not be able to interpret the digital signal it receives from the Raspberry.\n  5. Power up the motors controller with a 9 V battery. I didn't have a battery connector at hand, so I used a chewing gum to attach the wires to the battery.\n  6. Power up the Raspberry Pi. Finally, power up the Raspberry Pi with a power bank using a small USB cable.\n You may want to assemble the modules neatly so that the robot is actually autonomous. Make sure that the hardware is fixed well and will not move during the ride.\n   Sanity Check Let us test what robot we have built so far! First, we connect to the Raspberry Pi. This is done via SSH. Setup Raspberry Pi WiFi connection and connect to your board. Now, we can play with the robot interactively. Being connected to the Raspberry Pi via SSH, type python to open up the Python interpreter. Then define your robot object\nfrom gpiozero import Robot robot = Robot(left = (22, 17), right = (27, 6)) Note the GPIO numbers we have discussed before. Those are your connections to the bridge controller. To make the robot do something\nrobot.left(0.4) Here 0.4 is the speed with which to rotate the left wheel.\n  To stop the robot, issue\nrobot.stop() Type exit() (or press Ctrl-D) to exit the Python's interpreter's shell. In your favourite text editor you can create a sequence of actions to pre-program your robot.\nfrom gpiozero import Robot from time import sleep robot = Robot(left = (22, 17), right = (27, 6)) print(\u0026#34;Left\u0026#34;) robot.left(0.4) sleep(1) robot.stop() print(\u0026#34;Right\u0026#34;) robot.right(0.4) sleep(1) robot.stop() print(\u0026#34;Forward\u0026#34;) robot.forward(0.4) sleep(3) robot.stop() Here we ask the robot to rotate left for a second, then rotate right for a second, and finally, move forward for three seconds. Save it to a file on your Raspberry Pi, e.g. robo.py, and execute\npython robo.py If we have missed something, here is an official Raspberry Pi tutorial to assist with assembling your robot. Before you continue, now is a good time to attach the camera.\nPart II: Remote Control Now that the robot is based on the Raspberry Pi, we can do amazing things with it. For example, we can benefit from the DonkeyCar, an open source platform to build a small scale self driving car. You will have to follow the instructions to install the necessary software on your Raspberry Pi.\nAs soon as you finish with installation, create your Donkeycar App. Connect to your Pi with SSH and type\ndonkey createcar --path ~/mycar Then check the newly created project in ~/mycar. Thankfully, Donkey supports our two-wheel robot almost out of the box. The remaining bits are to edit config.py. Set\nDRIVE_TRAIN_TYPE = \u0026#34;DC_TWO_WHEEL\u0026#34; This will select the appropriate motor's driver. Finally, set the pins as discussed before:\nHBRIDGE_PIN_LEFT_FWD = 15 HBRIDGE_PIN_LEFT_BWD = 11 HBRIDGE_PIN_RIGHT_FWD = 13 HBRIDGE_PIN_RIGHT_BWD = 31 Now, test if everything was successful.\ncd ~/mycar python manage.py drive In under a minute, the Donkey server should be up and running. As a result, you will be able to access a control web interface using http://\u0026lt;your car's IP\u0026gt;:8887. This will allow you to remotely control the car and capture the video stream with its camera. Like in this video.\n  Part III: Making It Self-Driving Being able to control your car remotely is useful to collect the training data for the neural network. But first we may want to set up a driving track like this.\n  Now, drive several laps on your track. The captured image data are automatically stored in ~/mycar/data. Having collected a sufficient amount of data we pass to the driving model (neural network) training. Here is how the world looks like from the car's viewpoint.\n  Green vector illustrates human-driver commands and the blue one is the model after training.\nYou want to revisit ~/mycar/config.py to set the neural network parameters, e.g. LEARNING_RATE and BATCH_SIZE. The key training command is\npython manage.py train --type linear --model models/mypilot.h5 You can experiment with any of the driving models: linear, categorical, rnn, imu, behavior, etc. More housekeeping details can be found in this notebook adapted for TensorFlow 2. The original version can be found here.\nEt voilà, your self-driving car is ready!\n Now, run the auto-pilot\nscreen cd ~/mycar python manage.py drive --model models/mypilot.h5   What Could Be Done Better  The mechanical part could be better. After I have mounted the acrylic chasssis, I have realized that the motors do not hold firmly. Moreover, there was a friction between one of the wheels and chassis. Therefore, I had to programmatically adjust the power to balance the wheels.\n Moreover, I believe that having a streering wheel motor would be much more efficient, instead of differential drive, as they call this construction in the robotics community. To test in the future.\n Likewise, it is important to keep your camera in the same position during training and during self-piloting. Also make sure that the battery is always charged.\n It would be nice to have a 9 V battery connector, instead of a piece of putty. As simple as that.\n Quality of the training data. The better is your driving strategy, the better training data you will produce, the better the car will learn its turns.\n Transfer learning. You can pretrain your driving model in a virtual environment such as Donkey Gym. See also this video.\n     A robot from 70s I have seen at Musée des Arts et Métiers in Paris. The robot inspired me for this project.   I would like to thank the folks from the Integnano group who gave me a Raspberry Pi as a birthday gift. You are amazing!\nAs always, if you have any questions, remarks, or spotted any typos please send me a message.\nLearn More  Build a robot buggy tutorial Differential drive Donkey Car platform LitterBug donkey Driving Tips to Train your Autonomous End-to-End NN Driver  ","date":1605474000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605474000,"objectID":"d83c10c0bc92467385826080efb6aea9","permalink":"https://penkovsky.com/homelab/self-driving-robot/","publishdate":"2020-11-15T23:00:00+02:00","relpermalink":"/homelab/self-driving-robot/","section":"homelab","summary":"We will build an autonomous robot. Captured by robot's camera, video stream will be analyzed by a neural network. The network will be running on the onboard Raspberry Pi that will steer the robot. Before you start with the project, I want you to answer two questions. First, how everything will be attached mechanically? Second, what will be the energy source? While your autonomous robot can work from a cardboard box, having a mechanically sound chassis will greatly improve the result on the AI training stage.","tags":["DIY","Raspi","Deep Learning"],"title":"Making Your Self-Driving Robot","type":"homelab"},{"authors":["Bogdan Penkovsky","Marc Bocquet","Tifenn Hirtzlin","Jacques-Olivier Klein","Etienne Nowak","Elisa Vianello","Jean-Michel Portal","Damien Querlioz"],"categories":null,"content":"The results are very encouraging to reduce efficiently memory requirement of edge devices and thus to obtain low energy hardware. This is particularly useful for medical applications where little energy is available.\n","date":1592172000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592172000,"objectID":"e8727436b0c3566b3e7c7963be8b2a36","permalink":"https://penkovsky.com/publication/medical-bnn/","publishdate":"2020-06-15T00:00:00+02:00","relpermalink":"/publication/medical-bnn/","section":"publication","summary":"The advent of deep learning has considerably accelerated machine learning development. The deployment of deep neural networks at the edge is however limited by their high memory and energy consumption requirements. With new memory technology available, emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation, enabling machine learning on the edge devices and avoiding data transfer over the network. In this work, after presenting our implementation employing a hybrid CMOS - hafnium oxide resistive memory technology, we suggest strategies to apply BNNs to biomedical signals such as electrocardiography and electroencephalography, keeping accuracy level and reducing memory requirements.  We investigate the memory-accuracy trade-off when binarizing whole network and binarizing solely the classifier part. We also discuss how these results translate to the edge-oriented MobilenetV1 neural network on the Imagenet task. The final goal of this research is to enable smart autonomous healthcare devices.","tags":["Deep Learning","EEG"],"title":"In-Memory Resistive RAM Implementation of Binarized Neural Networks for Medical Applications","type":"publication"},{"authors":["Bogdan Penkovsky"],"categories":["Raspberry Pi","Homelab"],"content":" Raspberry Pi is great. It is a small and cheap computer that can be used for a variety of projects. Sometimes you may wish to remotely control a Rasperry Pi that is in your local WiFi network. This is possible and even quite easy.\nWe will use a Secure Shell aka SSH to establish a remote connection. I assume that you have previously connected your Raspberry Pi board to a WiFi network. I also assume that you are connecting from a Linux or OS X machine.\nStep 1: Enable SSH On Raspberry Pi There are several methods to enable SSH access, including graphical user interface (GUI). However, I find the easiest method is without even turning on your Raspi board. This also does not require any GUI to be installed. We enable SSH simply by creating a blank file named ssh on the Raspbian OS boot partition. That is on the memory card where Raspbian OS is intalled.\nSo insert the Raspi SD card in your computer. Assuming that the SD card maps to two devices1, /dev/mmcblk0p1 and /dev/mmcblk0p2, the first of them usually corresponds to the /boot partition. Below, we mount the partition to an existing directory /mnt/usbstick. Finally, we create (\u0026quot;touch\u0026quot;) a blank file ssh within the directory.\nsudo mount /dev/mmcblk0p1 /mnt/usbstick cd /mnt/usbstick sudo touch ssh Afterwards, before inserting the memory card back into Raspberry Pi, we unmount the partition:\ncd sudo umount /dev/mmcblk0p1 Done. Now, during its boot, Raspbian will find the ssh file and will allow SSH access.\nStep 2: Discover IP Address of Raspberry Pi One way is to boot your Raspi, open terminal and type:\n$ hostname -I 192.168.0.13 If this works for you, note the address and skip directly to Step 3. However, if there is no monitor connected to your Raspi board this is not a problem at all. You can discover connected devices on your WiFi network. First, learn your network address range and then scan it for connected devices.\nNetwork Information ip addr  Note that on some machines ip addr command is called ifconfig.\n This will provide the information about network interfaces.\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: wlp2s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 9c:b6:d0:8e:ad:19 brd ff:ff:ff:ff:ff:ff inet 192.168.0.11/24 brd 192.168.0.255 scope global dynamic noprefixroute wlp2s0 valid_lft 85985sec preferred_lft 85985sec inet6 fe80::42f4:2e5a:a6b2:9232/64 scope link noprefixroute valid_lft forever preferred_lft forever ... Our computer has local IP address 192.168.0.11. Therefore, our WiFi network has IPs starting with 192.168.0..\nScan Your WiFi Network Install nmap using your system's package manager. For example, on Arch Linux:\nsudo pacman -S nmap Scan with\n$ nmap -sn 192.168.0.0/24 ... Nmap done: 256 IP addresses (3 hosts up) scanned in 2.52 seconds If your Raspi board was off, boot it now, wait until it connects and scan the network again.\n$ nmap -sn 192.168.0.0/24 Starting Nmap 7.80 ( https://nmap.org ) at 2020-04-25 16:03 CEST Nmap scan report for 192.168.0.1 Host is up (0.044s latency). Nmap scan report for 192.168.0.5 Host is up (0.019s latency). Nmap scan report for 192.168.0.11 Host is up (0.000066s latency). Nmap scan report for 192.168.0.13 Host is up (0.016s latency). Nmap done: 256 IP addresses (4 hosts up) scanned in 2.37 seconds The newly appeared host is your board.\nStep 3: Connect To Raspi $ ssh pi@192.168.0.13 pi@192.168.0.13\u0026#39;s password: Linux raspberrypi 4.19.75-v7+ #1270 SMP Tue Sep 24 18:45:11 BST 2019 armv7l The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Sat Apr 25 16:03:26 2020 Now, we can control the board or even exchange files with scp.\n$ pi@raspberrypi:~ $ whoami pi Hint: do not forget to change the default user password:\nsudo passwd pi  Check dmesg|tail output to see the newly connected devices. ^   ","date":1587823380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587823380,"objectID":"a7475f0f309f876efd41bfdd9f150cb0","permalink":"https://penkovsky.com/homelab/raspi-ssh-over-wifi/","publishdate":"2020-04-25T16:03:00+02:00","relpermalink":"/homelab/raspi-ssh-over-wifi/","section":"homelab","summary":"Raspberry Pi is great. It is a small and cheap computer that can be used for a variety of projects. Sometimes you may wish to remotely control a Rasperry Pi that is in your local WiFi network. This is possible and even quite easy.\nWe will use a Secure Shell aka SSH to establish a remote connection. I assume that you have previously connected your Raspberry Pi board to a WiFi network.","tags":["DIY","Raspi"],"title":"Connecting To Raspberry Pi Over WiFi","type":"homelab"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Last week Apple has acquired XNOR.ai startup for amazing $200 million. The startup is known for promoting binarized neural network algorithms to save the energy and computational resources. That is definitely a way to go for mobile devices, and Apple just acknowledged that it is a great deal for them too. I feel now is a good time to explain what binarized neural networks are so that you can better appreciate their value for the industry.\nToday's post is based on\n Day 1: Learning Neural Networks The Hard Way Day 2: What Do Hidden Layers Do? Day 4: The Importance Of Batch Normalization  The source code from this post is available on Github.\nThe Basics Knowing what a (real-weight) neural network is you already have the most essential information. In contrast, a binarized neural network (BNN) has its weights and activations limited to a single bit precision. Meaning that those have values of either $+1$ or $-1$. Below is a comparison table.\n   Binarized neural network vs conventional neural network with real weights   Having single-bit values can drastically reduce hardware requirements to operate those networks. As you remember from Day 1, neural networks are built around activations of dot products. When every value is $+1$ or $-1$, multiplication can be replaced with $\\text{XNOR}$ binary operation, thus eliminating the need in hardware multipliers. The dot product sum is then replaced by a simple bit counting operation ($\\text{Popcount}$). And finally the activation is merely a thresholding ($\\text{Sign}$) function.\nOverall, the architecture can speed up conventional hardware or even better, drastically reduce the area of dedicated ASICs. Reduced area means smaller manufacturing cost. Not less important is another implication, reduced energy consumption. That is crucial for edge devices. To further save the energy one can even merge computation and memory giving rise to emerging in-memory computing technology.\nAs we have discussed before, batch normalization is helpful to prevent neurons from saturation and to faster train the network. It turns out that for binarized networks batchnorm is actually indispensable. Indeed, neurons with $\\text{Sign}$ activations tend to be useless (\u0026quot;dead\u0026quot;) otherwise. Moreover, for an efficient training, to make the error landscape smooth, it is advisable to also include batch normalization immediately before the softmax layer. Yes, the most important lesson to learn here is is actually how to train a BNN. I attempt to answer the most frequently encountered questions in the following section.\nBinarized Neural Networks FAQ Here are some typical questions that people tend to ask.\nQ: Do I need to provide binarized inputs to a BNN?\nA: No, BNN inputs can remain non-binarized (real). That results in the first layer slightly different from the others: having binary weights (+1 and -1), yet producing real pre-activation values. After activation, those values are converted to binary ones. Here is a hint:\n It is relatively easy to handle continuous-valued inputs as fixed point numbers, with $m$ bits of precision. Courbariaux et al. Binarized Neural Networks: Training Deep Neural Networks... (2016)\n You should also be aware that indeed there exist methods to have binary inputs as well.\nQ: Do I need more neurons?\nA: Yes, typically one needs 3 to 4 times more binary neurons to compensate for information loss.\nQ: Do I need a softmax layer?\nA: Only for BNN training. For inference (and simpler hardware) it can be removed.\nQ: Do binary neurons have learnable biases?\nA: No, biases are redundant as normally you want to use batch normalization layers. Those layers already learn bias-equivalent parameters.\nQ: Why BNNs are sometimes called \u0026quot;integer networks\u0026quot;?\nA: Because bit count (before activation) results in an integer value.\nQ: $\\text{Sign}$ activation gradient is zero\nA: We approximate $\\text{Sign}(x)$ derivative with $\\text{Hardtanh}(x)$1 derivative, i.e. $ 1_{|x| \\le 1} $.\nQ: Are gradients binarized in backprop training?\nA: No, during the training one typically deals with real gradients and thus, real weights. Therefore, it is only in the forward pass where the network applies its binarized weights.\nQ: So what is the interest then?\nA: BNNs are interesting to develop dedicated hardware hosting pretrained networks for inference, i.e. performing only the forward pass. Think about handwriting recognition-based automated systems as a use case. Think about energy-harvesting sensors. Solar-powered smart cameras. Drones. Smart wearable devices and implants. Binarized networks facilitate smaller, faster, and more energy-efficient inference devices. Though, there is also an effort towards (on-chip) binarized networks training.\nImplementing Binarized Neural Networks in Haskell There is no better way to understand a binarized network than to create one ourselves. This easy demo is built on top of Day 4. Here are a few code highlights.\nFirst of all, the layers types we will use\ndata Layer a = -- A linear layer (BNN training) | BinarizedLinear (Matrix a) -- Batch normalization with running mean, variance, and two -- learnable affine parameters | Batchnorm1d (Vector a) (Vector a) (Vector a) (Vector a) | Activation FActivation Second, we provide the $\\text{Sign}$ activation\nsign :: Matrix Float -\u0026gt; Matrix Float sign = computeMap f where f x = if x \u0026lt;= 0 then -1 else 1 and its gradient approximation:\nsign\u0026#39; :: Matrix Float -\u0026gt; Matrix Float -\u0026gt; Matrix Float sign\u0026#39; x = compute. A.zipWith f x where f x0 dy0 = if (x0 \u0026gt; (-1)) \u0026amp;\u0026amp; (x0 \u0026lt; 1) then dy0 else 0 Finally, we accommodate the forward and backward passes of the network through the BinarizedLinear layer.\n_pass inp (BinarizedLinear w : layers) = (dX, pred, BinarizedLinearGradients dW : t) where -- Binarize the weights (!) wB = sign w -- Forward lin = maybe (error \u0026#34;Incompatible shapes\u0026#34;) compute (inp |*| wB) (dZ, pred, t) = _pass lin layers -- Backward dW = linearW\u0026#39; inp dZ -- Gradient w.r.t. wB (!) dX = linearX\u0026#39; wB dZ In the main function we define the architecture, starting with the number of neurons in each layer. Good news: on the MNIST handwriting recognition benchmark we can achieve accuracy comparable to those of a 32 bit network. However, we may need more neurons (by infl factor) in the hidden layers.\nlet infl = 4 let [i, h1, h2, o] = [784, infl * 300, infl * 50, 10] Here is the network specification. Note the BinarizedLinear layers defined above. The first BinarizedLinear has real inputs, however every subsequent one receives binary inputs coming from Sign activation.\nlet net = [ BinarizedLinear w1 , Batchnorm1d (zeros h1) (ones h1) (ones h1) (zeros h1) , Activation Sign , BinarizedLinear w2 , Batchnorm1d (zeros h2) (ones h2) (ones h2) (zeros h2) , Activation Sign , BinarizedLinear w3 -- NB this batchnorm (!) , Batchnorm1d (zeros o) (ones o) (ones o) (zeros o) ] A final technical note is that dividing by the batch variance (and rescaling by a learnable parameter gamma) in batchnorm actually makes little sense since division (multiplication) by a positive constant does not change the sign. If you would like to get your hands dirty with the code, that is a good place to optimize. Another suggestion is to transfer your newly acquired skills to a convolutional network architecture2.\nSee the complete project on Github. If you have any questions, remarks, or any typos found please send me an email. For suggestions about the code, feel free to open a new issue.\nSummary Binarized neural networks (BNNs) are valuable for low-power edge devices. Starting with energy-harvesting sensors and finishing with smart wearable medical devices and implants. Binarized networks facilitate smaller, faster, and more energy-efficient hardware.\nWe have illustrated how to train a BNN solving a handwritten digits recognition task. Our binarized network can achieve accuracy comparable to a full-precision 32 bit network provided that we moderately increase the number of binary neurons. To better grasp today's concept, train your own BNNs. Pay attention to small details. Do not forget the batch normalization before the softmax. And good luck!\nFurther Reading Binarized neural networks:\n Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1 XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks Emerging technology: Edge AI In-Memory Resistive RAM Implementation of Binarized Neural Networks for Medical Applications  MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });   $\\text{Hardtanh}(x)=\\max(-1, \\min(1,x))$ ^ In convolutional layers you may also want to increase the number convolutional filters. ^   ","date":1579671000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579671000,"objectID":"de571b239da7604883d3b5f8d38a9652","permalink":"https://penkovsky.com/neural-networks/day6/","publishdate":"2020-01-22T07:30:00+02:00","relpermalink":"/neural-networks/day6/","section":"neural-networks","summary":"Last week Apple has acquired XNOR.ai startup for amazing $200 million. The startup is known for promoting binarized neural network algorithms to save the energy and computational resources. That is definitely a way to go for mobile devices, and Apple just acknowledged that it is a great deal for them too. I feel now is a good time to explain what binarized neural networks are so that you can better appreciate their value for the industry.","tags":["Deep Learning","Haskell"],"title":"Day 6: Saving Energy with Binarized Neural Networks","type":"neural-networks"},{"authors":null,"categories":null,"content":"We aim at bringing AI from data centers to the edge. Lower energy consumption will improve battery-powered devices autonomy. Moreover, no need for data transfer over the network will ultimately enhance privacy and provide better data security.\nTo help you getting familiar with the subject, here is an introductory post on binarized neural networks.\n","date":1579644000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579644000,"objectID":"75911690c8b7280fcb40751ac6f5e947","permalink":"https://penkovsky.com/project/edge-ai/","publishdate":"2020-01-22T00:00:00+02:00","relpermalink":"/project/edge-ai/","section":"project","summary":"Bringing AI to the edge, to battery-powered devices and away from the cloud.","tags":[],"title":"Energy-efficient AI","type":"project"},{"authors":["Tifenn Hirtzlin","Marc Bocquet","Bogdan Penkovsky","Jacques-Olivier Klein","Etienne Nowak","Elisa Vianello","Jean-Michel Portal","Damien Querlioz"],"categories":null,"content":"","date":1578524400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578524400,"objectID":"2d3d068e4390099f94d7047e0f210000","permalink":"https://penkovsky.com/publication/digital-biologically-plausible-binarized-neural-networks/","publishdate":"2020-01-09T00:00:00+01:00","relpermalink":"/publication/digital-biologically-plausible-binarized-neural-networks/","section":"publication","summary":"The brain performs intelligent tasks with extremely low energy consumption. This work takes its inspiration from two strategies used by the brain to achieve this energy efficiency: the absence of separation between computing and memory functions and reliance on low-precision computation. The emergence of resistive memory technologies indeed provides an opportunity to tightly co-integrate logic and memory in hardware. In parallel, the recently proposed concept of a Binarized Neural Network, where multiplications are replaced by exclusive NOR (XNOR) logic gates, offers a way to implement artificial intelligence using very low precision computation. In this work, we therefore propose a strategy for implementing low-energy Binarized Neural Networks that employs brain-inspired concepts while retaining the energy benefits of digital electronics. We design, fabricate, and test a memory array, including periphery and sensing circuits, that is optimized for this in-memory computing scheme. Our circuit employs hafnium oxide resistive memory integrated in the back end of line of a 130-nm CMOS process, in a two-transistor, two-resistor cell, which allows the exclusive NOR operations of the neural network to be performed directly within the sense amplifiers. We show, based on extensive electrical measurements, that our design allows a reduction in the number of bit errors on the synaptic weights without the use of formal error-correcting codes. We design a whole system using this memory array. We show on standard machine learning tasks (MNIST, CIFAR-10, ImageNet, and an ECG task) that the system has inherent resilience to bit errors. We evidence that its energy consumption is attractive compared to more standard approaches and that it can use memory devices in regimes where they exhibit particularly low programming energy and high endurance. We conclude the work by discussing how it associates biologically plausible ideas with more traditional digital electronics concepts.","tags":["Deep Learning"],"title":"Digital Biologically Plausible Implementation of Binarized Neural Networks with Differential Hafnium Oxide Resistive Memory Arrays","type":"publication"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Today we will talk about one of the most important deep learning architectures, the \u0026quot;master algorithm\u0026quot; in computer vision. That is how François Chollet, author of Keras, calls convolutional neural networks (CNNs). Convolutional network is an architecture that, like other artificial neural networks, has a neuron as its core building block. It is also differentiable, so the network is conveniently trained via backpropagation. The distinctive feature of CNNs, however, is the connection topology, resulting in sparsely connected convolutional layers with neurons sharing their weights.\nFirst, we are going to build an intuition behind CNNs. Then, we are taking a close look at a classic CNN architecture. After discussing the differences between convolutional layer types, we are going to implement a convolutional network in Haskell. We will see that on handwritten digits our CNN achieves a twice lower test error, compared to the fully-connected architecture. We will build up on what we have learned during the previous days, so do not hesitate to refresh your memory first.\nPrevious posts\n Day 1: Learning Neural Networks The Hard Way Day 2: What Do Hidden Layers Do? Day 3: Haskell Guide To Neural Networks Day 4: The Importance Of Batch Normalization  Convolution operator Previously, we have learned about fully-connected neural networks. Although, theoretically those can approximate any reasonable function, they have certain limitations. One of the challenges is to achieve the translation symmetry. To explain this, let us take a look at the two cat pictures below.\n   Translation symmetry: Same object in different locations.   For us, humans, it does not matter if a cat is in the right lower corner of it is somewhere in the top part of an image. In both cases we find a cat. So we can say that our human cat detector is translation invariant.\nHowever, if we look at the architecture of a typical fully-connected network, we may realize that there is actually nothing that prevents this network to work correctly only on some part of an image. The question we ask: Is there a way to make a neural network translation invariant?\n   Fully-connected neural network with two hidden layers. Image credit: Wikimedia.   Let us take a closer look at the cat image. Soon we realize that pixels representing cat's head are more contextually related to each other than they are related to pixels representing cat's tail. Therefore, we also want to make our neural network sparse so that neurons in the next layer are connected only to the relevant neighboring pixels. This way, each neuron in the next layer would be responsible only for a small feature in the original image. The area that a neuron \u0026quot;sees\u0026quot; is called a receptive field.\n Neighboring pixels give more relevant information than distant ones.  Zoom into the cats figure.   Convolutional neural networks (CNNs) or simply ConvNets were designed to address those two issues: translation symmetry and image locality. First, let us give an intuitive explanation of a convolution operator.\nYou may not be aware, but it is very likely you have already encountered convolution filters. Recall when you have first played with a (raster) graphics editor like GIMP or Photoshop. Probably you have been delighted obtaining effects such as sharpening, blur, or edge detection. If you haven't, then you probably should :). The secret of all those filters is the convolutional application of an image kernel. The image kernel is typically a $3 \\times 3$ matrix such as below.\n A single convolution step:  Dot product between pixel values and a kernel. Image credit: GIMP.   Here is shown a single convolution step. This step is a dot product between the kernel and pixel values. Since all the kernel values except the second one in the first row are zeros, the result is equal to the second value in the first row of the green frame, i.e. $40 \\cdot 0 + 42 \\cdot 1 + 46 \\cdot 0 + \\dotsc + 58 \\cdot 0 = 42$. The convolution operator takes an image and acts within the green \u0026quot;sliding window\u0026quot;1 to perform dot product over every part of that image. The result is a new, filtered image. Mathematically, the (discrete) convolution operator $(*)$ between an image $A \\in \\mathbb{R}^{D_{F_1} \\times D_{F_2}}$ and a kernel $K \\in \\mathbb{R}^{D_{K_1} \\times D_{K_2}}$ can be formalized as \\begin{equation} A * K = \\sum_{m=0}^{D_{K_1} - 1} \\sum_{n=0}^{D_{K_2} - 1} K_{m, n} \\cdot A_{i-m, j-n}, \\end{equation} where $0 \\le i \u0026lt; D_{K_1} + D_{F_1} - 1$ and $0 \\le i \u0026lt; D_{K_2} + D_{F_2} - 1$. To better understand how convolution with a kernel changes the original image, you can play with different image kernels.\nWhat is the motivation behind the sliding window/convolution operator approach? Actually, it has a biological background. In fact, human eye has a relatively narrow visual field. We perceive objects as a whole by constantly moving eyes around them. These rapid eye movements are called saccades. Therefore, convolution operator may be regarded as a simplified model of image scanning that occurs naturally. The important point is that convolutions achieve translation invariance thanks to the sliding window method. Moreover, since every dot product result is connected - through the kernel - only to a very limited number of pixels in the initial image, convolution connections are very sparse. Therefore, by using convolutions in neural networks we achieve both translation invariance and connection sparsity. Let us see how that works in practice.\nConvolutional Neural Network Architecture  An interesting property of convolutional layers is that if the input image is shifted, the feature map output will be shifted by the same amount, but it will be left unchanged otherwise. This property is at the basis of the robustness of convolutional networks to shifts and distortions of the input.\nOnce a feature has been detected, its exact location becomes less important. Only its approximate position relative to other features is relevant.\nLecun et al. Gradient-based learning applied to document recognition (1998)\n The prototype of what we call today convolutional neural networks has been first proposed back in late 1970s by Fukushima. There were proposed many unsupervised and supervised training methods, but today CNNs are trained almost exclusively with backpropagation. Let us take a look at one of the famous ConvNet architectures known as LeNet-5.\n   LeNet-5 architecture from Lecun et al. Gradient-based learning applied to document recognition.   The architecture is very close to modern CNNs. LeNet-5 was designed to perform handwritten digit recognition from $32 \\times 32$ black and white images. The two main building blocks, as we call them now, are a feature extractor and a classifier.\n With local receptive fields neurons can extract elementary visual features such as oriented edges, endpoints, corners...\nLecun et al. Gradient-based learning applied to document recognition (1998)\n The feature extractor consists of two convolutional layers. The first convolutional layer has six convolutional filters with $5 \\times 5$ kernels. Application of those filters with subsequent bias additions and hyperbolic tangent activations2 produces feature maps, essentially new, slightly smaller ($28 \\times 28$) images. By convention, we describe the result as a volume of $28 \\times 28 \\times 6$. To reduce the spatial resolution, a subsampling is then performed3. That outputs $14 \\times 14 \\times 6$ feature maps.\n All the units in a feature map share the same set of 25 weights and the same bias, so they detect the same feature at all possible locations on the input.\nLecun et al. Gradient-based learning applied to document recognition (1998)\n The next convolutions round results already in $10 \\times 10 \\times 16$ feature maps. Note that unlike the first convolutional layer, we apply $5 \\times 5 \\times 6$ kernels. That means that each of sixteen convolutions simultaneously processes all six feature maps obtained from the previous step. After subsampling we obtain a resulting volume of $5 \\times 5 \\times 16$.\nThe classifier consists of three densely connected layers with 120, 84, and 10 neurons each. The last layer provides a one-hot-encoded4 answer. The slight difference from modern archictures is in the final layer, which consists of ten Euclidean radial basis function units, whereas today this would be a normal fully-connected layer followed by a softmax layer.\nIt is important to understand that a single convolution filter is able to detect only a single feature. For instance, it may be able to detect horizontal edges. Therefore, we use several more filters with different kernels to have get features such as vertical edges, simple textures, or corners. As you have seen, the number of filters is typically represented in ConvNet diagrams as volume. Interestingly, layers deeper in the network will combine the most basic features detected in the first layers into more abstract representations such as eyes, ears, or even complete figures. To better understand this mechanism let us inspect receptive fields visualization below.\nYour browser does not support HTML5 canvas.  Receptive field visualization derived from Arun Mallya. Hover the mouse cursor over any neuron in top layers to see how extends its receptive field in previous (bottom) layers. As we can see by checking neurons in last layers, even a small $3 \\times 3$ receptive field grows as one moves towards first layers. Indeed, we may anticipate that \u0026quot;deeper\u0026quot; neurons will have better overall view on what happens in the image.\nThe beauty and the biggest achievement of deep learning is that filter kernels are self-learned by the network5, achieving even better accuracies compared to human-engineered features. A peculiarity of convolutional layers is that the result is obtained after repetitive application of a small number of weights as defined by a kernel. Thanks to this weight sharing, convolutional layers have drastically reduced number of trainable parameters6, compared to fully-connected layers.\nConvolution Types I decided to include this section for curious readers. If this is the first time you encounter CNNs, feel free to skip the section and revisit it later.\nThere is a lot of hype around convolutions nowadays. However, it is not made clear that low-level convolutions for computer vision are often different from those exploited by ConvNets. Yet, even in ConvNets there is a variety of convolutional layers inspired by Inception ConvNet architecture and shaped by Xception and Mobilenet works. I believe that you deserve to know that there exist multiple kinds of convolutions applied in different contexts and here I shall provide a general roadmap7.\n1. Computer Vision-Style Convolution   Low-level computer vision (CV), for instance graphics editors, typically operate one, three, or four channels images (e.g. red, green, blue, and transparency). An individual kernel is typically applied to each channel. In this case, usually there are as many resulting channels as there are channels in the input image. A special case of this style convolution is when the same convolution kernel is applied to each channel (e.g. blurring). Sometimes, resulting channels are summed producing a one-channel image (e.g. edge detection).\n2. LeNet-like Convolution   A pure CV-style convolution is different from those in ConvNets due to two reasons: (1) in CV kernels are manually defined, whereas the power of neural networks comes from training, and (2) in neural networks we build a deep structure by stacking multiple convolutions on top of each other. Therefore, we need to recombine the information coming from previous layers. That allows us to train higher-level feature detections8. Finally, convolutions in neural networks may contain bias terms, i.e. constants added to results of each convolution.\nRecombination of features coming from earlier layers was previously illustrated in LeNet-5 example. As you remember, in the second convolutional layer we would apply 3D kernels of size $5 \\times 5 \\times 6$ computing dot products simultaneously on all six feature maps from the first layer. There were sixteen different kernels thus producing sixteen new channels.\n Convolution filter with three input channels.  Each pixel in the feature map is obtained as a dot product between the RGB color channels and the sliding kernel. Image credit: Wikimedia.   To summarize, a single LeNet-like convolution operates simultaneously on all input channels and produces a single channel. By having an arbitrary number of kernels, any number of output channels is obtained. It is not uncommon to operate on volumes of 512 channels! The computation cost of such convolution is $D_K \\times D_K \\times M \\times N \\times D_F \\times D_F$ where $M$ is the number of input channels, $N$ is the number of output channels, $D_K \\times D_K$ is the kernel size and $D_F \\times D_F$ is the feature map size9.\n3. Depthwise Separable Convolution   LeNet-style convolution requires a large number of operations. But do we really need all of them? For instance, can spatial and cross-channel correlations be somehow decoupled? The Xception paper largely inspired by Inception architecture shows that indeed, one can build more efficient convolutions by assuming that spatial correlations and cross-channel correlations can be mapped independently. This principle was also applied in Mobilenet architectures.\nThe depthwise separable convolution works the following way. First, like in low-level computer vision, individual kernels are applied to each individual channel. Then, after optional activation10, there is another convolution, but this time exclusively in-between channels. That is typically achieved by applying a $1 \\times 1$ convolution kernel. Finally, there is a (ReLU) activation.\nThis way, depthwise separable convolution has two distinct steps: a space-only convolution and a channel recombination. This reduces the number of operations to $D_K \\times D_K \\times M \\times D_F \\times D_F + M \\times N \\times D_F \\times D_F$9.\nTo summarize, the main difference between low-level image processing (Photoshop) and neural networks is that image processing operates on lots of pixels, but the image depth remains unchanged (three-four channels). On the other hand, convolutional neural networks tend to operate on images of moderate width and height (e.g. $230 \\times 230$ pixels), but can achieve depth of thousands of channels, while simultaneously decreasing the number of \u0026quot;pixels\u0026quot; in feature maps towards to the end of processing pipeline.\nImplementing Convolutional Networks in Haskell Today, we will implement and train a convolutional network inspired by LeNet-5. We will witness that indeed this ConvNet has about twice lower error on the MNIST handwritten digit recognition task, while being four times smaller compared to the previous model! The source code from this post is available on Github.\nConvolutional Layer Gradients First of all, we need to know how to obtain convolutional layer gradients. Although convolution in neural networks is technically performed by the cross-correlation operator11, this does not matter since kernel parameters are learnable. To understand gradient derivation, let us take a look at the cross-correlation operation in single dimension:\n$$ \\begin{equation} (X * W)_i = \\sum_{j=1}^{k} x_{i+j-1} w_j, i=1 \\dots n. \\end{equation} $$\nFor instance, if we fix 1D kernel $W$ size to be $k=3$, equation above would simply mean that vector $X \\in \\mathbb{R}^{n+2}$ produces an output $Y \\in \\mathbb{R}^{n}$: $$y_1 = x_1 w_1 + x_2 w_2 + x_3 w_3,$$ $$y_2 = x_2 w_1 + x_3 w_2 + x_4 w_3,$$ $$y_3 = x_3 w_1 + x_4 w_2 + x_5 w_3,$$ $$y_4 = x_4 w_1 + x_5 w_2 + x_6 w_3,$$ $$y_5 = x_5 w_1 + x_6 w_2 + x_7 w_3,$$ $$\\dots$$ $$y_n = x_n w_1 + x_{n+1} w_2 + x_{n+2} w_3.$$\nDenoting error from the previous layer as $\\delta$, convolutional layer gradients w.r.t. input $X$ are\n$$ \\frac{\\partial L}{\\partial x_1} = w_1 \\delta_1, $$ $$ \\frac{\\partial L}{\\partial x_2} = w_2 \\delta_1 + w_1 \\delta_2, $$ $$ \\frac{\\partial L}{\\partial x_3} = w_3 \\delta_1 + w_2 \\delta_2 + w_1 \\delta_3, $$ $$ \\frac{\\partial L}{\\partial x_4} = w_3 \\delta_2 + w_2 \\delta_3 + w_1 \\delta_4, $$ $$\\dots$$ $$ \\frac{\\partial L}{\\partial x_{n}} = w_3 \\delta_{n-2} + w_2 \\delta_{n-1} + w_1 \\delta_{n}. $$ $$ \\frac{\\partial L}{\\partial x_{n+1}} = w_3 \\delta_{n-1} + w_2 \\delta_{n}, $$ $$ \\frac{\\partial L}{\\partial x_{n+2}} = w_3 \\delta_n. $$\nSo it can be seen that it is a cross-correlation operation again, but with flipped kernel. Hence\n\\begin{equation} \\boxed{ \\frac{\\partial L}{\\partial X} = \\delta * W_{flip}. } \\end{equation}\nSimilarly, gradients w.r.t. kernel $W$ are computed as\n$$\\frac{\\partial L}{\\partial w_1} = \\delta_1 x_1 + \\delta_2 x_2 + \\dots + \\delta_n x_n,$$ $$ \\frac{\\partial L}{\\partial w_2} = \\delta_1 x_2 + \\delta_2 x_3 + \\dots + \\delta_n x_{n+1},$$ $$ \\frac{\\partial L}{\\partial w_3} = \\delta_1 x_3 + \\delta_2 x_4 + \\dots + \\delta_n x_{n+2}.$$\nThus we have yet another cross-correlation\n\\begin{equation} \\boxed{ \\frac{\\partial L}{\\partial W} = X * \\delta. } \\end{equation}\nThese formulas will become handy when implementing the backward pass from convolutional layers.\nConvolutions in Haskell First, let us start with two most relevant imports: modules from array library massiv and automatic differentiation tools from backprop.\n-- Multidimensional arrays to store learnable -- parameters and represent image data import Data.Massiv.Array hiding ( map, zip, zipWith, flatten ) import qualified Data.Massiv.Array as A -- Automatic heterogeneous back-propagation library import Numeric.Backprop Let us brush up our data structures to match our convolutional needs. First, we need to represent a batch of images. We will use the following convention: $\\text{batch size} \\times \\text{channels} \\times \\text{height} \\times \\text{width}$. For instance, if we have a batch of 16 RGB images with dimensions $32 \\times 32$, then we get a volume of $16 \\times 3 \\times 32 \\times 32$. Here we have a Volume4 type:\ntype Volume4 a = Array U Ix4 a It is just an alias to a four-dimensional unboxed Array coming from the massiv package. In deep learning frameworks those n-dimensional arrays are conventionally called tensors. Note that during transformation in a neural network, the shape of data will change, except the batch dimension that will always remain the same. Similarly, we need a way to represent our convolutional filters. In LeNet, for instance, we can represent the first convolutional layer as a volume of $6 \\times 1 \\times 5 \\times 5$. In order to be able to distinguish between parameter and data volumes, we will introduce Conv2d data structure:\ndata Conv2d a = Conv2d { _kernels :: !(Volume4 a) } Note that for the sake of simplicity we decided not to implement biases. As previously, Linear will represent fully-connected layer parameters:\ndata Linear a = Linear { _weights :: !(Matrix a) , _biases :: !(Vector a) } As usual, type parameter a means that we may later decide whether we need a Float, a Double or some other weights encoding. Now, we have everything to describe learnable parameters (weights) in LeNet:\ndata LeNet a = LeNet { _conv1 :: !(Conv2d a) , _conv2 :: !(Conv2d a) , _fc1 :: !(Linear a) , _fc2 :: !(Linear a) , _fc3 :: !(Linear a) } Now, it becomes obvious that with every new layer type managing backpropagation data structures as we did on Days 2 and 4 quickly becomes tedious. Moreover, last time when we introduced a Batchnorm1d layer, our forward-backward pass function already oversized 100 lines of code. Today, we will make use of the backprop library introduced on Day 3 to better structure our project. Thus, LeNet can be represented as a differentiable function lenet, which is nothing more than a composition of other functions aka neural network layers:\nlenet :: (Reifies s W) =\u0026gt; BVar s (LeNet Float) -\u0026gt; Volume4 Float -- ^ Batch of images -\u0026gt; BVar s (Matrix Float) lenet l = constVar -- Feature extractor -- Layer (layer group) #1 ~\u0026gt; sameConv2d (l ^^. conv1) ~\u0026gt; relu ~\u0026gt; maxpool -- Layer #2 ~\u0026gt; validConv2d (l ^^. conv2) ~\u0026gt; relu ~\u0026gt; maxpool ~\u0026gt; flatten -- Classifier -- Layer #3 ~\u0026gt; linear (l ^^. fc1) ~\u0026gt; relu -- Layer #4 ~\u0026gt; linear (l ^^. fc2) ~\u0026gt; relu -- Layer #5 ~\u0026gt; linear (l ^^. fc3) This description pretty much talks for itself. The (~\u0026gt;) operator is a function composition, i.e. f ~\u0026gt; g = \\x -\u0026gt; g(f(x)). This could be written in other forms, though. We prefer the so-called pointfree notation:\n(~\u0026gt;) :: (a -\u0026gt; b) -\u0026gt; (b -\u0026gt; c) -\u0026gt; a -\u0026gt; c f ~\u0026gt; g = g. f That simply reverses the order of composition, i.e. first is applied f, and only then g12. I find this \u0026quot;backward composition\u0026quot; notation consistent with major neural network frameworks. Compare how an equivalent network can be represented in PyTorch:\nimport torch.nn as nn class LeNet(nn.Module): def __init__(self, num_classes=10): super(LeNet, self).__init__() # Feature extractor: a (sequential) composition of convolutional, # activation, and pooling layers self.features = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2, bias=False), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0, bias=False), nn.ReLU(), nn.MaxPool2d(kernel_size=2, stride=2) ) # Classifier: a composition of linear and activation layers self.classifier = nn.Sequential( nn.Linear(16 * 5 * 5, 120, bias=True), nn.ReLU(), nn.Linear(120, 84, bias=True), nn.ReLU(), nn.Linear(84, num_classes, bias=True) ) # Composition of feature extractor and classifier def forward(self, x): x = self.features(x) # Flatten x = x.view(x.size(0), -1) x = self.classifier(x) return x Despite some noise in the Python code above13, we see that nn.Sequential object acts simply as a function composition with (~\u0026gt;) combinators. Now, let us return back to lenet signature:\nlenet :: (Reifies s W) =\u0026gt; BVar s (LeNet Float) -\u0026gt; Volume4 Float -- ^ Batch of images -\u0026gt; BVar s (Matrix Float) We have already seen the BVar s (\u0026quot;backprop variable\u0026quot;) type meaning that we deal with a differentiable structure. The first argument has type BVar s (LeNet Float). That conventionally means that LeNet is the model optimized by backpropagation and shortly we will show how to achieve that. The second function argument is a batch of images (Volume4), and finally the result is a bunch of vectors (Matrix) containing classification results performed by our model.\nFunctions constVar and (^^.) are special ones. Both are coming from the Numeric.Backprop import. The first function \u0026quot;lifts\u0026quot; a batch to be consumed by the sequence of differentiable functions. The second one (^^.) allows us to conveniently access records from our LeNet data type. For instance, sameConv2d (l ^^. conv1) means that sameConv2d has access to the _conv1 field.\nLayers known from the previous days (linear, ReLU) will be reused in accordance to backprop conventions. What is left to implement are sameConv2d, validConv2d, and maxpool layers. To be able to do that simply and efficiently we will learn a neat tool called stencils.\nFun With Stencils Let us come back to 1D convolution from Equation (2). If we look closer, we notice a pattern in convolution/cross-correlation, i.e. $$y = \\bullet w_1 + \\bullet w_2 + \\bullet w_3.$$ Indeed, every time we multiply the same set of weights and the only thing that changes is source data (masked with bullets above). Therefore, to implement convolutions we only need to define some sort of pattern that processes given points. This fixed pattern applied to array elements is called a stencil. The stencil convolution approach can be also used to perform subsampling operations, e.g. max pooling. Moreover, the method allows for efficient data parallelism.\nPreviously, we have introduced Massiv library featuring parallel arrays computation. This library was preferable to hmatrix due to two reasons: multidimensional arrays and parallel computation. Today, we have yet one more reason: stencils. Good news: massiv already implements them for us! Those are based on this work.\nFirst, we warm up with a 1D cross-correlation from Equation (2). Suppose, we have a \u0026quot;delay\u0026quot; kernel $[1, 0, 0]$. Here is how this can be applied in an interactive shell (aka REPL):\n\u0026gt; import Data.Massiv.Array \u0026gt; type Vector a = Array U Ix1 a -- Convenience alias \u0026gt; k = fromList Par [1, 0, 0] :: Vector Int \u0026gt; sten = makeCorrelationStencilFromKernel k \u0026gt; dta = fromList Par [1, 2, 5, 6, 2, -1, 3, -2] :: Vector Int \u0026gt; mapStencil (Fill 0) sten dta Array DW Par (Sz1 8) [ 0, 1, 2, 5, 6, 2, -1, 3 ] Note that the first value was replaced with zero since we have applied mapStencil (Fill 0) that uses zero-filling padding strategy to preserve the size of the array. Our array was processed to look internally like this: $[0,1,2,5,6,2,-1,3,-2,0]$, with fake zeros inserted. Alternatively, we could use applyStencil noPadding as below:\n\u0026gt; applyStencil noPadding sten dta Array DW Par (Sz1 6) [ 1, 2, 5, 6, 2, -1 ] \u0026gt; dta\u0026#39; = fromList Par [0, 1, 2, 5, 6, 2, -1, 3, -2, 0] :: Vector Int \u0026gt; applyStencil noPadding sten dta\u0026#39; Array DW Par (Sz1 8) [ 0, 1, 2, 5, 6, 2, -1, 3 ] Here is a more interesting computer vision example. For comparison, we define an identity (no transformation) stencil and a box stencil (blurring effect). The identity kernel has one in its center (coordinate 0 :. 0) and zeroes everywhere else. The box transformation computes averaged value over nine neighboring pixels including the center pixel itself (see also this for more information), therefore the kernel has weights 1/9 everywhere.\nimport Data.Massiv.Array import Data.Massiv.Array.IO import Graphics.ColorSpace identity :: Elevator a =\u0026gt; Stencil Ix2 (Pixel RGB a) (Pixel RGB a) identity = makeStencil sz c $ \\get -\u0026gt; get (0 :. 0) -- `get` is a function that receives a coordinate relative to the -- stencil\u0026#39;s center, i.e. (0 :. 0) is the center itself. where sz = Sz (3 :. 3) -- Kernel size: 3 x 3 c = 1 :. 1 -- Center coordinate {-# INLINE identity #-} box :: (Elevator a, Fractional a) =\u0026gt; Stencil Ix2 (Pixel RGB a) (Pixel RGB a) box = makeStencil sz c $ \\get -\u0026gt; ( get (-1 :. -1) + get (-1 :. 0) + get (-1 :. 1) + get (0 :. -1) + get (0 :. 0) + get (0 :. 1) + get (1 :. -1) + get (1 :. 0) + get (1 :. 1)) / 9 where sz = Sz (3 :. 3) c = 1 :. 1 {-# INLINE box #-} main :: IO () main = do frog \u0026lt;- readImageAuto \u0026#34;files/frog.jpg\u0026#34; :: IO (Image S RGB Double) -- Identity transformation writeImageAuto \u0026#34;files/frog_clone.png\u0026#34; $ computeAs S (mapStencil Edge identity frog) -- Box filtering (blur) writeImageAuto \u0026#34;files/frog_blurred.png\u0026#34; $ computeAs S (mapStencil Edge box frog) In main function, we load an image and apply those two stencils. All we need to do is to specify the stencil pattern, i.e. convolution kernel, and massiv takes care to apply it. This is what we get:\n Original frog (left) and blurred (right). Image credit.   While in the last example above we have defined stencils manually with makeStencil, in practice we may prefer to use\nmakeCorrelationStencilFromKernel :: (Manifest r ix e, Num e) =\u0026gt; Array r ix e -\u0026gt; Stencil ix e e The function has a single argument, stencil's kernel.\nConvolutional Layers With Stencils Before actually diving into implementation, we have to understand that convolution decreases the number of \u0026quot;pixels\u0026quot; in the image, similar to the \u0026quot;delayed\u0026quot; 1D kernel above. Moreover, some information next to the image border is lost due to the fact that the sliding window \u0026quot;visits\u0026quot; bordering pixels less than those closer to the center. To avoid the information loss, often the area around the image is \u0026quot;padded\u0026quot; (filled) with zeros. Thus we can perform a \u0026quot;same\u0026quot; convolution, i.e. one that does not change the convolved dimensions. As of version 0.4.3, Massiv stencils already support different padding modes. In case of LeNet $5 \\times 5$ kernels, we define padding of 2 pixels on every side (top, right, bottom, and left) using top left and bottom right corners:\nsameConv2d = conv2d (Padding (Sz2 2 2) (Sz2 2 2) (Fill 0.0)) \u0026quot;Valid\u0026quot; convolution is a convolution with no padding:\nvalidConv2d = conv2d (Padding (Sz2 0 0) (Sz2 0 0) (Fill 0.0)) In both cases, we will use generic conv2d function with a padding argument. Supporting automatic differentiation, our conv2d combines both forward and backward passes:\nconv2d :: Reifies s W =\u0026gt; Padding Ix2 Float -\u0026gt; BVar s (Conv2d Float) -\u0026gt; BVar s (Volume4 Float) -\u0026gt; BVar s (Volume4 Float) The signature tells us that it is a differential function that takes a padding, Conv2d parameters, and a batch of images Volume4 and produces another batch of images. The most efficient way to define the function is to manually specify both forward and backward passes; although it is also possible to specify only the forward pass composing smaller functions operating on BVars and letting backprop to figure out their gradients. The general pattern in backprop is to provide the forward pass \\(Conv2d w) x -\u0026gt; ... and the backward pass using a lambda expression containing previously computed gradients dz as an argument. Those forward and backward passes are glued together with liftOp2. op2 (or liftOp1. op1 for layers with no learnable parameters) as below:\nconv2d p = liftOp2. op2 $ \\(Conv2d w) x -\u0026gt; (conv2d_ p w x, \\dz -\u0026gt; let dw = conv2d\u0026#39;\u0026#39; p x dz -- ... Compute padding p1 dx = conv2d\u0026#39; p1 w dz in (Conv2d dw, dx) ) Now, forward pass is a cross-correlation between the input channels and a kernel set from the filter bank. Recall that the number of kernel sets defines the number of output channels. We simultaneously perform cross-correlation operations on all images in the batch, therefore we concatenate the results over the channel dimension 3.\nconv2d_ pad w x = res where -- Some code omitted for simplicity -- Create a stencil (\u0026#34;sliding window\u0026#34;) from the given set -- in the filter bank sten = makeCorrelationStencilFromKernel. resize\u0026#39; (Sz4 1 cin w1 w2). (w !\u0026gt;) -- For each kernel set, create and run stencils on all images in the batch. -- Finally, concatenate (append) the results over the channel dimension res = foldl\u0026#39; (\\prev ch -\u0026gt; let conv = computeAs U $ applyStencil pad4 (sten ch) x in computeAs U $ append\u0026#39; 3 prev conv) empty [0..cout - 1] To better illustrate how this works on a batch of images, here is a visualization what is going on in the first convolutional layer.\n   Folding over six filter sets in the first convolutional layer. Currently active convolutional filter set (highlighted in red) contributes to so far processed results (in color) on the right.   In MNIST we have black and white images, thus a single-channel input. During each step in foldl', a single filter is applied to all images. There are six filters hence resulting in six new channels in each convolved image on the right. A similar operation is performed in the second convolutional layer, this time resulting in 16-channel images (see also Convolution Types. LeNet-like Convolution above for the illustration on a single image). Below is a Python/NumPy equivalent of foldl'/applyStencil combo from above.\nimport numpy as np # ... Get current batch, stride, and padding parameters # ... Using those parameters, determine # output image width, height, and number of channels # Initialize resulting array res = np.zeros([len(batch), out_channels, out_im_height, out_im_width]) # Iterate over all four dimensions for i in range(len(batch)): for j in range(out_im_height): for k in range(out_im_width): for c in range(out_channels): # Define the sliding window position y1 = j * stride y2 = y1 + kernel_height x1 = k * stride x2 = x1 + kernel_width im_slice = batch[i, :, y1:y2, x1:x2] # Get current channel weights Wcur = W[:,c,:,:] # Out pixel = dot product res[i, c, j, k] = np.sum(im_slice * Wcur) return res Now that we have accomplished convolution in the forward direction, we need to compute gradients for the backward pass.\nGradients Are Also Convolutions The image plane gradients are obtained by formulas (3) and (4). However, in practice we also have to take into account that we deal with multiple images that have multiple channels. In fact, I leave it to you to figure out exact formulas. Those can be inferred from Haskell equations below. Yes, any Haskell line is already written in a mathematical notation. That is essentially what Haskell \u0026quot;purity\u0026quot; is about. You might have realized by now that programming in Haskell is all about finding your own patterns. Now, let us compute gradients w.r.t. input $\\frac{\\partial L}{\\partial X}$:\nconv2d\u0026#39; p w dz = conv2d_ p w\u0026#39; dz where w\u0026#39; = (compute. rot180. transposeInner) w -- Rotate kernels by 180 degrees. This is expressed in terms -- of tensor reversal along width and height dimensions. rot180 = reverse\u0026#39; 1. reverse\u0026#39; 2 Let us remind that these compute functions evaluate delayed arrays into actual representation in memory. Please do not hesitate to find reverse and transposeInner in massiv documentation. Finally, here we have gradients w.r.t. convolution kernel weights $\\frac{\\partial L}{\\partial W}$:\nconv2d\u0026#39;\u0026#39; :: Padding Ix2 Float -\u0026gt; Volume4 Float -\u0026gt; Volume4 Float -\u0026gt; Volume4 Float -- Iterate over images in a batch conv2d\u0026#39;\u0026#39; p x dz = res -- computeMap (/fromIntegral bs) res where -- Batch size Sz (bs :\u0026gt; _) = size x -- Code omitted res = foldl\u0026#39; (\\acc im -\u0026gt; let cur = _conv2d\u0026#39;\u0026#39; p (compute $ dzd !\u0026gt; im) (compute $ xd !\u0026gt; im) in acc + cur) base [1..bs-1] We simply accumulate gradients from individual images in the batch using left strict fold. Finally, we generalize Equation (4) to get those transformations:\n_conv2d\u0026#39;\u0026#39; :: Padding Ix2 Float -\u0026gt; Volume Float -- ^ Gradients \\delta (dz) -\u0026gt; Volume Float -- ^ X -\u0026gt; Volume4 Float -- ^ dL/dW _conv2d\u0026#39;\u0026#39; (Padding (Sz szp1) (Sz szp2) pb) dz x = ... More Stencils: Max Pooling Layers Stencils are handy not only for convolutional layers. Another use case are pooling (subsampling) layers. To be able to perform max pooling, first we have to define the corresponding stencil pattern (or just use maxStencil from the library instead).\nmaxpoolStencil2x2 :: Stencil Ix4 Float Float maxpoolStencil2x2 = makeStencil (Sz4 1 1 2 2) 0 $ \\ get -\u0026gt; let max4 x1 x2 x3 x4 = max (max (max x1 x2) x3) x4 in max4 \u0026lt;$\u0026gt; get (0 :\u0026gt; 0 :\u0026gt; 0 :. 0) \u0026lt;*\u0026gt; get (0 :\u0026gt; 0 :\u0026gt; 1 :. 1) \u0026lt;*\u0026gt; get (0 :\u0026gt; 0 :\u0026gt; 0 :. 1) \u0026lt;*\u0026gt; get (0 :\u0026gt; 0 :\u0026gt; 1 :. 0) Here max4 is a function that receives four values and computes the maximal one. This function is applied to a patch of four neighboring pixels. Do not forget that we operate in a 4D space, therefore each point in the given patch has four coordinates. Here is how we interpret a coordinate:\n(0 :\u0026gt; 0 :\u0026gt; 0 :. 0) ^ ^ ^ ^ 4 | | 1 3 2 1 and 2 are coordinates in the image plane, 3 is the channel dimension, and 4 is the batch dimension. Note the :. and :\u0026gt; operators. The first one :. constructs a coordinates (index) \u0026quot;list\u0026quot; with two elements and :\u0026gt; just adds more coordinates for higher dimensions. To learn more about \u0026lt;$\u0026gt; and \u0026lt;*\u0026gt; functions, see Applicative Functors.\nWe would like to have non-overlapping sliding windows in max pooling, therefore, we want to use $2 \\times 2$ stride in the image plane. However, we would like to perform the same operation over every channel in every image. Therefore, we use computeWithStride and the stride is 1 :\u0026gt; 1 :\u0026gt; 2 :. 2 ($\\text{batch dim} \\times \\text{channel dim} \\times \\text{height} \\times \\text{width}$). Note that we do not want any padding since the goal of pooling is actually to reduce the number of pixels: four times in this case.\nmaxpool_ :: Volume4 Float -\u0026gt; Volume4 Float maxpool_ = computeWithStride (Stride (1 :\u0026gt; 1 :\u0026gt; 2 :. 2)). applyStencil noPadding maxpoolStencil2x2 Finally, we compute gradients using pixels that had maximal values in the forward pass. And we obtain a differentiable maxpool function lifting forward-backward passes with liftOp1. op1:\nmaxpool :: Reifies s W =\u0026gt; BVar s (Volume4 Float) -\u0026gt; BVar s (Volume4 Float) maxpool = liftOp1. op1 $ \\x -\u0026gt; let out = maxpool_ x s = Stride (1 :\u0026gt; 1 :\u0026gt; 2 :. 2) outUp = computeAs U $ zoom s out maxima = A.zipWith (\\a b -\u0026gt; if a == b then 1 else 0) outUp x in (out, \\dz -\u0026gt; let dzUp = computeAs U $ zoom s dz -- Elementwise product in maybe (error $ \u0026#34;Dimensions problem\u0026#34;) compute (maxima .*. delay dzUp)) Putting It All Together First, we fetch the MNIST data and randomly generate the initial model:\nmain :: IO () main = do trainS \u0026lt;- mnistStream 64 \u0026#34;data/train-images-idx3-ubyte\u0026#34; \u0026#34;data/train-labels-idx1-ubyte\u0026#34; testS \u0026lt;- mnistStream 1000 \u0026#34;data/t10k-images-idx3-ubyte\u0026#34; \u0026#34;data/t10k-labels-idx1-ubyte\u0026#34; net \u0026lt;- randNetwork As previously, data streams are loaded from binary MNIST files; however, the initial model generation has to take into account the new structure, LeNet.\nrandNetwork :: IO (LeNet Float) randNetwork = do -- Generate a new conv layer weights: -- 6 out channels, 1 input channel, kernel size: 5 x 5 _conv1 \u0026lt;- randConv2d (Sz4 6 1 5 5) -- 16 out channels, 6 input channels, kernel size: 5 x 5 _conv2 \u0026lt;- randConv2d (Sz4 16 6 5 5) let [i, h1, h2, o] = [16 * 5 * 5, 120, 84, 10] _fc1 \u0026lt;- randLinear (Sz2 i h1) _fc2 \u0026lt;- randLinear (Sz2 h1 h2) _fc3 \u0026lt;- randLinear (Sz2 h2 o) return $ LeNet { _conv1 = _conv1 , _conv2 = _conv2 , _fc1 = _fc1 , _fc2 = _fc2 , _fc3 = _fc3 } Finally, we train our model\n-- ... `main` function net\u0026#39; \u0026lt;- train TrainSettings { _printEpochs = 1 , _lr = 0.01 , _totalEpochs = 30 } net (trainS, testS) The train function is essentially a wraper around the sgd stochastic gradient descent function.\ntrain TrainSettings { _printEpochs = printEpochs , _lr = lr , _totalEpochs = totalEpochs } net (trainS, testS) = do (net\u0026#39;, _) \u0026lt;- iterN (totalEpochs `div` printEpochs) (\\(net0, j) -\u0026gt; do net1 \u0026lt;- sgd lr printEpochs net0 trainS -- ... Compute and print accuracies ) (net, 1) return net\u0026#39; In sgd we compute gradients over a mini-batch and subtract them to optimize the model parameters.\nsgd lr n net0 dataStream = iterN n epochStep net0 where -- Fold over the stream of all batches epochStep net = S.foldl\u0026#39; _trainStep net dataStream -- Update gradients based on a single batch _trainStep net (x, targ) = trainStep lr x targ net Now, the training step is a few lines of code thanks to Numeric.Backprop.gradBP function that applies the chain rule, thus replacing the entire pass function we had before.\ntrainStep :: Float -- ^ Learning rate -\u0026gt; Volume4 Float -- ^ Images batch -\u0026gt; Matrix Float -- ^ Targets -\u0026gt; LeNet Float -- ^ Initial network -\u0026gt; LeNet Float trainStep lr !x !targ !n = n - lr * (gradBP (crossEntropyLoss x targ) n) If we ever want to actually use our model to identify a handwritten digit, we call evalBP:\nforward :: LeNet Float -\u0026gt; Volume4 Float -\u0026gt; Matrix Float forward net dta = evalBP (`lenet` dta) net Now, we compile and run our program:\n$ ./run.sh 1 Training accuracy 10.4 Validation accuracy 10.3 2 Training accuracy 85.7 Validation accuracy 86.3 3 Training accuracy 95.9 Validation accuracy 96.2 4 Training accuracy 97.6 Validation accuracy 97.4 5 Training accuracy 98.4 Validation accuracy 98.1 6 Training accuracy 98.3 Validation accuracy 98.0 7 Training accuracy 99.0 Validation accuracy 98.8 8 Training accuracy 99.0 Validation accuracy 98.7 9 Training accuracy 99.1 Validation accuracy 98.7 ... 26 Training accuracy 99.8 Validation accuracy 99.0 27 Training accuracy 99.8 Validation accuracy 98.7 28 Training accuracy 99.9 Validation accuracy 98.9 29 Training accuracy 99.7 Validation accuracy 98.6 30 Training accuracy 99.9 Validation accuracy 98.9 Thus, after 30 training epochs, we have obtained a $\\sim 1\\%$ validation error, which is about twice as low compared to the simple fully-connected architecture. See the complete project on Github.\nModel Parameters Comparison Our three-layer model from Day 4 consumed\n$$(784+1) \\cdot 300 + (300+1) \\cdot 50 + (50+1) \\cdot 10 = 251,060$$\nlearnable parameters (not counting batchnorm parameters). In contrast, the convolutional neural network model that we have implemented today requires only\n$$1\\ \\cdot 5 \\cdot5 \\cdot 6 + 16 \\cdot 6 \\cdot 5 \\cdot 5 + (400+1) \\cdot 120 + (120+1) \\cdot 84 + $$ $$ + (84+1) \\cdot 10 = 61,684~\\text{parameters.}$$\nNote also that $1\\ \\cdot 5 \\cdot5 \\cdot 6 + 16 \\cdot 6 \\cdot 5 \\cdot 5 = 2,550$ are convolutional layers parameters, whereas the majority ($59,134$) reside in classifier's fully-connected layers.\nAlthough, we did not apply batch normalization today, like was in the original LeNet, feel free to experiment with adding batchnorm layers. Please also note that there is a difference between batch normalization after convolution and after fully-connected layers (see the batchnorm article).\nCitation  @article{penkovsky2019CNN, title = \"Convolutional Neural Networks Tutorial\", author = \"Penkovsky, Bogdan\", journal = \"penkovsky.com\", year = \"2019\", month = \"November\", url = \"https://penkovsky.com/neural-networks/day5/\" }  Summary Convolutional neural networks aka ConvNets achieve translation invariance and connection sparsity. Thanks to weight sharing, ConvNets dramatically reduce the number of trained parameters. The power of ConvNet comes from training convolution filters, in contrast to manual feature engineering. In future posts, we will gain the power of GPU to face bigger challenges. We will save the planet. And we might even try to read someone's mind with convolutional networks. Stay tuned!\nAcknowledgment I would like to acknowledge Alexey Kuleshevich for his help with massiv array library, upon which this convolutional neural network was built.\nFurther reading Learned today:\n Interactive Image Kernels Excellent tutorial on ConvNets The Ancient Secrets of Computer Vision (online course) Why I prefer functional programming Efficient Parallel Stencil Convolution in Haskell  Deeper into neural networks:\n LeNet-5 paper Inception paper Xception paper Mobilenet paper  MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });    A sliding window analogy can be for example a photo scanner. ^ The actual \u0026quot;squashing\u0026quot; activation was $f(x) = 1.7159 \\tanh(\\frac{2}{3} x)$. ^ By subsampling LeNet authors mean local averaging in $2 \\times 2$ squares with subsequent scaling by a constant, bias addition, and sigmoid activation. In modern CNNs, a simple max-pooling is performed instead. ^ We have discussed one-hot encoding on Day 1. ^ To refresh your memory about backprop algorithm, check out previous days. ^ Typically, for each convolutional layer the number of parameters is equal to the number of kernel weights plus a trainable bias. For instance, for a $5 \\times 5 \\times 6$ kernel, this number is equal to 151. ^ Whereas I discuss mostly 2D convolutions that are useful for image-like objects, those convolutions can be generalized to 1D and 3D. ^ By higher-level features I mean features detected by layers deeper in the network, such as geometric shapes, eyes, ears, or even complete figures. ^ For the details, see Mobilenet paper. ^ In Xception architecture a better result was achieved without an intermediate activation (on ImageNet classification challenge). In addition, activations - when they are present - are preceded by batch normalization. Batch normalization results in no added bias term after convolutions. ^ Convolution has flipped kernel whereas cross-correlation operator does not flip the kernel. ^ Yet another equivalent definition you may encounter is (~\u0026gt;) = flip (.). While it might look puzzling at first, this line just means that (~\u0026gt;) is a composition operator (.) with flipped arguments. ^ And yet you complain about cognitive overhead in Haskell (: ^   ","date":1574612400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574612400,"objectID":"7a0f15d97956e24f54bda4a002d7dfb3","permalink":"https://penkovsky.com/neural-networks/day5/","publishdate":"2019-11-24T18:20:00+02:00","relpermalink":"/neural-networks/day5/","section":"neural-networks","summary":"Today we will talk about one of the most important deep learning architectures, the \u0026quot;master algorithm\u0026quot; in computer vision. That is how François Chollet, author of Keras, calls convolutional neural networks (CNNs). Convolutional network is an architecture that, like other artificial neural networks, has a neuron as its core building block. It is also differentiable, so the network is conveniently trained via backpropagation. The distinctive feature of CNNs, however, is the connection topology, resulting in sparsely connected convolutional layers with neurons sharing their weights.","tags":["Deep Learning","Haskell","Tutorial"],"title":"Day 5: Convolutional Neural Networks Tutorial","type":"neural-networks"},{"authors":null,"categories":null,"content":"The advent of deep learning has substantially accelerated machine learning development during the last decade impacting computer vision, natural language processing, and other domains. However, those deep learning implementations require almost two orders of magnitude more power compared to the human brain, requiring thus constant connection to dedicated data centers. With new memories available, emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation, enabling machine learning on the edge devices and avoiding data transfer over the network. In the talk we will discuss strategies to apply BNNs to biomedical signals such as electrocardiography and electroencephalography, without sacrificing accuracy and improving energy use. The ultimate goal of this research is to enable smart autonomous healthcare devices.\n","date":1569159600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569159600,"objectID":"88dc317eab3b01fa3a74a02402b66f91","permalink":"https://penkovsky.com/talk/us-french-symposium2019/","publishdate":"2019-09-22T15:40:00+02:00","relpermalink":"/talk/us-french-symposium2019/","section":"talk","summary":"The advent of deep learning has substantially accelerated machine learning development during the last decade impacting computer vision, natural language processing, and other domains. However, those deep learning implementations require almost two orders of magnitude more power compared to the human brain, requiring thus constant connection to dedicated data centers. With new memories available, emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation, enabling machine learning on the edge devices and avoiding data transfer over the network.","tags":["Deep Learning"],"title":"Medical Applications of Low Precision Neuromorphic Systems","type":"talk"},{"authors":["Bogdan Penkovsky","Xavier Porte","Maxime Jacquot","Laurent Larger","Daniel Brunner"],"categories":null,"content":"","date":1564610400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564610400,"objectID":"9fdea2e5dd304f251dd9914087868268","permalink":"https://penkovsky.com/publication/coupled-delay-systems-as-deep-convolutional-networks/","publishdate":"2019-08-01T00:00:00+02:00","relpermalink":"/publication/coupled-delay-systems-as-deep-convolutional-networks/","section":"publication","summary":"Neural networks are currently transforming the field of computer algorithms, yet their emulation on current computing substrates is highly inefficient. Reservoir computing was successfully implemented on a large variety of substrates and gave new insight in overcoming this implementation bottleneck. Despite its success, the approach lags behind the state of the art in deep learning. We therefore extend time-delay reservoirs to deep networks and demonstrate that these conceptually correspond to deep convolutional neural networks. Convolution is intrinsically realized on a substrate level by generic drive-response properties of dynamical systems. The resulting novelty is avoiding vector-matrix products between layers, which cause low efficiency in today's substrates. Compared to singleton time-delay reservoirs, our deep network achieves accuracy improvements by at least an order of magnitude in Mackey-Glass and Lorenz timeseries prediction.","tags":["Deep Learning","Reservoir Computing"],"title":"Coupled Nonlinear Delay Systems As Deep Convolutional Neural Networks","type":"publication"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Which purpose do neural networks serve for? Neural networks are learnable models. Their ultimate goal is to approach or even surpass human cognitive abilities. As Richard Sutton puts it, 'The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective'. In his essay, Sutton argues that only models without encoded human-knowledge can outperform human-centeric approaches. Indeed, neural networks are general enough and they leverage computation. Then, it is not surprising how they can exhibit millions of learnable degrees of freedom.\nThe biggest challenge with neural networks is two-fold: (1) how to train those millions of parameters, and (2) how to interpret them. Batch normalization (shortly batchnorm) was introduced as an attempt to make training more efficient. The method can dramatically reduce the number of training epochs. Moreover, batchnorm is perhaps the key ingredient that made possible training of certain architectures such as binarized neural networks. Finally, batchnorm is one of the most recent neural network advances1.\nPrevious posts\n Day 1: Learning Neural Networks The Hard Way Day 2: What Do Hidden Layers Do? Day 3: Haskell Guide To Neural Networks  The source code from this post is available on Github.\nBatch Normalization In Short What Is A Batch? Until now, we have looked on toy datasets. These were so small that they could completely fit into the memory. However, in the real world exist huge databases occupying hundreds of gigabytes of memory, such as Imagenet for example. Those often would not fit into the memory. In that case, it makes more sense to split a dataset into smaller mini-batches. During a forward/backward pass, only one batch is typically processed.\nAs the name suggests, batchnorm transformation is acting on individual batches of data. The outputs of linear layers may cause activation function saturation/'dead neurons'. For instance, in case of ReLU (rectified linear unit) $f(x)=\\max(0, x)$ activation, all negative values will result in zero activations. Therefore, it is a good idea to normalize those values by subtracting the batch mean $\\mu$. Similarly, division by standard deviation $\\sqrt{\\text{var}}$ scales the amplitudes, which is especially beneficial for sigmoid-like activations.\nTraining And Batchnorm The batch normalization procedure differs between the training and inference phases. During the training, for each layer where we want to apply batchnorm we first compute the mini-batch mean:\n$$ \\begin{equation} \\mu = \\langle \\mathbf{X} \\rangle = \\frac{1}{m} \\sum_{i=1}^m \\mathbf{X}_i, \\end{equation} $$\nwhere $\\mathbf{X}_i$ is the $i$th feature-vector coming from the previous layer; $i = 1 \\dots m$, where $m \u0026gt; 1$ is the batch size. We also obtain the mini-batch variance:\n$$ \\begin{equation} \\text{var} = \\frac{1}{m} \\sum_{i=1}^m (\\mathbf{X}_i - \\mu)^2. \\end{equation} $$\nNow, the batchnorm's heart, normalization itself:\n$$ \\begin{equation} \\hat {\\mathbf{X}_i} = \\frac{\\mathbf{X}_i - \\mu}{\\sqrt{\\text{var} + \\epsilon}}, \\end{equation} $$\nwhere a small constant $\\epsilon$ is added for numerical stability. What if normalization of the given layer was harmful? The algorithm provides two learnable parameters that in the worst case scenario can undo the effect of batch normalization: scaling parameter $\\gamma$ and shift $\\beta.$ After (optionally) applying those, we obtain the output of the batchnorm layer:\n$$ \\begin{equation} \\mathbf{Y}_i = \\gamma * \\hat {\\mathbf{X}_i} + \\beta. \\end{equation} $$\nPlease note that both mean $\\mu$ and variance $\\text{var}$ are vectors of as many elements as neurons in a given hidden layer. Operator $*$ denotes element-wise multiplication.\nInference In the inference phase it is perfectly normal to have one data sample at a time. So how to calculate batch mean if the whole batch is a single sample? To properly handle this, during training we estimate mean and variance ($E[\\mathbf{X}]$ and $\\text{Var}[\\mathbf{X}]$) over all training set. Those vectors will replace $\\mu$ and $\\text{var}$ during inference, avoiding thus the problem of normalizing a singleton batch.\nHow Efficient Is Batchnorm So far we have played with tasks that provided low-dimensional input features. Now, we are going to test neural networks on a bit more interesting challenge. We will apply our skills to automatically recognize human-written digits on a famous MNIST dataset. This challenge came from the need to have zip-code machine reading for more efficient postal services.\nWe construct two neural networks, each having two fully-connected hidden layers (300 and 50 neurons). Both networks receive $28 \\times 28 = 784$ inputs, the number of image pixels, and give back 10 outputs, the number of recognized classes (digits). As in-between layer activations we apply ReLU $f(x) = \\max(0,x)$. To obtain the classification probabilities vector in the result, we use softmax activation $\\sigma(\\mathbf{x})_i = \\frac{\\exp{x_i}}{\\sum_{j=1}\\exp{x_j}}.$ One of the networks in addition performs batch normalization before ReLUs. Then, we train those using stochastic gradient descent2 with learning rate $\\alpha = 0.01$3 and batch size $m = 100$.\n Neural network training on MNIST data.  Training with batchnorm (blue) leads to high accuracies faster than without batchnorm (orange).   From the figure above we see that the neural network with batch normalization reaches about $98\\%$ accuracy in ten epochs, whereas the other one is struggling to reach comparable performance in fifty epochs! Similar results can be obtained for other architectures.\nIt is worth mentioning that we still may lack understanding how exactly does batchnorm help. In its original paper, it was hypothesized that batchnorm is reducing internal covariate shift. Recently, it was shown that that is not necessary true. The best up-to-date explanation is that batchnorm makes optimization landscape smooth, thus making gradient descent training more efficient. This, in its turn, allows using higher learning rates than without batchnorm!\nImplementing batchnorm We will base our effort on the code previously introduced on Day 24. First, we will redefined the Layer data structure making it more granular:\ndata Layer a = -- Linear layer with weights and biases Linear (Matrix a) (Vector a) -- Same as Linear, but without biases | Linear\u0026#39; (Matrix a) -- Batchnorm with running mean, variance, and two -- learnable affine parameters | Batchnorm1d (Vector a) (Vector a) (Vector a) (Vector a) -- Usually non-linear element-wise activation | Activation FActivation Amazing! Now we can distinguish between several kinds of layers: affine (linear), activation, and batchnorm. Since batchnorm already compensates for a bias, we do not actually need biases in the subsequent linear layers. That is why we define a Linear' layer without biases. We also extend Gradients to accommodate our new layers structure:\ndata Gradients a = -- Weight and bias gradients LinearGradients (Matrix a) (Vector a) -- Weight gradients | Linear\u0026#39;Gradients (Matrix a) -- Batchnorm parameters and gradients | BN1 (Vector a) (Vector a) (Vector a) (Vector a) -- No learnable parameters | NoGrad Next, we want to extend the neural network propagation function _pass, depending on the layer. That is easy with pattern matching. Here is how we match the Batchnorm1d layer and its parameters:\n_pass inp (Batchnorm1d mu variance gamma beta:layers) = (dX, pred, BN1 batchMu batchVariance dGamma dBeta:t) where As previously, the _pass function receives an input inp and layer parameters. The second argument is the pattern we are matching against, making our algorithm specific in this case for (Batchnorm1D ...). We will also specify _pass for other kinds of Layer. Thus, we have obtained a polymorphic _pass function with respect to the layers. Finally, the equation results in a tuple of three: gradients to back propagate dX, predictions pred, and prepended list t with values BN1 computed in this layer (batch mean batchMu, variance batchVariance, and learnable parameters gradients).\nThe forward pass as illustrated in this post: -- Forward eps = 1e-12 b = br (rows inp) -- Broadcast (replicate) rows from 1 to batch size m = recip $ (fromIntegral $ rows inp) -- Step 1: mean from Equation (1) batchMu :: Vector Float batchMu = compute $ m `_scale` (_sumRows inp) -- Step 2: mean subtraction xmu :: Matrix Float xmu = compute $ inp .- b batchMu -- Step 3 sq = compute $ xmu .^ 2 -- Step 4: variance, Equation (2) batchVariance :: Vector Float batchVariance = compute $ m `_scale` (_sumRows sq) -- Step 5 sqrtvar = sqrtA $ batchVariance `addC` eps -- Step 6 ivar = compute $ A.map recip sqrtvar -- Step 7: normalize, Equation (3) xhat = xmu .* b ivar -- Step 8: rescale gammax = b gamma .* xhat -- Step 9: translate, Equation (4) out0 :: Matrix Float out0 = compute $ gammax .+ b beta\nAs discussed on Day 2, there is a recurrent call obtaining gradients from the next layer, neural network prediction pred and computed values tail t:\n(dZ, pred, t) = _pass out layers I prefer to keep the backward pass without any simplifications. That makes clear which step corresponds to which:\n-- Backward -- Step 9 dBeta = compute $ _sumRows dZ -- Step 8 dGamma = compute $ _sumRows (compute $ dZ .* xhat) dxhat :: Matrix Float dxhat = compute $ dZ .* b gamma -- Step 7 divar = _sumRows $ compute $ dxhat .* xmu dxmu1 = dxhat .* b ivar -- Step 6 dsqrtvar = (A.map (negate. recip) (sqrtvar .^ 2)) .* divar -- Step 5 dvar = 0.5 `_scale` ivar .* dsqrtvar -- Step 4 dsq = compute $ m `_scale` dvar -- Step 3 dxmu2 = 2 `_scale` xmu .* b dsq -- Step 2 dx1 = compute $ dxmu1 .+ dxmu2 dmu = A.map negate $ _sumRows dx1 -- Step 1 dx2 = b $ compute (m `_scale` dmu) dX = compute $ dx1 .+ dx2 Note that often we need to perform operations like mean subtraction $\\mathbf{X} - \\mathbf{\\mu}$, where in practice we have a matrix $\\mathbf{X}$ and a vector $\\mathbf{\\mu}$. How do you subtract a vector from a matrix? Right, you don't. You can subtract only two matrices. Libraries like Numpy may have a broadcasting magic that would implicitly convert a vector to a matrix5. This broadcasting might be useful, but might also obscure different kinds of bugs. We instead perform explicit vector to matrix transformations. For our convenience, we have a shortcut b = br (rows inp) that will expand a vector to the same number of rows as in inp. Where function br ('broadcast') is:\nbr rows\u0026#39; v = expandWithin Dim2 rows\u0026#39; const v Here is an example how br works. First, we start an interactive Haskell session and load NeuralNetwork.hs module:\n$ stack exec ghci GHCi, version 8.2.2: http://www.haskell.org/ghc/ :? for help Prelude\u0026gt; :l src/NeuralNetwork.hs Then, we test br function on a vector [1, 2, 3, 4]:\n*NeuralNetwork\u0026gt; let a = A.fromList Par [1,2,3,4] :: Vector Float *NeuralNetwork\u0026gt; a Array U Par (Sz1 4) [ 1.0, 2.0, 3.0, 4.0 ] *NeuralNetwork\u0026gt; let b = br 3 a *NeuralNetwork\u0026gt; b Array D Seq (Sz (3 :. 4)) [ [ 1.0, 2.0, 3.0, 4.0 ] , [ 1.0, 2.0, 3.0, 4.0 ] , [ 1.0, 2.0, 3.0, 4.0 ] ] As we can see, a new matrix with three identical rows has been obtained. Note that a has type Array U Seq, meaning that data are stored in an unboxed array. Whereas the result is of type Array D Seq, a so-called delayed array. This delayed array is not an actual array, but rather a promise to compute6 an array in the future. In order to obtain an actual array residing in memory, use compute:\n*NeuralNetwork\u0026gt; compute b :: Matrix Float Array U Seq (Sz (3 :. 4)) [ [ 1.0, 2.0, 3.0, 4.0 ] , [ 1.0, 2.0, 3.0, 4.0 ] , [ 1.0, 2.0, 3.0, 4.0 ] ] You will find more information about manipulating arrays in massiv documentation. Similarly to br, there exist several more convenience functions, rowsLike and colsLike. Those are useful in conjunction with _sumRows and _sumCols:\n-- | Sum values in each column and produce a delayed 1D Array _sumRows :: Matrix Float -\u0026gt; Array D Ix1 Float _sumRows = A.foldlWithin Dim2 (+) 0.0 -- | Sum values in each row and produce a delayed 1D Array _sumCols :: Matrix Float -\u0026gt; Array D Ix1 Float _sumCols = A.foldlWithin Dim1 (+) 0.0 Here is an example of _sumCols and colsLike when computing softmax activation $\\sigma(\\mathbf{x})_i = \\frac{\\exp{x_i}}{\\sum_{j=1}\\exp{x_j}}$:\nsoftmax :: Matrix Float -\u0026gt; Matrix Float softmax x = let x0 = compute $ expA x :: Matrix Float x1 = compute $ (_sumCols x0) :: Vector Float x2 = x1 `colsLike` x in (compute $ x0 ./ x2) Note that softmax is different from element-wise activations. Instead, softmax acts as a fully-connected layer that receives a vector and outputs a vector. Finally, we define our neural network with two hidden linear layers and batch normalization as:\nlet net = [ Linear\u0026#39; w1 , Batchnorm1d (zeros h1) (ones h1) (ones h1) (zeros h1) , Activation Relu , Linear\u0026#39; w2 , Batchnorm1d (zeros h2) (ones h2) (ones h2) (zeros h2) , Activation Relu , Linear\u0026#39; w3 ] The number of inputs is the total number of $28 \\times 28 = 784$ image pixels and the number of outputs is the number of classes (ten digits). We randomly generate the initial weights w1, w2, and w3. And set initial batchnorm layer parameters as follows: means to zeroes, variances to ones, scaling parameters to ones, and translation parameters to zeroes:\nlet [i, h1, h2, o] = [784, 300, 50, 10] (w1, b1) \u0026lt;- genWeights (i, h1) let ones n = A.replicate Par (Sz1 n) 1 :: Vector Float zeros n = A.replicate Par (Sz1 n) 0 :: Vector Float (w2, b2) \u0026lt;- genWeights (h1, h2) (w3, b3) \u0026lt;- genWeights (h2, o) Remember that the number of batchnorm parameters equals to the number of neurons. It is a common practice to put batch normalization before activations, however this sequence is not strict: one can put batch normalization after activations too. For comparison, we also specify a neural network with two hidden layers without batch normalization.\nlet net2 = [ Linear w1 b1 , Activation Relu , Linear w2 b2 , Activation Relu , Linear w3 b3 ] In both cases the output softmax activation is omitted as it is computed together with loss gradients in the final recursive call in _pass:\n_pass inp [] = (loss\u0026#39;, pred, []) where pred = softmax inp loss\u0026#39; = compute $ pred .- tgt Here, [] on the left-hand side signifies an empty list of input layers, and [] on the right-hand side is the empty tail of computed values in the beginning of the backward pass.\nThe complete project is available on Github. I recommend playing with different neuron networks architectures and parameters. Have fun!\nBatchnorm Pitfalls There are several potential traps when using batchnorm. First, batchnorm is different during training and during inference. That makes your implementation more complicated. Second, batchnorm may fail when training data come from different datasets. To avoid the second pitfall, it is essential to ensure that every batch represents the whole dataset, i.e. it has data coming from the same distribution as the machine learning task you are trying to solve. Read more about that.\nSummary Despite its pitfalls, batchnorm is an important concept and remains a popular method in the context of deep neural networks. Batchnorm's power is that it can substantially reduce the number of training epochs or even help achieving better neural network accuracy. After discussing this, we are prepared for hot subjects such as convolutional neural networks. Stay tuned!\nFurther Reading AI And Neural Networks  Richard Sutton. The Bitter Lesson Using neural nets to recognize handwritten digits Batch normalization paper How Does Batch Normalization Help Optimization? On The Perils of Batch Norm  Haskell  Why I think Haskell is the best general purpose language An opinionated beginner's guide to Haskell in mid 2019 Massiv Array Library Alexey's presentation about massiv  MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });   Most of the currently used neural network techniques date back to 80s. They became widely popular only now because of faster hardware and large datasets availability. The distinctively new methods are few: long short-term memory (1997), generative adversarial networks (2014), batchnorm (2015). ^ The difference of stochastic gradient descent is that it approximates the true gradient by a gradient over a mini-batch. ^ We have previously called learning rate $\\gamma$. In this post we call learning rate $\\alpha$ since $\\gamma$ is a scaling parameter in the original batch normalization paper. ^ There are some technical differences, though. Namely, we use massiv library supporting multidimensional arrays and parallelism. For the reference, here are the first two days reimplemented with massiv. Those multidimensional arrays are crucial when dealing with convolutional neural networks for image processing. ^ In general, broadcasting is about making multidimensional arrays with different shapes compatible, not only vector to matrix translation. ^ These delayed arrays are useful for computing optimizations such as fusion. ^   ","date":1561813260,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561813260,"objectID":"7557d92743f87a22fbccd197d1ff459d","permalink":"https://penkovsky.com/neural-networks/day4/","publishdate":"2019-06-29T15:01:00+02:00","relpermalink":"/neural-networks/day4/","section":"neural-networks","summary":"Which purpose do neural networks serve for? Neural networks are learnable models. Their ultimate goal is to approach or even surpass human cognitive abilities. As Richard Sutton puts it, 'The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective'. In his essay, Sutton argues that only models without encoded human-knowledge can outperform human-centeric approaches. Indeed, neural networks are general enough and they leverage computation.","tags":["Deep Learning","Haskell"],"title":"Day 4: The Importance Of Batch Normalization","type":"neural-networks"},{"authors":["Tifenn Hirtzlin","Bogdan Penkovsky","Marc Bocquet","Jacques-Olivier Klein","Jean-Michel Portal","Damien Querlioz"],"categories":null,"content":"","date":1559685600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559685600,"objectID":"257f6e2e5325432535d1c13a9043c292","permalink":"https://penkovsky.com/publication/stochastic-bnn/","publishdate":"2019-06-05T00:00:00+02:00","relpermalink":"/publication/stochastic-bnn/","section":"publication","summary":"Binarized Neural Networks, a recently discovered class of neural networks with minimal memory requirements and no reliance on multiplication, are a fantastic opportunity for the realization of compact and energy efficient inference hardware. However, such neural networks are generally not entirely binarized: their first layer remains with fixed point input. In this work, we propose a stochastic computing version of Binarized Neural Networks, where the input is also binarized. Simulations on the example of the Fashion-MNIST and CIFAR-10 datasets show that such networks can approach the performance of conventional Binarized Neural Networks. We evidence that the training procedure should be adapted for use with stochastic computing. Finally, the ASIC implementation of our scheme is investigated, in a system that closely associates logic and memory, implemented by Spin Torque Magnetoresistive Random Access Memory. This analysis shows that the stochastic computing approach can allow considerable savings with regards to conventional Binarized Neural networks in terms of area (62% area reduction on the Fashion-MNIST task). It can also allow important savings in terms of energy consumption, if we accept reasonable reduction of accuracy: for example a factor 2.1 can be saved, with the cost of 1.4% in Fashion-MNIST test accuracy. These results highlight the high potential of Binarized Neural Networks for hardware implementation, and that adapting them to hardware constrains can provide important benefits.","tags":["Deep Learning"],"title":"Stochastic Computing for Hardware Implementation of Binarized Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"","date":1557226800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557226800,"objectID":"7c54a6f6c56c48630a4a0cc8ab363931","permalink":"https://penkovsky.com/talk/ictp2019/","publishdate":"2019-05-07T13:00:00+02:00","relpermalink":"/talk/ictp2019/","section":"talk","summary":"Neural networks are transforming the field of computer algorithms, yet their emulation on current computing substrates is highly inefficient. Reservoir computing was successfully implemented on a large variety of substrates and gave new insight in overcoming this implementation bottleneck. Despite its success, the approach lags behind the state of the art in deep learning. We therefore extend time-delay reservoirs to deep networks and demonstrate that these conceptually correspond to deep convolutional neural networks. Convolution is intrinsically realized on a substrate level by generic drive-response properties of dynamical systems. The resulting novelty is avoiding vector-matrix products between layers, which cause low efficiency in today’s substrates. Compared to singleton time-delay reservoirs, our deep network achieves accuracy improvements by at least an order of magnitude in Mackey-Glass and Lorenz timeseries prediction.","tags":["Deep Learning","Reservoir Computing"],"title":"Coupled Delay Systems For Brain-Inspired Computing","type":"talk"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Now that we have seen how neural networks work, we realize that understanding of the gradients flow is essential for survival. Therefore, we will revise our strategy on the lowest level. However, as neural networks become more complicated, calculation of gradients by hand becomes a murky business. Yet, fear not young padawan, there is a way out! I am very excited that today we will finally get acquainted with automatic differentiation, an essential tool in your deep learning arsenal. This post was largely inspired by Hacker's guide to Neural Networks. For comparison, see also Python version.\nBefore jumping ahead, you may also want to check the previous posts:\n Day 1: Learning Neural Networks The Hard Way Day 2: What Do Hidden Layers Do?  The source code from this guide is available on Github. The guide is written in literate Haskell, so it can be safely compiled.\nWhy Random Local Search Fails Following Karpathy's guide, we first consider a simple multiplication circuit. Well, Haskell is not JavaScript, so the definition is pretty straightforward:\nforwardMultiplyGate = (*) Or we could have written\nforwardMultiplyGate x y = x * y to make the function look more intuitively $f(x,y) = x \\cdot y$. Anyway,\nforwardMultiplyGate (-2) 3 returns -6. Exciting.\nNow, the question: is it possible to change the input $(x,y)$ slightly in order to increase the output? One way would be to perform local random search.\n_search tweakAmount (x, y, bestOut) = do x_try \u0026lt;- (x + ). (tweakAmount *) \u0026lt;$\u0026gt; randomDouble y_try \u0026lt;- (y + ). (tweakAmount *) \u0026lt;$\u0026gt; randomDouble let out = forwardMultiplyGate x_try y_try return $ if out \u0026gt; bestOut then (x_try, y_try, out) else (x, y, bestOut) Not surprisingly, the function above represents a single iteration of a \u0026quot;for\u0026quot;-loop. What it does, it randomly selects points around initial $(x, y)$ and checks if the output has increased. If yes, then it updates the best known inputs and the maximal output. To iterate, we can use foldM :: (b -\u0026gt; a -\u0026gt; IO b) -\u0026gt; b -\u0026gt; [a] -\u0026gt; IO b. This function is convenient since we anticipate some interaction with the \u0026quot;external world\u0026quot; in the form of random numbers generation:\nlocalSearch tweakAmount (x0, y0, out0) = foldM (searchStep tweakAmount) (x0, y0, out0) [1..100] What the code essentially tells us is that we seed the algorithm with some initial values of x0, y0, and out0 and iterate from 1 till 100. The core of the algorithm is searchStep:\nsearchStep ta xyz _ = _search ta xyz which is a convenience function that glues those two pieces together. It simply ignores the iteration number and calls _search. Now, we would like to have a random number generator within the range of [-1; 1). From the documentation, we know that randomIO produces a number between 0 and 1. Therefore, we scale the value by multiplying by 2 and subtracting 1:\nrandomDouble :: IO Double randomDouble = subtract 1. (*2) \u0026lt;$\u0026gt; randomIO The \u0026lt;$\u0026gt; function is a synonym to fmap. What it essentially does is attaching the pure function subtract 1. (*2) which has type Double -\u0026gt; Double, to the \u0026quot;external world\u0026quot; action randomIO, which has type IO Double (yes, IO = input/output)1.\nA hack for a numerical minus infinity:\ninf_ = -1.0 / 0 Now, we run localSearch 0.01 (-2, 3, inf_) several times:\n(-1.7887454910045664,2.910160042416705,-5.205535653974539) (-1.7912166830200635,2.89808308735154,-5.19109477484237) (-1.8216809458018006,2.8372869694452523,-5.168631610010152) In fact, we see that the outputs have increased from -6 to about -5.2. But the improvement is only about 0.8/100 = 0.008 units per iteration. That is an extremely inefficient method. The problem with random search is that each time it attempts to change the inputs in random directions. If the algorithm makes a mistake, it has to discard the result and start again from the previously known best position. Wouldn't it be nice if instead each iteration would improve the result at least by a little bit?\nAutomatic Differentiation Instead of random search in random direction, we can make use of the precise direction and amount to change the input so that the output would improve. And that is exactly what the gradient tells us. Instead of manually computing the gradient every time, we can employ some clever algorithm. There exist multiple approaches: numerical, symbolic, and automatic differentiation. In his article, Dominic Steinitz explains the differences between them. The last approach, automatic differentiation is exactly what we need: accurate gradients with minimal overhead. Here, we will briefly explain the concept.\nThe idea behind automatic differentiation is that we explicitly define gradients only for elementary, basic operators. Then, we exploit the chain rule combining those operators into neural networks or whatever we like. That strategy will infer the necessary gradients by itself. Let us illustrate the method with an example.\nBelow we define both multiplication operator and its gradient using the chain rule, i.e. $\\frac {d} {dt} x(t) y(t) = x(t) y'(t) + x'(t) y(t)$:\n(x, x\u0026#39;) *. (y, y\u0026#39;) = (x * y, x * y\u0026#39; + x\u0026#39; * y) The same can be done with addition, subtraction, division, and exponent:\n(x, x\u0026#39;) +. (y, y\u0026#39;) = (x + y, x\u0026#39; + y\u0026#39;) x -. y = x +. (negate1 y) negate1 (x, x\u0026#39;) = (negate x, negate x\u0026#39;) (x, x\u0026#39;) /. (y, y\u0026#39;) = (x / y, (y * x\u0026#39; - x * y\u0026#39;) / y^2) exp1 (x, x\u0026#39;) = (exp x, x\u0026#39; * exp x) We also have constOp for constants:\nconstOp :: Double -\u0026gt; (Double, Double) constOp x = (x, 0.0) Finally, we can define our favourite sigmoid $\\sigma(x)$ combining the operators above:\nsigmoid1 x = constOp 1 /. (constOp 1 +. exp1 (negate1 x)) Now, let us compute a neuron $f(x, y) = \\sigma(a x + b y + c)$, where $x$ and $y$ are inputs and $a$, $b$, and $c$ are parameters\nneuron1 [a, b, c, x, y] = sigmoid1 ((a *. x) +. (b *. y) +. c) Now, we can obtain the gradient of a in the point where $a = 1$, $b = 2$, $c = -3$, $x = -1$, and $y = 3$:\nabcxy1 :: [(Double, Double)] abcxy1 = [(1, 1), (2, 0), (-3, 0), (-1, 0), (3, 0)] neuron1 abcxy1 (0.8807970779778823,-0.1049935854035065) Here, the first number is the result of the neuron's output and the second one is the gradient with respect to a ($\\frac d {da}$). Let us verify the math behind the result:\n$$ \\begin{equation} \\sigma(ax + by + c) | _{a=(a,1), b=(b,0), c=(c,0), x=(x,0), y=(y,0)} = \\\\\n\\sigma[(a, 1) (x, 0) + (b, 0) (y, 0) + (c, 0)] = \\\\\n\\sigma[(ax, a \\cdot 0 + 1 \\cdot x) + (by, 0 \\cdot b + 0 \\cdot y) + (c, 0)] = \\\\\n\\sigma[(ax + by + c, x)] = \\\\\n\\frac {(1, 0)} {(1, 0) + \\exp \\left[ -(ax + by + c, x) \\right]} = \\\\\n\\frac {(1, 0)} {(1, 0) + \\exp \\left[ -ax - by - c, -x) \\right]} = \\\\\n\\frac {(1, 0)} {(1, 0) + (\\exp (-ax - by - c), -x \\exp (-ax - by - c))} = \\\\\n\\frac {(1, 0)} {(1 + \\exp(-ax - by - c), -x \\exp(-ax - by - c))} = \\\\\n\\left( \\sigma(ax + by + c), \\frac {x \\exp(-ax - by -c)} {(1 + \\exp(-ax - by -c))^2} \\right). \\end{equation} $$\nThe first expression is the result of neuron's computation and the second one is the exact analytic expression for $\\frac d {da}$. That is all the magic behind automatic differentiation! In a similar way, we can obtain the rest of the gradients:\nneuron1 [(1, 0), (2, 1), (-3, 0), (-1, 0), (3, 0)] (0.8807970779778823,0.3149807562105195) neuron1 [(1, 0), (2, 0), (-3, 1), (-1, 0), (3, 0)] (0.8807970779778823,0.1049935854035065) neuron1 [(1, 0), (2, 0), (-3, 0), (-1, 1), (3, 0)] (0.8807970779778823,0.1049935854035065) neuron1 [(1, 0), (2, 0), (-3, 0), (-1, 0), (3, 1)] (0.8807970779778823,0.209987170807013) Introducing backprop library The backprop library was specifically designed for differentiable programming. It provides combinators to reduce our mental overhead. In addition, the most useful operations such as arithmetics and trigonometry, have already been defined in the library. See also hmatrix-backprop for linear algebra. So all you need for differentiable programming now is to define some functions:\nneuron :: Reifies s W =\u0026gt; [BVar s Double] -\u0026gt; BVar s Double neuron [a, b, c, x, y] = sigmoid (a * x + b * y + c) sigmoid x = 1 / (1 + exp (-x)) Here BVar s wrapper signifies that our function is differentiable. Now, the forward pass is:\nforwardNeuron = BP.evalBP (neuron. BP.sequenceVar) We use sequenceVar isomorphism to convert a BVar of a list into a list of BVars, as required by our neuron equation. And the backward pass is\nbackwardNeuron = BP.gradBP (neuron. BP.sequenceVar) abcxy0 :: [Double] abcxy0 = [1, 2, (-3), (-1), 3] forwardNeuron abcxy0 -- 0.8807970779778823 backwardNeuron abcxy0 -- [-0.1049935854035065,0.3149807562105195,0.1049935854035065,0.1049935854035065,0.209987170807013] Note that all the gradients are in one list, the type of the first neuron argument.\nSummary Modern neural networks tend to be complex beasts. Writing backpropagation gradients by hand can easily become a tedious task. In this post we have seen how automatic differentiation can face this problem.\nIn the next posts we will apply automatic differentiation to real neural networks. We will talk about batch normalization, another crucial method in modern deep learning. And we will ramp it up to convolutional networks allowing us to solve some interesting challenges. Stay tuned!\nFurther reading  Visual guide to neural networks Backprop documentation Article on backpropagation by Dominic Steinitz   In fact, 64 bit double precision is not necessary for neural networks, if not an overkill. In practice you would prefer to use a 32 bit Float type. ^   ","date":1550358600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1550358600,"objectID":"0ecadba98bd5176dbaba0c2a409d8991","permalink":"https://penkovsky.com/neural-networks/day3/","publishdate":"2019-02-17T01:10:00+02:00","relpermalink":"/neural-networks/day3/","section":"neural-networks","summary":"Now that we have seen how neural networks work, we realize that understanding of the gradients flow is essential for survival. Therefore, we will revise our strategy on the lowest level. However, as neural networks become more complicated, calculation of gradients by hand becomes a murky business. Yet, fear not young padawan, there is a way out! I am very excited that today we will finally get acquainted with automatic differentiation, an essential tool in your deep learning arsenal.","tags":["Deep Learning","Haskell"],"title":"Day 3: Haskell Guide To Neural Networks","type":"neural-networks"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" In the previous article, we have introduced the concept of learning in a single-layer neural network. Today, we will learn about the benefits of multi-layer neural networks, how to properly design and train them.\nSometimes I discuss neural networks with students who have just started discovering machine learning techniques:\n\u0026quot;I have built a handwritten digits recognition network. But my accuracy is only Y.\u0026quot;\n\u0026quot;It seems to be much less than state-of-the-art\u0026quot;, I contemplate.\n\u0026quot;Indeed. Maybe the reason is X?\u0026quot;\nUsually X is not the reason. The real cause appears to be much more trivial: instead of a multi-layer neural network the student has built a single-layer neural network or its equivalent. This network acts as a linear classifier, therefore it cannot learn non-linear relations between the input and the desired output.\nSo what is a multi-layer neural network and how to avoid the linear classifier trap?\nMultilayer neural networks Multilayer neural network can be formalized as:\n$$ \\begin{equation} \\mathbf{y} = f_N \\left( \\mathbf{W}_N \\cdot \\left( \\dots f_2 \\left( \\mathbf{W}_2 \\cdot f_1( \\mathbf{W}_1 \\cdot \\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2 \\right) \\dots \\right) + \\mathbf{b}_N \\right), \\end{equation} $$\nwhere $\\mathbf{x}$ and $\\mathbf{y}$ is an input and output vectors respectively, $\\mathbf{W}_i, \\mathbf{b}_i, i=1..N$ are weight matrices and bias vectors, and $f_i, i=1..N$ are activations functions in the $i$th layer. The nonlinear activation functions $f_i$ are applied elementwise.\nFor the sake of illustration, today we will employ two-dimensional datasets as shown below.\n  Both datasets feature two classes depicted by orange and blue dots, respectively. The goal of the neural network training will be to learn how to prescribe a class to a new dot by knowing its coordinates $(x_1, x_2)$. It turns out these simple tasks cannot be solved with a single-layer architecture, as it is only capable of drawing straight lines in the input space $(x_1, x_2)$. Let us bring in an additional layer.\nTwo-layer network  Two-layer neural network.  Inputs $x_1$ and $x_2$ are multiplied by a weights matrix $W_1$, then activation function $f_1$ is applied element-wise. Finally, the data are transformed by $W_2$ followed by another activation $f_2$ (not depicted) to obtain the output $y$.   The figure above shows the single hidden layer network architecture, which we will interchangeably call a two-layer network:\n$$ y = f_2 \\left( \\mathbf{W}_2 \\cdot f_1 ( \\mathbf{W}_1 \\cdot \\mathbf{x} + \\mathbf{b}_1 ) + \\mathbf{b}_2 \\right), $$\nwhere $f_1(x) = \\max(0, x)$ also known as ReLU will be the first activation function and sigmoid $f_2(x) = \\sigma(x) = [1 + e^{-x}]^{-1}$ will be the second activation function, which you already have seen in the previous article. The particularity of sigmoid is squashing its inputs between 0 and 1. This output tends to be useful in our case where we only have two classes: blue dots denoted by 0 and orange dots, by 1. As a loss function to minimize during neural network training, we will use binary cross-entropy:\n$$ L(y, \\hat y) = - \\frac 1 m \\sum_{i=1}^m \\left[ y_i \\ln(\\hat y_i) + (1 - y_i) \\ln(1 - \\hat y_i) \\right], $$\nwhere $y$ is the target output and $\\hat y$ is neural network's prediction. This loss function was specifically designed to efficiently train neural networks. Another perk of cross-entropy is that in conjunction with sigmoid activation, the error gradient is simply a difference between predicted and desired outputs.\nHow To Initialize? The first mistake may creep in when you completely ignore the importance of the neural network weight initialization. In essence, you want to create neurons capable of learning different features. Therefore, you should select neural network weights randomly.\n The circles problem.  Decision boundaries for neural networks trained using the gradient descent method and initialized with (1) zero weights, (2) identical weights, and (3) random weights.   Above we can see the decision boundaries for three cases of initialization. First, when the weights are initialized as zeroes, the network cannot learn anything. In the second case, the weights are initialized as non-zero, but identical values. Even though the network becomes able to learn something, there is no distinction between individual neurons. Therefore, the entire network behaves like one big neuron. Finally, when the neural network is initialized with random weights, it becomes capable to learn.\nHow To Train? The universal approximation theorem claims that under certain assumptions, a single hidden layer architecture should be sufficient to approximate any reasonable function. However, it indicates nothing about how to obtain or how difficult is to obtain those neural network weights1. As we have seen from weights initialization example, using simple gradient descent training our network was able to distinguish between the two circles. Now, how about a more difficult example: spirals, which is hard due to its extreme nonlinearity. Check out the animation below, on the left, to see how a naive gradient descent training can get easily stuck in a suboptimal setting.\n The spirals problem.  Both neural networks have identical single hidden layer architecture with 512 neurons. The first one is trained with a naive gradient descent (GD), the second one is trained with Adam.   The network fails to properly distinguish between the two classes. One may suggest that a more adequate neural network architecture is needed. That is partially true. However, the mentioned above theorem hints that it might be not necessary.\nIt turns out a lot of research in neural networks and deep learning is exactly about how to obtain those set of weights, that is about neural network training algorithms. The disadvantage of the naive gradient descent is that the algorithm often gets stuck far from an acceptable solution. One of the ways to alleviate this issue is to introduce momentum. Perhaps, the most popular among the algorithms with momentum is Adam. As we can see from the diagram above, on the right, the Adam algorithm is able to efficiently build the appropriate decision boundary. I find it can be more illustrative if we visualize the neural network's output during its training.\n The spirals problem.  The output of two trained models before thresholding. One may see that the one trained with gradient descent (GD) is practically stuck in a suboptimal condition, whereas Adam algorithm efficiently leads to the separation between two classes.   This way we see the faster convergence of Adam on a more detailed level. If you want to learn more about different algorithms from the gradient descent family, here there is a nice comparison.\nAvoiding The Linear Classifier Pitfall Now, what if by mistake we have provided linear activations. What happens? If activations $f_i$ were linear, then the model in formula (2) would reduce to a single-layer neural network:\n$$ \\mathbf{y} = c_N \\cdot W_N \\cdot \\dots \\cdot c_2 \\cdot W_2 \\cdot c_1 \\cdot W_1 \\cdot \\mathbf{x} = W \\mathbf{x}, $$\nwhere $c_i$ denote constants. Thus, one obtains a linear classifier $W \\mathbf{x}$. This can be seen from the resulting decision boundaries.\n Two-layer network with linear activation $f_1(x) = x$.   We have used exactly the same architecture as above, except the activation $f_1(x) = x$. Now we see that the two-layer neural network acts as a simple single-layer one: it can only divide the input space using a straight line. To avoid this issue, all activation functions must remain nonlinear.\nHow Many Layers? We have solved the spirals problem with a single hidden layer network. Why add more layers? To address this question, let us animate the decision boundaries during neural network training for three different architectures. All the networks have ReLU activations in non-terminal layers and sigmoid in the final layer. As can be seen from the image below, the network with three hidden layers and with fewer neurons in each layer, learns faster. The reason can be understood intuitively. A neural network performs topological transformations of the input space. Composing several simple transformations leads to a more complex one. Therefore, for such non-linearly separated classes as in the spirals problem one benefits from adding more layers.\n Neural network training convergence (Adam).  Architectures: (1) single hidden layer with 128 neurons (513 total parameters), (2) single hidden layer, 512 neurons (2049 parameters), (3) three hidden layers with 40, 25, and 10 neurons (1416 parameters). The fastest convergence is demonstrated by the last architecture.   From the figure we may conclude that more neurons are not always the best option. The most contrasting border indicating the highest confidence in the least number of steps is achieved with the three hidden layers neural network. This network has 1416 parameters counting both weights and biases. This number is about 30 percent less than the second architecture with a single hidden layer. Indeed, one may need to carefully pick the architecture taking into consideration different factors such as dataset size, variance, input dimensionality, kind of the task, and others. With more layers, neural network can more efficiently represent complex relationships. On the other hand, if the number of layers is very large, one may need to apply different tricks in order to avoid such problems as fading gradients.\nGiven all the new information, how about experimenting with neural networks? In the rest of this post we will learn how to build multilayer networks in Haskell.\nImplementing Backpropagation In the previous article, we have implemented backpropagation (shortly backprop) for a single-layer (no hidden layer) network. This method easily extends towards the case of multiple layers. Here is a nice video to provide you with more technical details. Below, we remind how backprop works for each individual neuron.\n Backprop for a neuron. A neuron's computation $y = f(\\sum_i w_i \\cdot x_i)$ known as the forward pass is illustrated in black using a computational graph. The backward pass propagating the gradients in the opposite direction is depicted in red.   If you have ever found any other tutorial on backprop, chances are you will first implement the so-called forward pass and then, as a separate function, the backward pass. The forward pass calculates the neural network output, whereas the backward pass does the gradients bookkeeping. Last time we have provided an example of a forward pass in a single-layer architecture:\nforward x w = let h = x LA.\u0026lt;\u0026gt; w y = sigmoid h in [h, y] where h = x LA.\u0026lt;\u0026gt; w calculates $\\mathbf{h} = \\mathbf{x \\cdot W^{\\intercal}}$2 and y = sigmoid h is an elementwise activation $\\mathbf{y} = \\sigma(\\mathbf{h})$. Here we have provided both the result of neural network computation y and an intermediate value h, which was subsequently used in the backward pass to compute the weights gradient dW:\ndE = loss\u0026#39; y y_target dY = sigmoid\u0026#39; h dE dW = linear\u0026#39; x dY If there are multiple layers, the two passes (forward and backward) are typically computing all the intermediate results $\\mathbf{h}_i = \\mathbf{x}_{i - 1} \\mathbf{W}_i^{\\intercal} + \\mathbf{b}_i$ and $\\mathbf{y}_i = f(\\mathbf{x}_i)$. Then, these intermediate results are used in reverse (backward) order to compute weight gradients dW.\nThe strategy of storing intermediate results is typical for many tutorials in imperative languages (looking at you, Python). In fact, in functional languages you can do the same thing. The disadvantage of separate forward and backward implementations is that it becomes tedious to keep track of all the intermediate results and may require more mental effort to implement the backprop. I also feel that such backprop explanation is not very intuitive. The whole pattern of forward and backward passes may seem strangely familiar. Hey, that is actually recursion! Recursion is bread and butter of functional languages, so why not apply it to the backprop? Instead of defining two separate methods and manually managing all the intermediate results, we will define both in one place.\nTo begin our new multilayer networks project, we start with data structures. First, we define a data type that will represent a single layer.\n-- Neural network layer: weights, biases, and activation data Layer a = Layer (Matrix a) (Matrix a) Activation where the first Matrix a stands for $\\mathbf{W}^{\\intercal} \\in \\mathbb{R}^{\\text{inp} \\times \\text{out}}$2, the second Matrix a means $\\mathbf{b} \\in \\mathbb{R}^{\\text{1} \\times \\text{out}}$, and Activation is an algebraic data type, which in practice denotes known activation functions in a symbolic fashion:\n-- Activation function: -- * Rectified linear unit (ReLU) -- * Sigmoid -- * Hyperbolic tangent -- * Identity (no activation) data Activation = Relu | Sigmoid | Tanh | Id Then, the neural network from Equation (2) can be serialized as a list of Layers3. This list will bear the type of\n[Layer Double] indicating that each element in the list has to be a Layer operating on real numbers (Double).\nNow, we can easily update any Layer in a single gradient descent step\nf :: Layer Double -\u0026gt; Gradients Double -\u0026gt; Layer Double f (Layer w b act) (Gradients dW dB) = Layer (w - lr `scale` dW) (b - lr `scale` dB) act The signature f :: Layer Double -\u0026gt; Gradients Double -\u0026gt; Layer Double means nothing else but: f is a function that can take an old layer and gradients and produce a new layer. Here, lr is the learning rate and scale is a multiplication by a constant written in a fancy infix way4. We also have to define the Gradients data type:\ndata Gradients a = Gradients (Matrix a) (Matrix a) To update all layers in functional programming we would need an analog to the for loop construct known from imperative programming languages. Great news: there are different for-loops for different needs: map, zip, zipWith, foldl, scanr, etc. And zipWith is exactly what we need. Its polymorphic type\nzipWith :: (a -\u0026gt; b -\u0026gt; c) -\u0026gt; [a] -\u0026gt; [b] -\u0026gt; [c] in our context can be interpreted as\nzipWith :: (Layer a -\u0026gt; Gradients a -\u0026gt; Layer a) -\u0026gt; [Layer a] -\u0026gt; [Gradients a] -\u0026gt; [Layer a] where (Layer a -\u0026gt; Gradients a -\u0026gt; Layer a) denotes a function which takes a Layer and Gradients and produces a new Layer. This is the signature of f defined above with the only precision that the type a is Double. Therefore, zipWith can be interpreted as: take a function f from above, take a list of layers and a list of gradients, then apply f to each Layer in the first list and each Gradient in the second list. Obtain a new list of Layers. The new list of layers [Layer] is our modified neural network after a single gradient descent step. Now, the complete gradient descent implementation is\noptimize lr iterN net0 dataSet = last $ take iterN (iterate step net0) where step net = zipWith f net dW where (_, dW) = pass net dataSet f :: Layer Double -\u0026gt; Gradients Double -\u0026gt; Layer Double f (Layer w b act) (Gradients dW dB) = Layer (w - lr `scale` dW) (b - lr `scale` dB) act We now see how well previously defined f fits in. The resulting function optimize should look familiar from the previous post. Back then, it was called descend. Now, we generalize it to operate on a list of layers, instead of a single layer weights. Finally, we provide the dataSet as an argument. This last interface change is a matter of convenience.\nNow, as promised, we implement the backprop algorithm in a single pass.\n80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101  pass net (x, tgt) = (pred, grads) where (_, pred, grads) = _pass x net _pass inp [] = (loss\u0026#39;, pred, []) where pred = sigmoid inp -- Gradient of cross-entropy loss -- after sigmoid activation loss\u0026#39; = pred - tgt _pass inp (Layer w b sact:layers) = (dX, pred, Gradients dW dB:t) where lin = (inp LA.\u0026lt;\u0026gt; w) + b y = getActivation sact lin (dZ, pred, t) = _pass y layers dY = getActivation\u0026#39; sact lin dZ dW = linearW\u0026#39; inp dY dB = bias\u0026#39; dY dX = linearX\u0026#39; w dY   Let us break down what the code above does. First, the inputs are an initial neural network net and training data with corresponding targets (x, tgt). The function will provide inference result and gradients (pred, grads) as the computation result. Line 82 launches the _pass function which accepts two arguments: initial input x and neural network net.\n91 92 93 94 95 96 97 98 99 100 101  _pass inp (Layer w b sact:layers) = (dX, pred, Gradients dW dB:t) where lin = (inp LA.\u0026lt;\u0026gt; w) + b y = getActivation sact lin (dZ, pred, t) = _pass y layers  dY = getActivation\u0026#39; sact lin dZ dW = linearW\u0026#39; inp dY dB = bias\u0026#39; dY dX = linearX\u0026#39; w dY   This piece of code above performs computations with respect to the current layer. First, we perform pattern matching selecting current layer weights and biases w b and activation sact and also we get the tail of the layers list layers on line 91. Lines 93-94 perform the forward pass, i.e. the $f (\\mathbf{W} \\mathbf{x} + \\mathbf{b})$ operation. Line 96 (highlighted) launches a recursive call to _pass providing the arguments of current layer computation y and the rest of the tail of the network layers. As a result of the recursive call we obtain gradient coming back from the layer dZ, prediction result pred, and partially computed weights gradients list t. Lines 98-101 compute new gradients, which _pass returns to its previous caller.\nNow, what happens when we finish our forward propagation? Lines 84-89 below contain the stop condition for the recursive call. The empty list [] on line 84 signifies that we have reached the end of the layers list. Now, we compute the network prediction: We apply sigmoid to squash the outputs between 0 and 1 on line 86. This, together with a binary cross-entropy loss function, will allow for a very simple gradient in the final layer: loss' = pred - tgt on line 89 (highlighted).\n84 85 86 87 88 89  _pass inp [] = (loss\u0026#39;, pred, []) where pred = sigmoid inp -- We calculate the gradient of cross-entropy loss -- after sigmoid activation. loss\u0026#39; = pred - tgt   The function _pass provides three results: the loss gradient loss', the predicted result pred, and an empty list [] to accumulate the weight gradients in the backward pass (lines 96-101). Note that since sigmoid activation has been already calculated on line 86, the final layer must have no activation, i.e. we will specify the identity transformation Id of the final layer, when defining the network architecture.\nNow, the question: how can we calculate the forward pass separately, i.e. without the need to compute all the gradients? It turns out that Haskell has lazy evaluation: it computes only what we specifically ask for. In the forward implementation below, gradients are never asked, and therefore never computed.\nforward net dta = fst $ pass net (dta, undefined) Normally, undefined will return an error when encountered. However, since we never calculate the gradients, the undefined target is never evaluated.\nPutting It All Together Now, we have all the necessary components to create a multilayer neural network. First, we generate our data. Then, we define the architecture and initialize the actual neural networks. Finally, we run our experiments.\nTrain And Validation Datasets We generate a random circles dataset for our experiments. We will provide the coordinate pairs $(x_1,x_2)$ as well as target class labels $y$.\nmakeCircles :: Int -\u0026gt; Double -\u0026gt; Double -\u0026gt; IO (Matrix Double, Matrix Double) makeCircles m factor noise = do -- Code omitted return (x, y) In general, it is a good idea to have at least two separate machine learning datasets: one used for training and another one for model validation. Therefore, we will create two of those with randomly generated samples.\ntrainSet \u0026lt;- makeCircles 200 0.6 0.1 testSet \u0026lt;- makeCircles 100 0.6 0.1 We create the trainSet with 200 and the testSet with 100 data samples. 0.6 and 0.1 are model parameters, for the full implementation feel free to check Main.hs. In a similar way we will create training and validation datasets for the spirals problem.\nDefining The Architecture Below we define an architecture for a single hidden layer network.\n-- Initializing a single hidden layer network let (nIn, hidden, nOut) = (2, 512, 1) -- Generate weights/biases W1 and W2 (w1_rand, b1_rand) \u0026lt;- genWeights (nIn, hidden) (w2_rand, b2_rand) \u0026lt;- genWeights (hidden, nOut) let net = [ Layer w1_rand b1_rand Relu , Layer w2_rand b2_rand Id ] Note that the activation of the final layer is calculated separately, therefore we leave no activation Id. w*_rand and b*_rand are randomly generated weights and biases. For convenience we define a genNetwork function which will combine the operations from above. This is how it is used:\nnet \u0026lt;- genNetwork [2, 512, 1] [Relu, Id] Initializing Weights We remember that for the greatest benefit of a neural network, the weights should be initialized randomly. Here we employ a common initialization strategy from this paper.\n-- Generate new random weights and biases genWeights :: (Int, Int) -\u0026gt; IO (Matrix Double, Matrix Double) genWeights (nin, nout) = do w \u0026lt;- _genWeights (nin, nout) b \u0026lt;- _genWeights (1, nout) return (w, b) where _genWeights (nin, nout) = do let k = sqrt (1.0 / fromIntegral nin) w \u0026lt;- randn nin nout return (k `scale` w) This strategy becomes more useful as soon as your neural network becomes \u0026quot;deeper\u0026quot;, i.e. gets more and more layers.\nAdam Previously we have seen that naive gradient descent may easily get stuck in a suboptimal setting. Let us implement Adam optimization strategy, explained in this video.\nadam p dataSet iterN w0 = w where -- Some code omitted (_, dW) = pass dataSet w loss\u0026#39; sN = zipWith f2 s dW vN = zipWith f3 v dW wN = zipWith3 f w vN sN f :: Layer Double -\u0026gt; (Matrix Double, Matrix Double) -\u0026gt; (Matrix Double, Matrix Double) -\u0026gt; Layer Double f (w_, b_, sf) (vW, vB) (sW, sB) = ( w_ - lr `scale` vW / ((sqrt sW) `addC` epsilon) , b_ - lr `scale` vB / ((sqrt sB) `addC` epsilon) , sf) addC m c = cmap (+ c) m f2 :: (Matrix Double, Matrix Double) -\u0026gt; Gradients Double -\u0026gt; (Matrix Double, Matrix Double) f2 (sW, sB) (dW, dB) = ( beta2 `scale` sW + (1 - beta2) `scale` (dW^2) , beta2 `scale` sB + (1 - beta2) `scale` (dB^2)) f3 :: (Matrix Double, Matrix Double) -\u0026gt; Gradients Double -\u0026gt; (Matrix Double, Matrix Double) f3 (vW, vB) (dW, dB) = ( beta1 `scale` vW + (1 - beta1) `scale` dW , beta1 `scale` vB + (1 - beta1) `scale` dB) The Adam implementation is a derived from optimize. The most prominent change is additional \u0026quot;zipping\u0026quot; with two new functions: f2 embodies the RMSprop strategy and f3 performs momentum estimation.\nRunning the experiments Now, we shall train our network:\nlet epochs = 1000 lr = 0.001 -- Learning rate net\u0026#39; = optimize lr epochs net trainSet netA = optimizeAdam adamParams epochs net trainSet It is time to compile and run our networks. Please check the output below. The complete project is available on Github. Feel free to play with neural network architecture and initialization strategies.\n$ stack --resolver lts-10.6 --install-ghc ghc \\ --package hmatrix-0.18.2.0 \\ --package hmatrix-morpheus-0.1.1.2 \\ -- -O2 Main.hs $ ./Main Circles problem, 1 hidden layer of 128 neurons, 1000 epochs --- Training accuracy (gradient descent) 76.5 Validation accuracy (gradient descent) 74.0 Training accuracy (Adam) 100.0 Validation accuracy (Adam) 96.0 Spirals problem, Adam, 700 epochs --- 1 hidden layer, 128 neurons (513 parameters) Training accuracy 77.5 Validation accuracy 72.0 1 hidden layer, 512 neurons (2049 parameters) Training accuracy 97.2 Validation accuracy 97.0 3 hidden layers, 40, 25, and 10 neurons (1416 parameters) Training accuracy 100.0 Validation accuracy 100.0 The first experiment shows the advantage of Adam over a naive gradient descent training algorithm on a simple circles dataset. The second experiment compares three architectures solving the spirals challenge. I had to reduce the number of training epochs to 700 to show which architecture will converge first.\nSummary When designing and training neural networks one should keep in mind several simple heuristics.\n Initialize weights randomly. Keep activation functions nonlinear. Remember, a superposition of linear operators is a linear operator. Use more layers and more neurons if the application appears to be complex. Reduce the number of layers in the opposite case. Apply training strategies with momentum instead of naive gradient descent.  The universal approximation theorem claims that a single hidden layer neural network architecture should be sufficient to approximate any reasonable map. However, how to obtain the weights is a much harder question. For instance, one of the first networks solving the tough spirals challenge with backpropagation had not one, but three hidden layers. Indeed, we have seen that due to the highly nonlinear relationships in the dataset, a three-layer network with fewer parameters converges faster than the one with a single hidden layer.\nIn the next posts we will take a look on automatic differentiation and regularization techniques. We will also discuss how Haskell compiler can help ensure that our neural network is correct. And we will ramp it up to convolutional networks allowing us to solve some interesting challenges. Stay tuned.\nI would like to dedicate this article to Maxence Ernoult for his early feedback and constructive criticism. I would also like to thank Elena Prokopets for proofreading the article.\nCitation  @article{penkovsky2019, title = \"What Do Hidden Layers Do?\", author = \"Penkovsky, Bogdan\", journal = \"penkovsky.com\", year = \"2019\", month = \"February\", url = \"https://penkovsky.com/neural-networks/day2/\" }  Further Reading Haskell  A great start to learning Haskell Try Haskell tutorial Another example NN in Haskell Yet one more  Neural Networks And Backpropagation  A brief tutorial on backprop More intuition on backprop and slides Derivation of gradients Another tutorial on derivatives Notes on softmax and crossentropy A more or less complete introduction to deep learning More about the spirals challenge  MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });   Such a cheeky theorem! ^ The attentive reader may have noticed that in the code we apply $\\mathbf{x W^{\\intercal}}$ rather than $\\mathbf{W x}$, which is a matter of convenience. In many libraries you will find this transposition is done in order to keep data samples index in 0-th dimension (e.g. matrix rows). Also we have introduced biases $\\mathbf{b}$ as in $\\mathbf{W x} + \\mathbf{b}$ or $\\mathbf{x W^{\\intercal}} + \\mathbf{b}$, which were last time omitted for brevity. ^ For now, there is nothing that prevents us from mismatching matrix dimensions, however, we will revisit this issue in the future. ^ To put simply, a `f` b is the same as f a b, or $f(a,b)$. ^   ","date":1549293300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549293300,"objectID":"fd4bb37ffc24d85b22aed36f24bc1c0e","permalink":"https://penkovsky.com/neural-networks/day2/","publishdate":"2019-02-04T17:15:00+02:00","relpermalink":"/neural-networks/day2/","section":"neural-networks","summary":"In the previous article, we have introduced the concept of learning in a single-layer neural network. Today, we will learn about the benefits of multi-layer neural networks, how to properly design and train them.\nSometimes I discuss neural networks with students who have just started discovering machine learning techniques:\n\u0026quot;I have built a handwritten digits recognition network. But my accuracy is only Y.\u0026quot;\n\u0026quot;It seems to be much less than state-of-the-art\u0026quot;, I contemplate.","tags":["Deep Learning","Haskell"],"title":"Day 2: What Do Hidden Layers Do?","type":"neural-networks"},{"authors":["Bogdan Penkovsky"],"categories":["10 Days Of Grad"],"content":" Neural networks is a topic that recurrently appears throughout my life. Once, when I was a BSc student, I got obsessed with the idea to build an \u0026quot;intelligent\u0026quot; machine1. I spent a couple of sleepless nights thinking. I read a few essays shedding some light on this philosophical subject, among which the most prominent, perhaps, stand Marvin Minsky's writings2. As a result, I came across the neural networks idea. It was 2010, and deep learning was not nearly as popular as it is now3. Moreover, no one made much effort linking to neural networks in calculus or linear algebra curricula. Even folks doing classical optimization and statistics sometimes seemed puzzled when hearing about neural nets.\nThis may seem surprising given that the most of the concepts we use today were already known in early 90s. However, before the advent of GPUs and sufficiently large and diverse datasets, neural networks remained a realm of a relatively small academic community.\nI was lucky enough to implement my first neural net as a part of the AI curriculum. It was a simple network made of what is called fully-connected layers and sigmoid activations, which we shall study today. At the time I realized that existing state of our knowledge was still lacking to really build \u0026quot;thinking computers\u0026quot;4.\nIt was 2012, the conference in Crimea, Ukraine where I attended a brilliant talk by Prof. Laurent Larger. He explained how to build a high-speed hardware for speech recognition using a laser. The talk has inspired me, and a year later I started a PhD training with an aim to develop reservoir computing methods, that is recurrent neural networks implemented directly in hardware. Finally, now I am using deep neural networks as a part of my daily job (still true in 2023).\nIn this series I will highlight some curious details of problem solving with neural networks. I will try to avoid an unnecessary long introduction and go straight to the subject. (Otherwise feel free to check Chapter 1.2 of my thesis). Briefly, (artificial) neural networks are to some extent inspired by biological neurons. Similarly to its biological counterpart, artificial neuron receives many inputs, performs a nonlinear transformation, and produces an output. The equation below formalizes this kind of behavior:\n$$ \\begin{equation} y = f(w_1 x_1 + w_2 x_2 + \\dots + w_N x_N) = f(\\sum_i w_i x_i), \\end{equation} $$\nwhere $N$ is the number of inputs $x_i$, $w_i$ are synaptic weights5, and $y$ is the result. Surprising as it may appear, in a modern neural network $f$ can be practically any nonlinear function. This nonlinear function is often called an activation function. Congratulations, we have arrived at a neural network from 1950-s. Curiously, at the time it was reported that the perceptron will be \u0026quot;the embryo of an electronic computer that expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence\u0026quot;. Today, we can say it was a bit of an over statement. Nevertheless, it is not the first hype over the so-called \u0026quot;AI\u0026quot; that we experience nowadays. Later it was an \u0026quot;AI winter\u0026quot;, when everyone lost the interest. And it happened twice.\nTo continue reading this article, some mathematical background is useful, but not mandatory. Anyone can develop an intuition about neural networks!\nA Word On Haskell This series of posts practically illustrate all the concepts in Haskell programming language. To motivate you, here are some Q \u0026amp; A you always wanted to know.\nQ: Is there anything that makes Haskell particularly good for Neural Networks or are you simply doing it because you prefer to use Haskell?\nA: Neural networks are very \u0026quot;function objects\u0026quot;. A network is just a big composition of functions. These kinds of things are very natural in a functional language.\nQ: As I am completely new to Haskell, would like to know what are the benefits of using Haskell vs python or other languages?\nA: The benefits of using Haskell:\n It is much easier to reason about what your program is doing. Great when refactoring existing code base. Haskell shapes your thinking towards problem solving in a pragmatic way. Haskell programs are fast.  Q: I figured my lack of Haskell knowledge would make it hard to read, but your code examples still make sense to me.\nA: Thank you. I find Haskell to be very intuitive when explaining neural nets.\nEdit 2023: There are several Haskell-based projects that have recently evolved. For instance, you may want to check Dex, 'a functional, statically typed language for array processing', by Google Research. There also exist PureScript and Elm aimed at JavaScript. And Clash that translates to hardware (VHDL, Verilog, System Verilog).\nGradient Descent: A CS Freshman Year  Gradient Descent. Image credit   The very basic idea behind neural networks training and deep learning is a local optimization method known as gradient descent. For those who have hard time remembering their freshman year, just watch an introductory video explaining the idea.\nHow does a concept as simple as gradient descent work for a neural network? Well, a neural network is only a function6, mapping an input to some output. By comparing the neural network's output to some desired output, one can obtain another function known as an error function. This error function has a certain error landscape, like in the mountains. By using gradient descent, we modify our neural network in such a way that we are descending this landscape. Therefore, we aim at finding an error minimum. The key concept of the optimization method is that the error gradients give us a direction in which to change the neural network. In a similar way one would be able to descend a hill covered with a thick fog. In both cases only a local gradient (slope) is available.\nGradient descent can be described by a formula:\n$$\\begin{equation} x_{n+1} = x_n - \\gamma \\cdot \\nabla F(x_n), \\end{equation} $$\nwhere constant $\\gamma$ is what is referred to in deep learning as the learning rate, i.e. the amount of learning per iteration $n$. In the simplest case, $x$ is a scalar variable. Below is an interactive visualization for gradient descent in case of a one-dimensional function, e.g. $f(x) = (x - 3)^2$, starting from an initial guess $x_0 = 0$:\n #vis-form-1 { display: flex; align-items: center; } #vis-form-1 label { margin-right: 8px; } #vis-form-1 input { width: 3em; } #vis-form-1 button { padding: 4px; font-size: 16px; border-radius: 4px; border: 1px solid #ccc; } #vis-form-1 button { background-color: #4CAF50; color: white; cursor: pointer; } #vis-form-1 button:hover { background-color: #45a049; }   $\\gamma=$ \u0026nbsp; $x_0=$ \u0026nbsp; Run      const width = 650; const height = 500; const svg = d3.select(\"#visualization\") .attr(\"width\", width) .attr(\"height\", height); const xScale = d3.scaleLinear() .domain([-2, 8]) .range([0, width]); const yScale = d3.scaleLinear() .domain([0, 16]) .range([height - 31, 5]); const xAxis = d3.axisBottom(xScale); const yAxis = d3.axisLeft(yScale); function target(x) { return Math.pow(x - 3, 2); } function grad_target(x) { return 2 * (x - 3); } function static_plot() { svg.append(\"g\") .attr(\"transform\", `translate(0, ${15 * height / 16})`) .call(xAxis); svg.append(\"g\") .attr(\"transform\", `translate(${width / 5}, 0)`) .call(yAxis); const xValues = d3.range(-0.8, 6.8, 0.1); const yValues = xValues.map(target); const line = d3.line() .x((d, i) = xScale(xValues[i])) .y(d = yScale(d)); svg.append(\"path\") .datum(yValues) .attr(\"d\", line) .attr(\"stroke\", \"gray\") .attr(\"stroke-width\", 2) .attr(\"fill\", \"none\"); } function descend(gradF, gamma, x0) { let result = []; let x = x0; let prevX = x0; const convergenceThreshold = 0.001; const maxSteps = 100; let step = 0; while (step { const startX = xScale(d); const endX = xScale(points[i + 1]); const startY = yScale(target(d)); const endY = yScale(target(points[i + 1])); return `M ${startX}, ${startY} L ${endX},${endY}`; }) .attr(\"stroke\", \"orange\") .attr(\"stroke-width\", \"5px\") .style(\"opacity\", 0); const circle = svg.selectAll(\"circle\") .data(points) .enter() .append(\"circle\") .attr(\"cx\", (d) = xScale(d)) .attr(\"cy\", (x) = yScale(target(x))) .attr(\"r\", 6.5) .attr(\"fill\", \"steelblue\") .style(\"opacity\", 0); circle.transition() .delay((_, i) = i * 1000) .duration(500) .style(\"opacity\", 1); arrowPaths.each(function (_, i) { const currentPath = d3.select(this); const pathLength = currentPath.node().getTotalLength(); currentPath .attr(\"stroke-dasharray\", pathLength) .attr(\"stroke-dashoffset\", pathLength) .style(\"opacity\", 1) .transition() .delay(i * 1000 + 500) .duration(500) .attr(\"stroke-dashoffset\", 0); }); static_plot(); } static_plot();  Try different values of learning rate $\\gamma$ (e.g. 0.2, 0.1, 0.7, 1.1) and see how fast the method converges (and if converges at all). When learning rate is too large, the algorithm may overshoot and not converge! Thus it might be challenging to find the best learning rate $\\gamma$. Keep this in mind when training neural networks. Also learn about Adam during Day 2. Yet another challenge is that the optimized function is rarely convex and nice (unlike the one above). As a result, the naive gradient descent method might struggle. We will address this challenge during Day 4 using batch normalization.\nThe gradient descent method can be implemented in a few lines of code. Play with the code snippet. Press the green triangle button below to run the code.\n That outputs the following sequence: $0.0,2.4,2.88,2.976,2.9952,2.99904,2.999808,2.9999616,\\dots$ Indeed, the value minimizing function $f(x)$ is $3$, i.e. $\\min f(x) = f(3) = (x-3)^2 = 0$. And the sequence is gradually converging towards that number. Let us look more carefully what does the code above do.\nLines 1-3: We define the gradient descent method, which iteratively applies the function step implementing equation (2). We provide the intermediate results taking the first iterN values.\nLine 5: Suppose, we would like to optimize a function $f(x) = (x-3)^2$. Its gradient gradF_test is then $\\nabla f(x) = 2\\cdot(x-3)$.\nLines 7-10: Finally, we run our gradient descent using learning rate $\\gamma = 0.4$.\nIt is crucial to realize that the value of $\\gamma$ affects the convergence. When $\\gamma$ is too small, the algorithm will take many more iterations to converge, however, when $\\gamma$ is too large the algorithm will never converge. At the moment, I am not aware of a good way how to determine the best $\\gamma$ for a given problem. Therefore, often different $\\gamma$ values have to be tried. Feel free to modify the code above and see what comes out! The method is generalizable to $N$ dimensions. Essentially, we would replace the gradF_test function with the one operating on vectors rather than scalars.\nNeural Network Ingredients For Classification  Les Iris (Irises) by Vincent van Gogh   Now that we realize how the gradient descent works, we may want to train a moderately useful network. Let's say we want to perform Iris flower classification using four distinctive features7: sepal length, sepal width, petal length, petal width. There are three classes of flowers we want to be able to recognize: Setosa, Versicolour, and Virginica. Now, there is a problem: how do we encode those three classes so that our neural network can handle them?\nNaive Solution And Why It Does Not Work The most simple solution to indicate each species would be using natural numbers. For instance, Iris Setosa can be encoded as 1, Versicolour, as 2, and Virginica, as 3. There is, however, a problem with this kind of encoding: we impose a bias. First, by encoding those classes as numbers, we impose a linear order over those three classes. It means, we start our count with Setosa, then, Versicolour, and then we arrive at Virginica. However, in reality it doesn't really matter if we end with Virginica or Vernicolour. Second, we also assume that the distance between Virginica and Setosa 3 - 1 = 2 is larger than between Virginica and Versicolor 3 - 2 = 1, which is a priory wrong.\nOne-Hot Encoding So which kind of encoding do we need? First, we want not to impose any restriction on ordering and second, we want the distances between classes to be equal. Therefore, we would prefer encoding each class to be orthogonal, i.e. independent from the other two. That becomes possible if we use vectors of three dimensions (as there are three classes). Therefore, now Setosa class is encoded as $[1, 0, 0]$, Versicolour, as $[0, 1, 0]$, and Virginica as $[0, 0, 1]$. The Euclidean distance between any pair of classes is equal to $\\sqrt 2$. Update8: For example, the distance between Setosa and Versicolour is computed as\n$$\\sqrt{(1-0)^2 + (0-1)^2 + (0-0)^2}=\\sqrt{2}.$$\nPutting It All Together Now that we are familiar with basic neural networks and gradient descent and also have some data to play with7, let the fun begin!\nFirst, we create a network of three neurons. To do that, we generalize formula (1):\n$$\\begin{equation} y_{i}=f (\\sum_k w_{ik} x_k), \\end{equation} $$ where $(x_1, x_2, x_3, x_4) = \\mathbf{x}$ is a 4D input vector, $w_{ik} \\in \\mathbf{W}, i=1 \\dots 3, k=1 \\dots 4$ is synaptic weights matrix, and result $(y_1, y_2, y_3) = \\mathbf{y}$ is a 3D vector. Generally speaking, we perform a matrix-vector multiplication with a subsequent element-wise activation:\n$$\\begin{equation} \\mathbf y = f (\\mathbf {W x}). \\end{equation} $$\nAs a nonlinear activation function $f$ we will use the sigmoid function9 $\\sigma(x) = [1 + e^{-x}]^{-1}$. We will exploit hmatrix Haskell library for linear algebra operations such as matrix multiplication. With hmatrix, Equation (4) can be written as:\nimport Numeric.LinearAlgebra as LA sigmoid = cmap f where f x = recip $ 1.0 + exp (-x) forward x w = let h = x LA.\u0026lt;\u0026gt; w y = sigmoid h in [h, y] where \u0026lt;\u0026gt; denotes the matrix product function from LA module. Note that x can be a vector, but it can be also a dataset matrix. In the latter case, forward will transform our entire dataset. Notice that we provide not only the result of our computation y, but also an intermediate step h since it will be later reused for w gradient computation.\nEach neuron $y_i$ is supposed to fire when it 'thinks' that it has detected one of the three species. E.g. when we have an output [0.89, 0.1, 0.2], we would assume that the fist neuron is the most 'confident', i.e. we interpret the result as Setosa. In other words, this output is treated as similar to [1, 0, 0]. As you can see, the maximal element was set to one and others, to zero. This is a so-called 'winner takes all' rule.\nBefore training the neural network, we need some measure of error or loss function to minimize. For instance, we can use the Euclidean loss $\\text{loss} = \\sum_i (\\hat y_i - y_i)^2$ where $\\hat y_i$ is a prediction and $y_i$ is a real answer from our dataset:\nloss y tgt = let diff = y - tgt in sumElements $ cmap (^2) diff For the sake of gradient descend illustration, we will reuse the descend function defined above. Now, we have to specify the gradient function for our neural network equation (4). We use what is called a backpropagation or shortly backprop method, which is essentially a result of the chain rule and is illustrated for an individual neuron in the figure below10.\n Backpropation for a single neuron.  First, in the forward pass, initial output $y$ is calculated. Then, this output is compared to some desired output and the error gradient $dy$ is passed back. Afterwards, the activation function gradient $df$ is obtained using $dy$. That ultimately leads to the remaining gradients $dw_1$, $dx_1$, $dw_2$, $dx_2, \\dots .$   Now we can calculate the weights gradient dW using the backprop method from above:\ngrad (x, y) w = dW where [h, y_pred] = forward x w dE = loss\u0026#39; y_pred y dY = sigmoid\u0026#39; h dE dW = linear\u0026#39; x dY Here linear', sigmoid', loss' are gradients of linear operation (multiplication), sigmoid activation $\\sigma(x)$, and the loss function. Note that by operating on matrices rather than scalar values we calculate the gradients vector dW denoting every synaptic weight gradient $dw_i$. Below are those \u0026quot;vectorized\u0026quot; functions definitions in Haskell using hmatrix library11:\nlinear\u0026#39; x dy = cmap (/ m) (tr\u0026#39; x LA.\u0026lt;\u0026gt; dy) where m = fromIntegral $ rows x sigmoid\u0026#39; x dY = dY * y * (ones - y) where y = sigmoid x ones = (rows y) \u0026gt;\u0026lt; (cols y) $ repeat 1.0 loss\u0026#39; y tgt = let diff = y - tgt in cmap (* 2) diff To test our network, we download the dataset from here (there are two files: x.dat and y.dat) and the code here. As instructed in the comments, we run our program:\n$ stack --resolver lts-10.6 --install-ghc runghc --package hmatrix-0.18.2.0 Iris.hs Initial loss 169.33744797846379 Loss after training 61.41242708538934 Some predictions by an untrained network: (5\u0026gt;\u0026lt;3) [ 8.797633210095851e-2, 0.15127581829026382, 0.9482351750129188 , 0.11279346747947296, 0.1733431584272155, 0.9502442520696124 , 0.10592462402394615, 0.17057190568339017, 0.9367875655363787 , 0.10167941966201806, 0.20651101803783944, 0.9300343579182122 , 8.328154248684484e-2, 0.15568011758813116, 0.940816298954776 ] Some predictions by a trained network: (5\u0026gt;\u0026lt;3) [ 0.6989749292681016, 0.14916793398555747, 0.1442697900857393 , 0.678406436711954, 0.1691062984304366, 0.2052955124240905 , 0.6842327447503195, 0.16782087736820395, 0.16721778476233148 , 0.6262988163006756, 0.19656943129188192, 0.17521133197774072 , 0.6905553549763312, 0.15299944611286123, 0.12910826989854146 ] Targets (5\u0026gt;\u0026lt;3) [ 1.0, 0.0, 0.0 , 1.0, 0.0, 0.0 , 1.0, 0.0, 0.0 , 1.0, 0.0, 0.0 , 1.0, 0.0, 0.0 ] That's all for today. Please feel free to play with the code. Hint: you may have noticed that grad calls sigmoid twice on the same data: once in forward and once in sigmoid'. Try optimizing the code to avoid this redundancy.\nAs soon as you understand the basics of neural networks, make sure you continue to Day 2. In the next post you will learn how to make your neural network operational. First of all, we will highlight the importance of multilayer structure. We will also show that nonlinear activations are crucial. Finally, we will improve neural network training and discuss weights initialization.\nCitation  @article{penkovsky2023NN, title = \"Learning Neural Networks The Hard Way\", author = \"Penkovsky, Bogdan\", journal = \"penkovsky.com\", year = \"2023\", month = \"January\", url = \"https://penkovsky.com/neural-networks/day1/\" }  MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\" } } });   There even exists a term describing exactly what I dreamed to achieve. ^ For instance, check Why people think computers can't. ^ With Google Trends in hand, we can witness the raise of global deep learning interest. ^ Despite people have fantasized about it long before Alan Turing. ^ Essentially, a synaptic weight $w_i$ determines the strength of connection to the $i$-th input. ^ With recurrent neural networks it is not true. Those have an internal state, making such networks equivalent to computer programs, i.e. potentially more complex than maps. ^ Here we refer to the classical Iris dataset. If you prefer another light dataset please let me know. ^ Kudos to Peter Harpending who spotted a typo in Euclidean distance. ^ In this example nonlinear activation is not essential. However, as we will see in future posts, in multilayer neural networks nonlinear activations are strongly required. ^ We look closer at the backpropagation mechanics here. ^ And here are their mathematical derivations. ^   ","date":1544541748,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686082835,"objectID":"90386fdd6894c45c023df9bc9b9794a1","permalink":"https://penkovsky.com/neural-networks/day1/","publishdate":"2018-12-11T16:22:28+01:00","relpermalink":"/neural-networks/day1/","section":"neural-networks","summary":"Neural networks is a topic that recurrently appears throughout my life. Once, when I was a BSc student, I got obsessed with the idea to build an \u0026quot;intelligent\u0026quot; machine1. I spent a couple of sleepless nights thinking. I read a few essays shedding some light on this philosophical subject, among which the most prominent, perhaps, stand Marvin Minsky's writings2. As a result, I came across the neural networks idea. It was 2010, and deep learning was not nearly as popular as it is now3.","tags":["Deep Learning","Haskell"],"title":"Day 1: Learning Neural Networks The Hard Way","type":"neural-networks"},{"authors":null,"categories":null,"content":"B. Penkovsky, M. Bocquet, T. Hirztlin, J.-O. Klein, E. Nowak, E. Vianello, J.-M. Portal and D. Querlioz\nThe advent of deep learning has substantially accelerated machine learning development during the last decade. Multiple tasks such as computer vision have been drastically improved and even outperformed the human accuracy. However, those deep learning implementations require almost two orders of magnitude more power comparing to the human brain. Moreover, with the end of the Moore’s law, new hardware insights are necessary to keep steady the progress. With new memories available, such as Resistive and Magnetoelectric Random Access Memory (RRAM and MRAM), thanks to the latest advancements in nanotechnology, emerging Binarized Neural Networks (BNNs) are promising to reduce the energy impact of the forthcoming machine learning hardware generation. The simplicity and energy efficiency of BNNs makes them especially suitable for wearable devices, including but not limited to biomedical, healthcare, and sport application domains. In this talk, we discuss our latest BNN developments along with our vision towards the future of energy-efficient smart devices.\n","date":1543060800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543060800,"objectID":"a654f84fcc9b9d0ce4bbc9dab29a66fb","permalink":"https://penkovsky.com/talk/icee2018/","publishdate":"2018-11-24T13:00:00+01:00","relpermalink":"/talk/icee2018/","section":"talk","summary":"B. Penkovsky, M. Bocquet, T. Hirztlin, J.-O. Klein, E. Nowak, E. Vianello, J.-M. Portal and D. Querlioz\nThe advent of deep learning has substantially accelerated machine learning development during the last decade. Multiple tasks such as computer vision have been drastically improved and even outperformed the human accuracy. However, those deep learning implementations require almost two orders of magnitude more power comparing to the human brain. Moreover, with the end of the Moore’s law, new hardware insights are necessary to keep steady the progress.","tags":["Deep Learning"],"title":"Towards Binarized Neural Networks Hardware","type":"talk"},{"authors":null,"categories":null,"content":"We study chimera states and dissipative solitons syncronization patterns in delayed feedback systems. High dynamics multistability and robustness of those self-regenerating systems makes them suitable for photonic memory applications. Based on existing hardware, this memory is compatible with photonic reservoir computing.\n","date":1540890010,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540890010,"objectID":"6aa9da148c457fb94df1f176b4e8fbde","permalink":"https://penkovsky.com/project/sync/","publishdate":"2018-10-30T11:00:10+02:00","relpermalink":"/project/sync/","section":"project","summary":"Chimera states and dissipative soliton patterns for fast self-healing memory.","tags":["Chimera","Dissipative Solitons","DDE"],"title":"Syncronization patterns","type":"project"},{"authors":["Bogdan Penkovsky","Laurent Larger","Daniel Brunner"],"categories":null,"content":"","date":1540854000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540854000,"objectID":"c575a20a91455873544c0febb73861d4","permalink":"https://penkovsky.com/publication/hardware-enabled-reservoir-computing/","publishdate":"2018-10-30T00:00:00+01:00","relpermalink":"/publication/hardware-enabled-reservoir-computing/","section":"publication","summary":"In this work, we propose a new approach towards the efficient optimization and implementation of reservoir computing hardware reducing the required domain expert knowledge and optimization effort. First, we adapt the reservoir input mask to the structure of the data via linear autoencoders. We therefore incorporate the advantages of dimensionality reduction and dimensionality expansion achieved by conventional algorithmically efficient linear algebra procedures of principal component analysis. Second, we employ evolutionary-inspired genetic algorithm techniques resulting in a highly efficient optimization of reservoir dynamics with dramatically reduced  number of evaluations comparing to exhaustive search. We illustrate the method on the so-called single-node reservoir computing architecture, especially suitable for implementation in ultrahigh-speed hardware. The combination of both methods and the resulting reduction of time required for performance optimization of a hardware system establish a strategy towards machine learning hardware capable of self-adaption to optimally solve specific problems. We confirm the validity of those principles building reservoir computing hardware based on a field-programmable gate array.","tags":["Reservoir Computing"],"title":"Efficient Design of Hardware-Enabled Reservoir Computing in FPGAs","type":"publication"},{"authors":["Daniel Brunner","Bogdan Penkovsky","Bicky A. Marquez","Maxime Jacquot","Ingo Fischer","Laurent Larger"],"categories":null,"content":"","date":1539727200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539727200,"objectID":"1a39b52b9823cc29c5a3c8661bcdba5c","permalink":"https://penkovsky.com/publication/photonic-neural-networks/","publishdate":"2018-10-17T00:00:00+02:00","relpermalink":"/publication/photonic-neural-networks/","section":"publication","summary":"Photonic delay systems have revolutionized the hardware implementation of Recurrent Neural Networks and Reservoir Computing in particular. The fundamental principles of Reservoir Computing strongly benefit a realization in such complex analog systems. Especially delay systems, potentially providing large numbers of degrees of freedom even in simple architectures, can efficiently be exploited for information processing. The numerous demonstrations of their performance led to a revival of photonic Artificial Neural Network. Today, an astonishing variety of physical substrates, implementation techniques as well as network architectures based on this approach have been successfully employed. Important fundamental aspects of analog hardware Artificial Neural Networks have been investigated, and multiple high-performance applications have been demonstrated. Here, we introduce and explain the most relevant aspects of Artificial Neural Networks and delay systems, the seminal experimental demonstrations of Reservoir Computing in photonic delay systems, plus and the most recent and advanced realizations.","tags":[],"title":"Tutorial: Photonic Neural Networks in Delay Systems","type":"publication"},{"authors":["Daniel Brunner","Bogdan Penkovsky","Roman Levchenko","Eckehard Schoell","Laurent Larger","Yuri Maistrenko"],"categories":null,"content":"","date":1539036000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539036000,"objectID":"7d33dbb08f927cc8dc389782e59b9d56","permalink":"https://penkovsky.com/publication/chimeras-and-dissipative-solitons/","publishdate":"2018-10-09T00:00:00+02:00","relpermalink":"/publication/chimeras-and-dissipative-solitons/","section":"publication","summary":"We demonstrate for a nonlinear photonic system that two highly asymmetric feedback delays can induce a variety of emergent patterns which are highly robust during the system's global evolution. Explicitly, two-dimensional chimeras and dissipative solitons become visible upon a space-time transformation. Switching between chimeras and dissipative solitons requires only adjusting two system parameters, demonstrating self-organization exclusively based on the system's dynamical properties. Experiments were performed using a tunable semiconductor laser's transmission through a Fabry-Perot resonator resulting in an Airy function as nonlinearity. Resulting dynamics were band-pass filtered and propagated along two feedback paths whose time delays differ by two orders of magnitude. An excellent agreement between experimental results and theoretical model given by modified Ikeda equations was achieved.","tags":["chimera"],"title":"Spatio-temporal complexity in dual delay nonlinear laser dynamics: chimeras and dissipative solitons","type":"publication"},{"authors":null,"categories":null,"content":"Conceiving the next-generation computing devices inspired by biological systems such as the human brain. We apply the principles of nonlinear transient and low-precision computing.\n","date":1534752079,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534752079,"objectID":"923792b1ce401b15f8264992b3d2de91","permalink":"https://penkovsky.com/project/bioinspired/","publishdate":"2018-08-20T10:01:19+02:00","relpermalink":"/project/bioinspired/","section":"project","summary":"Conceiving next-generation computing principles inspired by biological systems such as the human brain.","tags":["machine learning","RNN","Reservoir Computing"],"title":"Bio-inspired computing","type":"project"},{"authors":null,"categories":null,"content":"","date":1534330800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534330800,"objectID":"f16ba9b8ac51dd50e79b434d5ece911c","permalink":"https://penkovsky.com/talk/ddays2018/","publishdate":"2018-08-15T13:00:00+02:00","relpermalink":"/talk/ddays2018/","section":"talk","summary":"Coherent-incoherent motions known as chimera states have recently sparked substantial interest in the nonlinear dynamics community. Recent findings show that chimera states may appear in a wide range of natural phenomena. In the present contribution, we exploit the analogy between delayed-feedback systems and symmetric networks of nonlinear oscillators in this context. We demonstrate the existence of two-dimensional chimera states created in a system consisting of two long delays, where one delay exceeds the other by two orders of magnitude.We present the first experimental demonstration of 2D chimera states in nonlinear delay systems and obtain an excellent agreement between numerical simulations and the experiment. Observed chimeras are highly robust, i.e. are stable with respect to the noise in the experimental setup and exist for a wide range of parameters. Results can potentially be applied in multiple areas including power grids, optical memory, and neuromorphic computing.","tags":["chimera"],"title":"Chimera States in Nonlinear Systems with Multiple Delayed Feedbacks","type":"talk"},{"authors":null,"categories":null,"content":"","date":1527937200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527937200,"objectID":"ba3ede6a6a17f2ae4c81a4e7655b61c6","permalink":"https://penkovsky.com/talk/juelich2018/","publishdate":"2018-06-02T13:00:00+02:00","relpermalink":"/talk/juelich2018/","section":"talk","summary":"The motivation behind our research in dynamical systems is an implementation of brain-inspired hardware. In the first part of the talk, we discuss the emergence of complex patterns in a nonlinear delay oscillator. Those self-organized formations represent coherent-incoherent motions known as chimera states. As the number of delays is increased, we observe the appearance of new structures such as dissipative solitons. Those structures exhibit high multistability which suggests their potential application as an optical memory. In the second part of the talk, we study the delay oscillator from reservoir computing (RC) perspective, i.e. recurrent neural networks leveraging physically existing dynamical systems. The strength of the RC technique is the possibility to deal with complex time-dependent data such as sounds and chaotic time-series. Finally, we demonstrate a stand-alone delay dynamics-based reservoir computer built on top of FPGA hardware.","tags":["chimera","Reservoir Computing","RNN"],"title":"On Theory and Modeling of Complex Nonlinear Delay Dynamics","type":"talk"},{"authors":["Laurent Larger","Bogdan Penkovsky","Yuri Maistrenko"],"categories":null,"content":"","date":1436824800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436824800,"objectID":"ef9500ddd199fe694421e3b3fefb8449","permalink":"https://penkovsky.com/publication/laser-chimeras/","publishdate":"2015-07-14T00:00:00+02:00","relpermalink":"/publication/laser-chimeras/","section":"publication","summary":"A chimera state is a rich and fascinating class of self-organized solutions developed in high-dimensional networks. Necessary features of the network for the emergence of such complex but structured motions are non-local and symmetry breaking coupling. An accurate understanding of chimera states is expected to bring important insights on deterministic mechanism occurring in many structurally similar high-dimensional dynamics such as living systems, brain operation principles and even turbulence in hydrodynamics. Here we report on a powerful and highly controllable experiment based on an optoelectronic delayed feedback applied to a wavelength tuneable semiconductor laser, with which a wide variety of chimera patterns can be accurately investigated and interpreted. We uncover a cascade of higher-order chimeras as a pattern transition from N to N+1 clusters of chaoticity. Finally, we follow visually, as the gain increases, how chimera state is gradually destroyed on the way to apparent turbulence-like system behaviour.","tags":["Chimera"],"title":"Laser chimeras as a paradigm for multistable patterns in complex systems","type":"publication"},{"authors":["Laurent Larger","Bogdan Penkovsky","Yuri Maistrenko"],"categories":null,"content":"","date":1377813600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377813600,"objectID":"30b6a279820a81aa85ec456346cbe108","permalink":"https://penkovsky.com/publication/virtual-chimera/","publishdate":"2013-08-30T00:00:00+02:00","relpermalink":"/publication/virtual-chimera/","section":"publication","summary":"Time-delayed systems are found to display remarkable temporal patterns the dynamics of which split into regular and chaotic components repeating at the interval of a delay. This novel long-term behavior for delay dynamics results from strongly asymmetric nonlinear delayed feedback driving a highly damped harmonic oscillator dynamics. In the corresponding virtual space-time representation, the behavior is found to develop as a chimeralike state, a new paradigmatic object from the network theory characterized by the coexistence of synchronous and incoherent oscillations. Numerous virtual chimera states are obtained and analyzed, through experiment, theory, and simulations.","tags":["Chimera"],"title":"Virtual Chimera States for Delayed-Feedback Systems","type":"publication"}]